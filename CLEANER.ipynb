{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Synopsis<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#IPYNB-Basics\" data-toc-modified-id=\"IPYNB-Basics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>IPYNB Basics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Shell-CMD\" data-toc-modified-id=\"Shell-CMD-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Shell CMD</a></span></li><li><span><a href=\"#Jupyter-Tips\" data-toc-modified-id=\"Jupyter-Tips-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Jupyter Tips</a></span></li></ul></li><li><span><a href=\"#Data-Structure\" data-toc-modified-id=\"Data-Structure-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Structure</a></span><ul class=\"toc-item\"><li><span><a href=\"#Container-Sequences\" data-toc-modified-id=\"Container-Sequences-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Container Sequences</a></span></li><li><span><a href=\"#Collection-module-(advanced-container)\" data-toc-modified-id=\"Collection-module-(advanced-container)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Collection module (advanced container)</a></span><ul class=\"toc-item\"><li><span><a href=\"#CASE:-Parse-Datefile\" data-toc-modified-id=\"CASE:-Parse-Datefile-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>CASE: Parse Datefile</a></span></li></ul></li></ul></li><li><span><a href=\"#Miscellaneous-NP,-PD\" data-toc-modified-id=\"Miscellaneous-NP,-PD-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Miscellaneous NP, PD</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Two-types-of-args-and-*kwargs\" data-toc-modified-id=\"Two-types-of-args-and-*kwargs-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Two types of <em>args and *</em>kwargs</a></span></li><li><span><a href=\"#Map-Function\" data-toc-modified-id=\"Map-Function-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Map Function</a></span></li><li><span><a href=\"#Filter-func\" data-toc-modified-id=\"Filter-func-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Filter func</a></span></li><li><span><a href=\"#Reduce-from-functools\" data-toc-modified-id=\"Reduce-from-functools-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>Reduce from functools</a></span></li><li><span><a href=\"#Error-Control-(try,-except,-assert,-final)\" data-toc-modified-id=\"Error-Control-(try,-except,-assert,-final)-3.0.5\"><span class=\"toc-item-num\">3.0.5&nbsp;&nbsp;</span>Error Control (try, except, assert, final)</a></span></li><li><span><a href=\"#Iterable\" data-toc-modified-id=\"Iterable-3.0.6\"><span class=\"toc-item-num\">3.0.6&nbsp;&nbsp;</span>Iterable</a></span></li><li><span><a href=\"#CASE:-counting-example-using-tweet\" data-toc-modified-id=\"CASE:-counting-example-using-tweet-3.0.7\"><span class=\"toc-item-num\">3.0.7&nbsp;&nbsp;</span>CASE: counting example using tweet</a></span></li><li><span><a href=\"#List-Comprehension\" data-toc-modified-id=\"List-Comprehension-3.0.8\"><span class=\"toc-item-num\">3.0.8&nbsp;&nbsp;</span>List Comprehension</a></span></li><li><span><a href=\"#Context-Manager,-Res-allocation-(connection)\" data-toc-modified-id=\"Context-Manager,-Res-allocation-(connection)-3.0.9\"><span class=\"toc-item-num\">3.0.9&nbsp;&nbsp;</span>Context Manager, Res allocation (connection)</a></span></li><li><span><a href=\"#DateFrame-in-Chunk\" data-toc-modified-id=\"DateFrame-in-Chunk-3.0.10\"><span class=\"toc-item-num\">3.0.10&nbsp;&nbsp;</span>DateFrame in Chunk</a></span></li></ul></li></ul></li><li><span><a href=\"#Numpy\" data-toc-modified-id=\"Numpy-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Numpy</a></span></li><li><span><a href=\"#PANDAS\" data-toc-modified-id=\"PANDAS-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>PANDAS</a></span><ul class=\"toc-item\"><li><span><a href=\"#Basic\" data-toc-modified-id=\"Basic-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Basic</a></span></li></ul></li><li><span><a href=\"#File-IO\" data-toc-modified-id=\"File-IO-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>File IO</a></span></li><li><span><a href=\"#Wrangling-(clean,-transform,-merge,-reshape)\" data-toc-modified-id=\"Wrangling-(clean,-transform,-merge,-reshape)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Wrangling (clean, transform, merge, reshape)</a></span></li><li><span><a href=\"#TIME-SERIES\" data-toc-modified-id=\"TIME-SERIES-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>TIME SERIES</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Datetime-forsmat-specs\" data-toc-modified-id=\"Datetime-forsmat-specs-8.0.1\"><span class=\"toc-item-num\">8.0.1&nbsp;&nbsp;</span>Datetime forsmat specs</a></span></li><li><span><a href=\"#Locale-spec-formating\" data-toc-modified-id=\"Locale-spec-formating-8.0.2\"><span class=\"toc-item-num\">8.0.2&nbsp;&nbsp;</span>Locale-spec formating</a></span></li><li><span><a href=\"#Base-Time-Series-Frequencies\" data-toc-modified-id=\"Base-Time-Series-Frequencies-8.0.3\"><span class=\"toc-item-num\">8.0.3&nbsp;&nbsp;</span>Base Time Series Frequencies</a></span></li><li><span><a href=\"#Resample-Method-Arguments\" data-toc-modified-id=\"Resample-Method-Arguments-8.0.4\"><span class=\"toc-item-num\">8.0.4&nbsp;&nbsp;</span>Resample Method Arguments</a></span></li><li><span><a href=\"#Moving-Window-and-Exp-weighted-func\" data-toc-modified-id=\"Moving-Window-and-Exp-weighted-func-8.0.5\"><span class=\"toc-item-num\">8.0.5&nbsp;&nbsp;</span>Moving Window and Exp weighted func</a></span></li><li><span><a href=\"#Other-Time-Series-Examples-in-FinEcon\" data-toc-modified-id=\"Other-Time-Series-Examples-in-FinEcon-8.0.6\"><span class=\"toc-item-num\">8.0.6&nbsp;&nbsp;</span>Other Time Series Examples in FinEcon</a></span></li></ul></li></ul></li><li><span><a href=\"#Advanced-NumPy\" data-toc-modified-id=\"Advanced-NumPy-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Advanced NumPy</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Performance-Tips\" data-toc-modified-id=\"Performance-Tips-9.0.1\"><span class=\"toc-item-num\">9.0.1&nbsp;&nbsp;</span>Performance Tips</a></span></li><li><span><a href=\"#Cython\" data-toc-modified-id=\"Cython-9.0.2\"><span class=\"toc-item-num\">9.0.2&nbsp;&nbsp;</span>Cython</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPYNB Basics\n",
    "\n",
    "## Shell CMD\n",
    "! + CMD or just CMD for common ones:\n",
    "- !pip version\n",
    "- pwd, ls, whos, lsmagic, env, \n",
    "%% special block code\n",
    "- %%bash \\n for i in a b c; \\n do \n",
    "- %%HTML \\n `<a id='asdf' class='gie-single' href='url' target='_blank' style='color:#a7a7a7;text-decoratin:none;font-weight:normal !important;border:none;display:inline-block;'>text</a><script>window.gie=window.git||fucntion(c){(...</script>`\n",
    "    \n",
    "## Jupyter Tips\n",
    "- MAGIC: IPython kernel\n",
    "- %run python code, %load, %store, %prun - time running func, %pdb - debugging. %config InlineBackend.figure_format='retina'\n",
    "- Other codes: %%bash, %%ruby, %%r\n",
    "- Running R and Python in the same notebook Pip install rpy2 %load_ext rpy2.ipython df = pd.DateFrame( {‘Letter’: [‘a’,…], ‘X’: [] } ) %%R -I df ggplot(data=df) + gemo_point(aes(x=X, y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Multiple</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Display Elements</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**awesome**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://avatars2.githubusercontent.com/u/12401040?v=3&s=200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "display(HTML(\"<h1>Multiple</h1>\"))\n",
    "display(HTML(\"<p>Display Elements</p>\"))\n",
    "display(Markdown('**awesome**'))\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(url='https://avatars2.githubusercontent.com/u/12401040?v=3&s=200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'acos',\n",
       " 'acosh',\n",
       " 'asin',\n",
       " 'asinh',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atanh',\n",
       " 'ceil',\n",
       " 'copysign',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'degrees',\n",
       " 'e',\n",
       " 'erf',\n",
       " 'erfc',\n",
       " 'exp',\n",
       " 'expm1',\n",
       " 'fabs',\n",
       " 'factorial',\n",
       " 'floor',\n",
       " 'fmod',\n",
       " 'frexp',\n",
       " 'fsum',\n",
       " 'gamma',\n",
       " 'gcd',\n",
       " 'hypot',\n",
       " 'inf',\n",
       " 'isclose',\n",
       " 'isfinite',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'ldexp',\n",
       " 'lgamma',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'modf',\n",
       " 'nan',\n",
       " 'pi',\n",
       " 'pow',\n",
       " 'radians',\n",
       " 'sin',\n",
       " 'sinh',\n",
       " 'sqrt',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'tau',\n",
       " 'trunc']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "dir(math)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structure\n",
    "\n",
    "## Container Sequences\n",
    "- Lists\n",
    "    - Combine: `+ .extend() .index() .pop(position) sorted()`\n",
    "- Tuple\n",
    "    - ZIP and UNPACK\n",
    "    - `enumerate(): for idx, item in enumerate(tuple)`\n",
    "    - ',' COMMA creates Tuple\n",
    "- Set for unordered, unique data\n",
    "    - `set()` retain only unique items\n",
    "    - `.add() .update() discard() pop() union() intersection() difference()`\n",
    "- Dictionary\n",
    "    - \"nestable\" dict as val in dict\n",
    "    - tuple2dict `for key, val in tuple: name[key] = val`\n",
    "    - safely find key `.get('key', 'name for unfound')\n",
    "    - `.keys() dict[key].update(tuple of key-val) del or dict.pop(key)`\n",
    "    - `for key, val in dict.items(), if key in dict`\n",
    "- CSV\n",
    "    - `import csv` `open(filename, 'r')\n",
    "    - `for row in csv.reader(file)`\n",
    "    - dict2csv `for row in csv.DictReader(file)`\n",
    "\n",
    "## Collection module (advanced container)\n",
    "- Counter\n",
    "    - `from collections import Counter`\n",
    "    - `count = Counter(dict)` `count[key]` `count.most_common(3)`\n",
    "- Preload DICT defaultdict\n",
    "    - `defaultdict = defaultdict(list)` `for key, val in defaultdict: defaultdict[key].append(val)`\n",
    "    - `for key in defaultdict: if key.get('name'): defaultdict['name'] += 1\n",
    "```python\n",
    "\t\t\t\t\t# Create an empty dictionary: ridership\n",
    "\t\t\t\t\tridership = {}\n",
    "\n",
    "\t\t\t\t\t# Iterate over the entries\n",
    "\t\t\t\t\tfor date, stop, riders in entries:\n",
    "\t\t\t\t\t    # Check to see if date is already in the dictionary\n",
    "\t\t\t\t\t    if date not in ridership:\n",
    "\t\t\t\t\t        # Create an empty list for any missing date\n",
    "\t\t\t\t\t        ridership[date] = []\n",
    "\t\t\t\t\t    # Append the stop and riders as a tuple to the date keys list\n",
    "\t\t\t\t\t    ridership[date].append((stop, riders))\n",
    "\t\t\t\t\t    \n",
    "\t\t\t\t\t# Print the ridership for '03/09/2016'\n",
    "\t\t\t\t\tprint(ridership['03/09/2016'])\n",
    "\n",
    "\t\t\t\t\t# Import defaultdict\n",
    "\t\t\t\t\tfrom collections import defaultdict\n",
    "\n",
    "\t\t\t\t\t# Create a defaultdict with a default type of list: ridership\n",
    "\t\t\t\t\tridership = defaultdict(list)\n",
    "\n",
    "\t\t\t\t\t# Iterate over the entries\n",
    "\t\t\t\t\tfor date, stop, riders in entries:\n",
    "\t\t\t\t\t    # Use the stop as the key of ridership and append the riders to its value\n",
    "\t\t\t\t\t    ridership[stop].append(riders)\n",
    "\t\t\t\t\t    \n",
    "\t\t\t\t\t# Print the first 10 items of the ridership dictionary\n",
    "\t\t\t\t\tprint(list(ridership.items())[:10])\n",
    "```\n",
    "\n",
    "- NamedTuple\n",
    "    - `DateDetails = namedtuple('DateDetails', ['date', 'stop', 'riders'])`\n",
    "```python\n",
    "\t# Create the empty list: labeled_entries\n",
    "\tlabeled_entries = []\n",
    "\n",
    "\t# Iterate over the entries\n",
    "\tfor date, stop, riders in entries:\n",
    "\t    # Append a new DateDetails namedtuple instance for each entry to labeled_entries\n",
    "\t    labeled_entries.append(DateDetails(date, stop, riders))\n",
    "\t    \n",
    "\t# Print the first 5 items in labeled_entries\n",
    "\tprint(labeled_entries[:5])\n",
    "\t# Iterate over the first twenty items in labeled_entries\n",
    "\tfor item in labeled_entries[:20]:\n",
    "\t    # Print each item's stop\n",
    "\t    print(item.stop)\n",
    "\n",
    "\t    # Print each item's date\n",
    "\t    print(item.date)\n",
    "\n",
    "\t    # Print each item's riders\n",
    "\t    print(item.riders)\n",
    "```\n",
    "\n",
    "- DateTime\n",
    "```python\n",
    "\t\t# Import the datetime object from datetime\n",
    "\t\tfrom datetime import datetime\n",
    "\n",
    "\t\t# Iterate over the dates_list \n",
    "\t\tfor date_str in dates_list:\n",
    "\t\t    # Convert each date to a datetime object: date_dt\n",
    "\t\t    date_dt = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "\t\t    \n",
    "\t\t    # Print each date_dt\n",
    "\t\t    print(date_dt)\n",
    "\t\t# Loop over the first 10 items of the datetimes_list\n",
    "\t\tfor item in datetimes_list[:10]:\n",
    "\t\t    # Print out the record as a string in the format of 'MM/DD/YYYY'\n",
    "\t\t    print(item.strftime('%m/%d/%Y'))\n",
    "\t\t    \n",
    "\t\t    # Print out the record as an ISO standard string\n",
    "\t\t    print(item.isoformat())\n",
    "```\n",
    "\n",
    "- Timezone\n",
    "\n",
    "```python\n",
    "\t\t# Create a defaultdict of an integer: monthly_total_rides\n",
    "\t\tmonthly_total_rides = defaultdict(int)\n",
    "\n",
    "\t\t# Loop over the list daily_summaries\n",
    "\t\tfor daily_summary in daily_summaries:\n",
    "\t\t    # Convert the service_date to a datetime object\n",
    "\t\t    service_datetime = datetime.strptime(daily_summary[0], '%m/%d/%Y')\n",
    "\n",
    "\t\t    # Add the total rides to the current amount for the month\n",
    "\t\t    monthly_total_rides[service_datetime.month] += int(daily_summary[4])\n",
    "\t\t    \n",
    "\t\t# Print monthly_total_rides\n",
    "\t\tprint(monthly_total_rides)\n",
    "\n",
    "\t\t# Import datetime from the datetime module\n",
    "\t\tfrom datetime import datetime\n",
    "\n",
    "\t\t# Compute the local datetime: local_dt\n",
    "\t\tlocal_dt = datetime.now()\n",
    "\n",
    "\t\t# Print the local datetime\n",
    "\t\tprint(local_dt)\n",
    "\n",
    "\t\t# Compute the UTC datetime: utc_dt\n",
    "\t\tutc_dt = datetime.utcnow()\n",
    "\n",
    "\t\t# Print the UTC datetime\n",
    "\t\tprint(utc_dt)\n",
    "\n",
    "\t\t# Create a Timezone object for Chicago\n",
    "\t\tchicago_usa_tz = timezone('US/Central')\n",
    "\n",
    "\t\t# Create a Timezone object for New York\n",
    "\t\tny_usa_tz = timezone('US/Eastern')\n",
    "\n",
    "\t\t# Iterate over the daily_summaries list\n",
    "\t\tfor orig_dt, ridership in daily_summaries:\n",
    "\n",
    "\t\t    # Make the orig_dt timezone \"aware\" for Chicago\n",
    "\t\t    chicago_dt = orig_dt.replace(tzinfo=chicago_usa_tz)\n",
    "\t\t    \n",
    "\t\t    # Convert chicago_dt to the New York Timezone\n",
    "\t\t    ny_dt = chicago_dt.astimezone(ny_usa_tz)\n",
    "\t\t    \n",
    "\t\t    # Print the chicago_dt, ny_dt, and ridership\n",
    "\t\t    print('Chicago: %s, NY: %s, Ridership: %s' % (chicago_dt, ny_dt, ridership))\n",
    "```\n",
    "            \n",
    "- Time Change\n",
    "\n",
    "```python\n",
    "\t\t# Import timedelta from the datetime module\n",
    "\t\tfrom datetime import timedelta\n",
    "\n",
    "\t\t# Build a timedelta of 30 days: glanceback\n",
    "\t\tglanceback = timedelta(days=30)\n",
    "\n",
    "\t\t# Iterate over the review_dates as date\n",
    "\t\tfor date in review_dates:\n",
    "\t\t    # Calculate the date 30 days back: prior_period_dt\n",
    "\t\t    prior_period_dt = date - glanceback\n",
    "\t\t    \n",
    "\t\t    # Print the review_date, day_type and total_ridership\n",
    "\t\t    print('Date: %s, Type: %s, Total Ridership: %s' %\n",
    "\t\t         (date, \n",
    "\t\t          daily_summaries[date]['day_type'], \n",
    "\t\t          daily_summaries[date]['total_ridership']))\n",
    "\n",
    "\t\t    # Print the prior_period_dt, day_type and total_ridership\n",
    "\t\t    print('Date: %s, Type: %s, Total Ridership: %s' %\n",
    "\t\t         (prior_period_dt, \n",
    "\t\t          daily_summaries[prior_period_dt]['day_type'], \n",
    "\t\t          daily_summaries[prior_period_dt]['total_ridership']))\n",
    "\n",
    "\t\t# Iterate over the date_ranges\n",
    "\t\tfor start_date, end_date in date_ranges:\n",
    "\t\t    # Print the End and Start Date\n",
    "\t\t    print(end_date, start_date)\n",
    "\t\t    # Print the difference between each end and start date\n",
    "\t\t    print(end_date - start_date)\n",
    "\n",
    "\n",
    "\t\t# Import the pendulum module\n",
    "\t\timport pendulum\n",
    "\n",
    "\t\t# Create a now datetime for Tokyo: tokyo_dt\n",
    "\t\ttokyo_dt = pendulum.now('Asia/Tokyo')\n",
    "\n",
    "\t\t# Covert the tokyo_dt to Los Angeles: la_dt\n",
    "\t\tla_dt = tokyo_dt.in_timezone('America/Los_Angeles')\n",
    "\n",
    "\t\t# Print the ISO 8601 string of la_dt\n",
    "\t\tprint(la_dt.to_iso8601_string())\n",
    "\n",
    "\t\t# Iterate over date_ranges\n",
    "\t\tfor start_date, end_date in date_ranges:\n",
    "\n",
    "\t\t    # Convert the start_date string to a pendulum date: start_dt \n",
    "\t\t    start_dt = pendulum.parse(start_date, strict = False)\n",
    "\t\t    \n",
    "\t\t    # Convert the end_date string to a pendulum date: end_dt \n",
    "\t\t    end_dt = pendulum.parse(end_date, strict = False)\n",
    "\t\t    \n",
    "\t\t    # Print the End and Start Date\n",
    "\t\t    print(end_dt, start_dt)\n",
    "\t\t    \n",
    "\t\t    # Calculate the difference between end_dt and start_dt: diff_period\n",
    "\t\t    diff_period = end_dt - start_dt\n",
    "\t\t    \n",
    "\t\t    # Print the difference in days\n",
    "\t\t    print(diff_period.in_days())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE: Parse Datefile\n",
    "\n",
    "\n",
    "```python\n",
    "csvfile = open('file.csv', 'r')\n",
    "data = []\n",
    "\n",
    "for row in csv.reader(csvfile):\n",
    "    data.append(\n",
    "        (row[0], row[2], row[4], row[5])\n",
    "    )\n",
    "\n",
    "data.pop(0) # remove first element\n",
    "\n",
    "data_month = Counter()\n",
    "\n",
    "for row in data:\n",
    "    date = datetime.strptime(row[0], '%m/%d/%Y %I:%M:%S %p')\n",
    "        # convert first element of each into Datetime object\n",
    "    data_month[date.month] += 1 \n",
    "        # raise counter for the month of row by one\n",
    "\n",
    "data_month.most_common(3) # top 3 common months\n",
    "\n",
    "location_month = defaultdict(list)\n",
    "\n",
    "for row in data:\n",
    "    date = datetime.strptime(row[0], '%m/%d/%Y %I:%M:%S %p')\n",
    "    \n",
    "    if date.year == 2016:\n",
    "        location_month[date.month].append(row[4])\n",
    "            # set dict key to month adding location to value\n",
    "\n",
    "for month, location in location_month.items():\n",
    "    location_count = Counter(location)\n",
    "    print(month)\n",
    "    print(location_count.most_common(5))\n",
    "\n",
    "for row in csv.DictReader(csvfile):\n",
    "    dictrict = row.pop('District')\n",
    "    data_district[district].append(row)\n",
    "        # append rest of data to list for proper disctrict in data by district\n",
    "        \n",
    "for district, crime in data_district.items():\n",
    "    year_count = Counter()\n",
    "    \n",
    "    for crime in crimes:\n",
    "        if crime['Arrest'] == 'true':\n",
    "            year = datetime.strptime(crime['Date'], '%m/%d/%Y %I:%M:%S %p').year\n",
    "                # convert Date to datetime and get year\n",
    "            year_count[year] += 1\n",
    "\n",
    "n_state = set(crime_block['001XX N STATE ST'])\n",
    "    # unique list of crimes for first block\n",
    "crime_diff = n_state1.difference(n_state2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous NP, PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 6, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  2.,  4.,  6.,  8., 10.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n",
      "[[0 3]\n",
      " [1 4]\n",
      " [2 5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np_arange = np.arange(0, 10, 2)\n",
    "np_arange\n",
    "\n",
    "np_arange1 = np.arange(6)\n",
    "np_arange1\n",
    "\n",
    "np_linspace = np.linspace(0, 10, 6)\n",
    "np_linspace\n",
    "\n",
    "np_arange1.shape\n",
    "np_arange1.resize(2,3)\n",
    "np_arange1\n",
    "\n",
    "print(np_arange1.ravel())\n",
    "\n",
    "print(np_arange1.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "hellohello hellohellohello\n"
     ]
    }
   ],
   "source": [
    "# alter global object within functions\n",
    "global_val = 10\n",
    "\n",
    "def square(value):\n",
    "    global global_val # call global val\n",
    "    global_val = global_val*2\n",
    "    return global_val\n",
    "\n",
    "print(global_val)\n",
    "\n",
    "# nested function or recursive functions\n",
    "def echo(n):\n",
    "    \"\"\"returning inner echo\"\"\"\n",
    "    def inner_echo(word1):\n",
    "        echo_word = word1*n\n",
    "        return echo_word\n",
    "    return inner_echo\n",
    "twice = echo(2)\n",
    "thrice = echo(3)\n",
    "print(twice('hello'), thrice('hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two types of *args and **kwargs \n",
    "```python\n",
    "def count_entries(df, *args):\n",
    "    # Return a dictionary with counts of\n",
    "    # occurrences as value for each key.\n",
    "    cols_count = {}\n",
    "    for col_name in args:\n",
    "        col = df[col_name]\n",
    "        for entry in col:\n",
    "            if entry in cols_count.keys():\n",
    "                cols_count[entry] += 1\n",
    "            else:\n",
    "                cols_count[entry] = 1\n",
    "    return cols_count\n",
    "result1 = count_entries(tweets_df, 'lang')\n",
    "\n",
    "# Call count_entries(): result2\n",
    "result2 = count_entries(tweets_df, 'lang', 'source')\n",
    "def report_status(**kwargs):\n",
    "    # Print out the status of a movie character.\n",
    "    for key, val in kwargs.items():\n",
    "        print(key + \": \" + val)\n",
    "report_status(name='luke', affiliation='jedi')\n",
    "```\n",
    "\n",
    "### Map Function\n",
    "```python\n",
    "spells = ['protego', 'accio', 'expecto patronum', 'legilmnes']\n",
    "shout_spells = map(lambda item: item+'!!!', spells)\n",
    "#     mapping lambda function onto object \n",
    "shout_spells_list = list(shout_spells)\n",
    "```\n",
    "\n",
    "### Filter func\n",
    "```python\n",
    "fellowship = ['frodo', 'samwise', 'merry', 'aragorn']\n",
    "\n",
    "result = filter(lambda member: len(member) > 6, fellowship)\n",
    "    # filtering func onto object\n",
    "print(result)\n",
    "    # beware of object form of map and filter, to convert into list\n",
    "result_list = list(result)\n",
    "\n",
    "# example filtering tweets\n",
    "result = filter(lambda x: x[0:2] == 'RT', tweets_df['text'])\n",
    "result_list = list(result)\n",
    "for tweet in result_list: print(tweet)\n",
    "```\n",
    "\n",
    "### Reduce from functools\n",
    "```python\n",
    "from functools import reduce\n",
    "\n",
    "stark = ['robb', 'sansa', 'arya', 'eddard', 'jon']\n",
    "result = reduce(lambda item1, item2: item1+item2, stark)\n",
    "```\n",
    "\n",
    "### Error Control (try, except, assert, final)\n",
    "```python\n",
    "def shout_echo(word1, echo = 1):\n",
    "    echo_word = str()\n",
    "    shout_word = str()\n",
    "    try:\n",
    "        echo_word = word1 * echo\n",
    "        shout_word = echo_word + '!!!'\n",
    "    except:\n",
    "        print('word1 must be a string, and echo an integer.')\n",
    "    return shout_word\n",
    "\n",
    "shout_echo('particle', echo='accelerator')\n",
    "\n",
    "# alternative use of if\n",
    "# add:  if echo < 0: raise ValueError('echo must be positive')\n",
    "```\n",
    "\n",
    "### Iterable\n",
    "```python\n",
    "flash = ['jay garrick', 'barry allen', 'wally west', 'bart allen']\n",
    "for person in flash:\n",
    "    print(person)\n",
    "\n",
    "# flash is a list, to create an ITERATOR for flash\n",
    "\n",
    "superspeed = iter(flash)\n",
    "\n",
    "print(next(superspeed))\n",
    "print(next(superspeed))\n",
    "\n",
    "# to create a LIST OF TUPLES\n",
    "mutants = ['charles xavier', \n",
    "            'bobby drake', \n",
    "            'kurt wagner', \n",
    "            'max eisenhardt', \n",
    "            'kitty pride']\n",
    "\n",
    "mutant_list = list(enumerate(mutants))\n",
    "\n",
    "# unpack and print the tuple paris\n",
    "\n",
    "for index1, value1 in enumerate(mutants):\n",
    "    print(index1, value1)\n",
    "\n",
    "# change starting index\n",
    "for index2, value2 in enumerate(mutants, start=1):\n",
    "    print(index2, value2)\n",
    "    \n",
    "# Create a list of tuples: mutant_data\n",
    "mutant_data = list(zip(mutants, aliases, powers))\n",
    "# Print the list of tuples\n",
    "print(mutant_data)\n",
    "# Create a zip object using the three lists: mutant_zip\n",
    "mutant_zip = zip(mutants, aliases, powers)\n",
    "# Print the zip object\n",
    "print(mutant_zip)\n",
    "# Unpack the zip object and print the tuple values\n",
    "for value1,value2,value3 in zip(mutants, aliases, powers):\n",
    "    print(value1, value2, value3)\n",
    "\n",
    "# Create a zip object from mutants and powers: z1\n",
    "z1 = zip(mutants, powers)\n",
    "# Print the tuples in z1 by unpacking with *\n",
    "print(*z1)\n",
    "z1Dict = dict(z1)\n",
    "# Re-create a zip object from mutants and powers: z1\n",
    "z1 = zip(mutants, powers)\n",
    "# 'Unzip' the tuples in z1 by unpacking with * and zip(): result1, result2\n",
    "result1, result2 = zip(*z1)\n",
    "# Check if unpacked tuples are equivalent to original tuples\n",
    "print(result1 == mutants)\n",
    "print(result2 == powers)\n",
    "```\n",
    "\n",
    "### CASE: counting example using tweet\n",
    "```python\n",
    "# Define count_entries()\n",
    "def count_entries(csv_file, c_size, colname):\n",
    "    \"\"\"Return a dictionary with counts of\n",
    "    occurrences as value for each key.\"\"\"\n",
    "    # Initialize an empty dictionary: counts_dict\n",
    "    counts_dict = {}\n",
    "    # Iterate over the file chunk by chunk\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=c_size):\n",
    "        # Iterate over the column in DataFrame\n",
    "        for entry in chunk[colname]:\n",
    "            if entry in counts_dict.keys():\n",
    "                counts_dict[entry] += 1\n",
    "            else:\n",
    "                counts_dict[entry] = 1\n",
    "    # Return counts_dict\n",
    "    return counts_dict\n",
    "# Call count_entries(): result_counts\n",
    "result_counts = count_entries('tweets.csv', c_size=10,colname='lang')\n",
    "```\n",
    "\n",
    "### List Comprehension\n",
    "```python\n",
    "squares = [i**2 for i in range(0, 10)]\n",
    "\n",
    "# LoL syntax\n",
    "matrix = [ [ col for col in range(5) ] for row in range(5) ] \n",
    "\n",
    "print(squares)\n",
    "print(matrix)\n",
    "\n",
    "new_fellowship = [member for member in fellowship if len(member) >= 7]\n",
    "new_fellowship = [member if len(member) >= 7 else '' for member in fellowship]\n",
    "new_fellowship = {member:len(member) for member in fellowship}\n",
    "\n",
    "# LoL to LoD\n",
    "list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lits]\n",
    "# LoD to DF\n",
    "df = pd.DataFrame(list_of_dicts)\n",
    "```\n",
    "\n",
    "### Context Manager, Res allocation (connection)\n",
    "```python\n",
    "with open('world_dev_ind.csv') as file:\n",
    "    # skip col names\n",
    "    file.readline()\n",
    "    counts_dict = {}\n",
    "    \n",
    "    for j in range(1000):\n",
    "        line = file.readline().split(',')\n",
    "        first_col = line[0]\n",
    "        \n",
    "        if first_col in counts_dict.keys():\n",
    "            counts_dict[first_col] += 1\n",
    "        else:\n",
    "            counts_dict[first_col] = 1\n",
    "\n",
    "# read large file\n",
    "def read_large_file(file_object):\n",
    "    \"\"\"a GENERATOR func to read lazily.\"\"\"\n",
    "    while True:\n",
    "        data = file_object.readline()\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n",
    "\n",
    "with open('world_dev_ind.csv') as file:\n",
    "    gen_file = read_large_file(file)\n",
    "    print(next(gen_file))\n",
    "    print(next(gen_file))\n",
    "\n",
    "    \n",
    "# another way to read large data BY CHUNKS as DF, creating iterable 'reader object' that can be\n",
    "# next()\n",
    "df_reader = pd.read_csv('ind_pop.csv', chunksize=10)\n",
    "print(next(df_reader))\n",
    "```\n",
    "\n",
    "### DateFrame in Chunk\n",
    "```python\n",
    "]:\n",
    "# Get the first DataFrame chunk: df_urb_pop\n",
    "df_urb_pop = next(urb_pop_reader)\n",
    "# Check out the head of the DataFrame\n",
    "print(df_urb_pop.head())\n",
    "# Check out specific country: df_pop_ceb\n",
    "df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']\n",
    "# Zip DataFrame columns of interest: pops\n",
    "pops = zip(df_pop_ceb['Total Population'], df_pop_ceb['Urban population (% of total)'])\n",
    "# Turn zip object into list: pops_list\n",
    "pops_list = list(pops)\n",
    "\n",
    "# Use list comprehension to create new DataFrame column 'Total Urban Population'\n",
    "df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]\n",
    "# Plot urban population data\n",
    "df_pop_ceb.plot(kind='scatter', x='Year', y='Total Urban Population')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Initialize reader object: urb_pop_reader\n",
    "urb_pop_reader = pd.read_csv('ind_pop_data.csv', chunksize=1000)\n",
    "# Initialize empty DataFrame: data\n",
    "data = pd.DataFrame()\n",
    "# Iterate over each DataFrame chunk\n",
    "for df_urb_pop in urb_pop_reader:\n",
    "    # Check out specific country: df_pop_ceb\n",
    "    df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']\n",
    "    # Zip DataFrame columns of interest: pops\n",
    "    pops = zip(df_pop_ceb['Total Population'],\n",
    "                df_pop_ceb['Urban population (% of total)'])\n",
    "    # Turn zip object into list: pops_list\n",
    "    pops_list = list(pops)\n",
    "    # Use list comprehension to create new DataFrame column 'Total Urban Population'\n",
    "    df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1]) for tup in pops_list]\n",
    "    # Append DataFrame chunk to data: data\n",
    "    data = data.append(df_pop_ceb)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "np.int64\n",
    "np.float32\n",
    "np.complex\n",
    "np.bool\n",
    "np.object\n",
    "np.string_\n",
    "np.unicode_\n",
    "\n",
    "a.shape\n",
    "len(a)\n",
    "b.ndim\n",
    "e.size\n",
    "b.dtype\n",
    "b.dtype.name\n",
    "b.astype(int)\n",
    "\n",
    "np.exp(b)\n",
    "np.sqrt(b)\n",
    "np.sin(a)\n",
    "np.cos(b)\n",
    "np.log(a)\n",
    "e.dot(f)\n",
    "\n",
    "a.sum()\n",
    "a.min()\n",
    "b.max(axis=0)\n",
    "b.cumsum(axis=1)\n",
    "a.mean()\n",
    "b.median()\n",
    "a.corrcoef()\n",
    "np.std(b)\n",
    "\n",
    "# Create a view of the array with the same data\n",
    "h = a.view()\n",
    "# Create a copy of the array\n",
    "np.copy(a)\n",
    "# Create a deep copy of the array\n",
    "h = a.copy()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating by `np.array()` auto-convert __any__ sequences to dtype\n",
    "- __NDARRAY for memmory & scale ALWAYS NO COPY BUT VIEW___\n",
    "    - `arr_slice = arr[5:8]; mod arr_slice and arr mod`\n",
    "- Need explicit copy - `arr[5:8].copy()`\n",
    "- Access dim recursively `arr[n][n-1][n-2]...`\n",
    "- Dimension == Axis `arr2D[1:6]` means slicing along AXIS-0\n",
    "- Boolean Indexing\n",
    "    - `arr[names == 'bob', 2:]`\n",
    "    - Negation by `!=` or `-` like `data[-(name=='bob')]`\n",
    "    - `data[cond1 | or & cond2]`\n",
    "    - ALWAYS COPY\n",
    "- Fancy Indexing\n",
    "    - `data[ [4, 3, 0, 6] ]` indexing axis=0 elements \n",
    "- Question: `arr[ [1, 5, 7, 2] [0, 3, 1, 2] ]`\n",
    "    - Tuple-element (1, 0) (5, 3) etc\n",
    "    - BUT `[[1,5,7,2]][:, [0,3,1,2]]` logically select ordered elements from Axis-0, then select all element per Axis-0 with ordered elements from Axis-1 !!\n",
    "- Transpose `arr.T` and InnerProd `np.dot(arr.T, arr)`\n",
    "- High Dimension (2, 2, 4) transposed as `arr.transpose((1, 0, 2))` \n",
    "- `arr.swapaxes(1, 2)` then becomes (2, 4, 2) !!\n",
    "- Universal Functions `np.sqrt(arr), exp(), max(x, y)` ELEMENT-WISE ACTION\n",
    "    - `isnan isinf add subtract multiply divide power r`\n",
    "- MESHGRID for vectorised actions 2 1D -> 2 2D or (x)(y) becomes (x,y) paris \n",
    "    - `points = (1000,) xs, ys = np.meshgrid(points, points)`\n",
    "    - xs = (1000, 1000) in shape\n",
    "    - `z = np.sqrt(xs**2 + ys**2)`\n",
    "    - `plt.imshow(z, cmap = plt.cm.gray); plt.colorbar()`\n",
    "- Conditional Assigning (x if True, y if False) equi-array\n",
    "    - `[ (x if c else y) for x, y, c in zip(xarr, yarr, cond) ]`\n",
    "    - BUT Faster in NumPy method `.where()`\n",
    "    - `np.where(cond, xarr, yarr)`\n",
    "- EX: 4 possible conditions in nested where\n",
    "```python\n",
    "    np.where(cond1 & cond2, 0,\n",
    "             np.where(cond1, 1,\n",
    "                      np.where(cond2, 2, 3)))\n",
    "```\n",
    "- Math/Stats\n",
    "```python\n",
    "    np.random.randn(5, 4) # normal random\n",
    "    arr.mean(); np.mean(arr); arr.sum()\n",
    "    # AXIS ALWAYS OPTIONAL\n",
    "    arr.mean(axis=1)\n",
    "    std, var, min, max, argmin # first index loc of \n",
    "    cumsum, cumprod\n",
    "    (arr > 0).sum() # Num of positive\n",
    "    any(), all() # useful\n",
    "    arr.sort(1) # axis=1\n",
    "    arr[int(0.05 * len(arr))] # 5% Quantile\n",
    "    np.unique(names)\n",
    "    np.in1d(arr, [2,3,6]) # intersect1d(x, y), union1d(x,y)\n",
    "```\n",
    "\n",
    "- File I/O\n",
    "```python\n",
    "    np.save(filename, arr) # binary .npy file\n",
    "    np.load(filename)\n",
    "    np.savez(filename, a=arr, b=arr) # zipping dict-lke\n",
    "    np.loadtxt(filename, delimiter=',')\n",
    "    np.savetxt(filename) # write out\n",
    "    np.genfromtxt() # similar but for structured ARR and missing\n",
    "```\n",
    "\n",
    "- Linear Algebra\n",
    "```python\n",
    "    x.dot(y) == np.dot(x, y)\n",
    "    np.linalg\n",
    "    from numpy.linalg import inv, qr\n",
    "    mat = X.T.dot(X)\n",
    "    inv(mat), mat.dot(inv(mat)) = eye\n",
    "    svd, solve, lstsq\n",
    "```\n",
    "\n",
    "- Random\n",
    "```python\n",
    "    np.random.normal(size = (4, 4))\n",
    "    seed, permutation, shuffle, rand, randint, randn, binomial, normal, beta, chisquare, gamma, uniform\n",
    "    # EX first cross line\n",
    "    nstep = 1000; draws = np.random.randint(0,2, size=nstep)\n",
    "    steps = np.where(draws > 0, 1, -1)\n",
    "    walk = steps.cumsum(); walk.min()/max()\n",
    "    np.abs(walk) >= 10).argmax() # first index of max in bool-arr\n",
    "    # Simulation\n",
    "    nwalks = 5000; draws = np.random.randint(0, 2, size = (nwalks, nsteps))\n",
    "    walks = steps.cumsum(1) # axis-1\n",
    "    hits30 = (np.arbs(walks) >= 30).any(1) \n",
    "    crossing = (np.abs(walks[hits30] >= 30).argmax())\n",
    "    crossing.mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PANDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Pandas Trick\n",
    "\n",
    "### read_csv only few lines to check (nrows=5) and extract columns to list (df.columns.tolist()) and take needed later with (usecols = ['c1', 'c2', ...]) plus speed up by (dtype = {'c1': str, 'c2': int, ...})\n",
    "\n",
    "### select_dtypes \n",
    "\n",
    "```python\n",
    "coinbase_final.dtypes.value_counts()\n",
    "\n",
    "coinbase_final.select_dtypes(include=['object'])\n",
    "```\n",
    "\n",
    "### map by {'old': 'new}\n",
    "\n",
    "### value_counts (args = normalize, dropna, sort) ; with .reset_index() making result into df\n",
    "\n",
    "```python\n",
    "coinbase_final.date.value_counts()\n",
    "```\n",
    "\n",
    "### number of missing\n",
    "\n",
    "```python\n",
    "btcnews_final.isna().sum(axis=1)\n",
    "\n",
    "btcnews_final.isnull().sum(axis=1)\n",
    "```\n",
    "\n",
    "### select rows with specific values\n",
    "\n",
    "```python\n",
    "df_filter = df['ID'].isin(['A001'])\n",
    "df[df_filter]\n",
    "```\n",
    "\n",
    "### percentile groups\n",
    "\n",
    "```python\n",
    "cut_points = [np.percentile(df['c'], i) for i in [50, 80, 95]]\n",
    "df['group'] = 1\n",
    "for i in range(3):\n",
    "    df['group'] = df['group'] + (df['c'] < cut_points[i])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Basic\n",
    "- `Series()` and `series.values series.index`\n",
    "- `obj = Series( [4, 7, -5, 3], index = ['a', 'b', 'a', 'c'] )`\n",
    "- `Series( dict() )` auto-indexing\n",
    "- Missing:\n",
    "    - `pd.isnull()` `pd.notnull()` boolean\n",
    "\n",
    "```python\n",
    "    Series.name = \"name\"; Series.index.name = \"indexName\"\n",
    "    # DF as Dict of Series sharing same index\n",
    "    # 2D but multi-index can repr high-dimension !!\n",
    "    frame = DataFrame( dict() )\n",
    "    frame = DataFrame( np.array() ) # equi-length\n",
    "    orderedCol = DataFrame(data, columns=['year', 'etc'])\n",
    "    frame.columns; frame.index.name; frame.columns.name\n",
    "    frame.values; index = pd.index()\n",
    "```\n",
    "\n",
    "- Index Object !\n",
    "    - Index: Axis labels in NdArray or python\n",
    "    - Int64Index: integer values\n",
    "    - MultiIndex: Hierarchical similar to tuples\n",
    "    - DatetimeIndex: nanosecond timestamps\n",
    "    - PeriodIndex: timespans\n",
    "- Methods of DF object\n",
    "```python\n",
    "    append diff intersection union isin delete insert drop is_unique unique\n",
    "```\n",
    "\n",
    "- Reindexing\n",
    "```python\n",
    "    obj2 = obj.reindex(list()); obj.reindex(list(), fill_value=0)\n",
    "    , method = \"ffill\"\n",
    "    col = obj.reindex(columns = list())\n",
    "    # method\n",
    "    limit, level, copy\n",
    "```\n",
    "\n",
    "- Drop entries\n",
    "```python\n",
    "    obj.drop('c'); obj.drop('two', axis=1)\n",
    "    data[data['axis-0'] > 5]\n",
    "```\n",
    "\n",
    "- Functions\n",
    "```python\n",
    "    df1.add(df2, fill_value=0)\n",
    "    df1.reindex(columns=df2.columns, fill_value=0)\n",
    "    # NUMPY UFUNC WORKS WITH PANDAS OBJ\n",
    "    np.abs(frame) # element-wise\n",
    "    # APPLY\n",
    "    f = lambda x: x.max() - x.min()\n",
    "    frame.apply(f, axis=1)\n",
    "    def f(x): return Series([x.min(), x.max(), index=['min','max']])\n",
    "    frame.apply(f)\n",
    "    # APPLYMAP Series has map method for same purpose so renamed\n",
    "    format = lambda x: '%.2f' % x\n",
    "    frame.applymap(format)\n",
    "    frame['e'].map(format) # series map\n",
    "    # SORTING\n",
    "    obj.sort_index() # series\n",
    "    frame.sort_index(axis=1, ascending=False)\n",
    "    obj.order() # by values\n",
    "    frame.sort_index(by='b')\n",
    "    obj.index.is_unique # values\n",
    "    # METHODS of DF\n",
    "    count, argmin, quantile, mad, cusum, cumprod, diff, pct_change\n",
    "    .corr() .cov()\n",
    "    frame.corrwith(series)\n",
    "    series.value_counts()\n",
    "    data.apply(pd.value_counts).fillna(0)\n",
    "    # MISSING\n",
    "    np.nan data.isnull() .dropna(how='all')\n",
    "    df.fillna( {1:0.5, 3:-1} )\n",
    "    data.fillna(data.mean())\n",
    "    # HIERARCHICAL INDEXING\n",
    "    Series(data, index[ [level1], [level2] ])\n",
    "    data.unstack().stack()\n",
    "    sum(level = 'key2')\n",
    "    frame.swaplevel('key1','key2')\n",
    "    set_index(['c','d'], drop=False)\n",
    "    .reset_index()\n",
    "```\n",
    "\n",
    "- PANEL DATA\n",
    "```python\n",
    "    import pandas.io.data as web\n",
    "    pdata = pd.Panel(dict(stk, web.get_data_yahoo(stk, '1/1/2009', '6/1/2012')) for stk in ['APPL', 'GOOG', 'MSFT', 'DELL']))\n",
    "    pdata = pdata.swapaxes('items', 'minor')\n",
    "    pdata['Adj Close']\n",
    "    # ALternative repr panel is STACKED DF\n",
    "    stacked = pdata.loc[:, '5/30/2012':, :].to_frame()\n",
    "    stacked.to_panel()\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f4761fba1505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Multiple row and column selections using iloc and DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# first five rows of dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# first two columns of data frame with all rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 1st, 4th, 7th, 25th row + 1st 6th 7th columns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Multiple row and column selections using iloc and DataFrame\n",
    "data.iloc[0:5] # first five rows of dataframe\n",
    "data.iloc[:, 0:2] # first two columns of data frame with all rows\n",
    "data.iloc[[0,3,6,24], [0,5,6]] # 1st, 4th, 7th, 25th row + 1st 6th 7th columns.\n",
    "data.iloc[0:5, 5:8] # first 5 rows and 5th, 6th, 7th columns of data frame (county -> phone1).\n",
    "\n",
    "# setting index in df\n",
    "data.set_index(\"last_name\", inplace=True)\n",
    "# via loc method\n",
    "# Select same rows, with just 'first_name', 'address' and 'city' columns\n",
    "data.loc['Andrade':'Veness', ['first_name', 'address', 'city']]\n",
    "# boolean/logical indexing\n",
    "# Select rows with first name Antonio, # and all columns between 'city' and 'email'\n",
    "data.loc[data['first_name'] == 'Antonio', 'city':'email']\n",
    "# Select rows where the email column ends with 'hotmail.com', include all columns\n",
    "data.loc[data['email'].str.endswith(\"hotmail.com\")]   \n",
    "# Select rows with last_name equal to some values, all columns\n",
    "data.loc[data['first_name'].isin(['France', 'Tyisha', 'Eric'])]   \n",
    "# Select rows with first name Antonio AND hotmail email addresses\n",
    "data.loc[data['email'].str.endswith(\"gmail.com\") & (data['first_name'] == 'Antonio')] \n",
    "# select rows with id column between 100 and 200, and just return 'postal' and 'web' columns\n",
    "data.loc[(data['id'] > 100) & (data['id'] <= 200), ['postal', 'web']]\n",
    "\n",
    "# A lambda function that yields True/False values can also be used.\n",
    "# Select rows where the company name has 4 words in it.\n",
    "data.loc[data['company_name'].apply(lambda x: len(x.split(' ')) == 4)]  \n",
    "# Selections can be achieved outside of the main .loc for clarity:\n",
    "# Form a separate variable with your selections:\n",
    "idx = data['company_name'].apply(lambda x: len(x.split(' ')) == 4)\n",
    "# Select only the True values in 'idx' and only the 3 columns specified:\n",
    "data.loc[idx, ['email', 'first_name', 'company']]\n",
    "\n",
    "# Change the first name of all rows with an ID greater than 2000 to \"John\"\n",
    "data.loc[data['id'] > 2000, \"first_name\"] = \"John\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO SORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.ix[:, 'Capital']`\n",
    "\n",
    "`df.idmax()`\n",
    "\n",
    "`df.iteritems() # (Column-index, Series) pairs`\n",
    "\n",
    "`df.iterrows() # (Row-index, Series) pairs`\n",
    "\n",
    "**Advanced Indexing**\n",
    "\n",
    "```python\n",
    "Selecting\n",
    ">>> df3.loc[:,(df3>1).any()]\n",
    ">>> df3.loc[:,(df3>1).all()]\n",
    ">>> df3.loc[:,df3.isnull().any()]\n",
    ">>> df3.loc[:,df3.notnull().all()]\n",
    "Indexing With isin\n",
    ">>> df[(df.Country.isin(df2.Type))]\n",
    ">>> df3.filter(items=”a”,”b”])\n",
    ">>> df.select(lambda x: not x%5)\n",
    "Where\n",
    ">>> s.where(s > 0)\n",
    "Query\n",
    ">>> df6.query('second > first')\n",
    "\n",
    "# Transformation\n",
    ">>> customSum = lambda x: (x+x%2)\n",
    ">>> df4.groupby(level=0).transform(customSum)\n",
    "\n",
    "# datetime\n",
    ">>> df2['Date']= pd.to_datetime(df2['Date'])\n",
    ">>> df2['Date']= pd.date_range('2000-1-1',\n",
    "                               freq='M')\n",
    ">>> dates = [datetime(2012,5,1), datetime(2012,5,2)]\n",
    ">>> index = pd.DatetimeIndex(dates)\n",
    ">>> index = pd.date_range(datetime(2012,2,1), end, freq='BM')\n",
    "\n",
    "# dupes\n",
    ">>> df2.duplicated('Type')\n",
    ">>> df2.drop_duplicates('Type', keep='last')\n",
    ">>> df.index.duplicated()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  BUILDING DF from zero\n",
    "\n",
    "\n",
    "cities = ['Montreal', 'Toronto', 'Vancouver', 'Halifax', 'Ottawa']\n",
    "signups = [7, 12, 23, 5, 10]\n",
    "visitors = [139, 235, 333, 123, 49]\n",
    "weekdays = ['Sun', 'Mon', 'Tue', 'Wed', 'Thur']\n",
    "\n",
    "list_lables = ['city', 'visitors', 'signups',  'weekdays']\n",
    "list_cols = [cities, signups, visitors, weekdays]\n",
    "\n",
    "zipped = list(zip(list_lables, list_cols))\n",
    "    # list of tuples\n",
    "print(zipped)\n",
    "print(type(zip(list_lables, list_cols)))\n",
    "\n",
    "data = dict(zipped)\n",
    "print(data)\n",
    "\n",
    "users = pd.DataFrame(data)\n",
    "users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcasting (similar to map?)\n",
    "\n",
    "users['fees'] = 0\n",
    "# data.columns = ['newnames'...]\n",
    "# data.index = ['newindex'....]\n",
    "\n",
    "# CLEANING IN CSV IMPORTING\n",
    "\n",
    "pd.read_csv(filepath, header=None)\n",
    "    names = col_names_list # rename columns\n",
    "    na_values = 'missing' # assigning missing value\n",
    "    na_values = {'col1': ['missing', 'other'], 'col2':[....]} # specify each col missing value\n",
    "    parse_dates = [ [0, 1, 2] ] # parsing date indexs, e.g. 0 = year, 1 = month, 2 = day\n",
    "\n",
    "df.index.name = 'date' # renaming\n",
    "\n",
    "cols = [ ones to keep ]\n",
    "df = df[cols]\n",
    "\n",
    "# writing out CSV\n",
    "df.to_csv('filename', sep='\\t')\n",
    "df..to_excel('filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT WITH PANDAS\n",
    "appl = pd.read_csv(filepath, index_col='date', parse_dates = True)\n",
    "value = appl['close price'].values\n",
    "plt.pllot(value)\n",
    "\n",
    "# direct plot series without interacting with ndarray\n",
    "series = appl['close price']\n",
    "series.plot()\n",
    "appl.plot() # all col at once may result in distortion due to scale\n",
    "\n",
    "plt.yscale('log')) \n",
    "\n",
    "# some customising\n",
    "appl['open'].plot(color='b', style='.-', legend=True)\n",
    "plt.axis( ('2001', '2002', 0, 100) )\n",
    "\n",
    "# saving\n",
    "appl.loc['2001':'2004',['open','close','high']].plot()\n",
    "plt.savefig('appl.png')\n",
    "plt.savefig('appl.jpg')\n",
    "plt.savefig('appl.pdf')\n",
    "plt.show()\n",
    "\n",
    "df.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = pd.read_csv('seaborn-data/iris.csv')\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# various ways to plot hist\n",
    "df_iris.plot(kind='hist')\n",
    "df_iris.plot.hist()\n",
    "df_iris.hist()\n",
    "\n",
    "df_iris.plot(y='sepal_length', kind='hist', bins=30, range=(4,8), cumulative=True, normed=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another type of col-specified plot\n",
    "y_col = ['sepal_length', 'petal_length']\n",
    "df_iris.plot(x='sepal_width', y=y_col)\n",
    "\n",
    "df_iris[y_col].plot(kind='box', subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formating plots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1)\n",
    "df_iris['sepal_length'].plot(ax = axes[0], kind='hist', bins=30, normed=True)\n",
    "\n",
    "df_iris['petal_length'].plot(ax=axes[1], kind='hist', cumulative=True, normed=True, bins=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iris = df_iris.mean(axis=0)\n",
    "mean_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting category data separately\n",
    "indices = df_iris['species'] == 'setosa'\n",
    "setosa = df_iris.loc[indices, :]\n",
    "    # repeat for other cats...\n",
    "# check uniqueness\n",
    "setosa['species'].unique()\n",
    "df_iris['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-51f704337f50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtime_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%Y-%m-%d %H:%M'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmy_datetimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtime_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_datetimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Time Series\n",
    "\n",
    "time_format = '%Y-%m-%d %H:%M'\n",
    "\n",
    "my_datetimes = pd.to_datetime(date_list, format=time_format)\n",
    "\n",
    "time_series = pd.Series(temperature_list, index=my_datetimes)\n",
    "\n",
    "# extract date by time index\n",
    "ts3 = ts0.loc['2010-12-15' : '2010-12-31']\n",
    "\n",
    "ts4 = ts2.reindex(ts1.index, method='ffill')\n",
    "    # reindex using ts1's index, by forward filling missing (or bfill)\n",
    "    \n",
    "# RESAMPLING TIME SERIES\n",
    "df1 = df['Temperature'].resample('6h').mean()\n",
    "    # aggregation is must, e.g. mean\n",
    "\n",
    "df2 = df['Temperature'].resample('D').count()\n",
    "\n",
    "august = df['Temperature'].loc['2010-08']\n",
    "\n",
    "august_highs = august.resample('D').max()\n",
    "\n",
    "\n",
    "# ROLLING or SMOOTHING over aggregate chain method\n",
    "\n",
    "unsmoothed = df['Temperature']['2010-Aug-01':'2010-Aug-15']\n",
    "\n",
    "smoothed = unsmoothed.rolling(window=24).mean()\n",
    "    # rolling depends on date interval, here unit is hour\n",
    "\n",
    "august = pd.DataFrame( {'smoothed': smoothed, 'unsmoothed': unsmoothed})\n",
    "\n",
    "\n",
    "# WRANGLING TS in PANDAS\n",
    "\n",
    "sales['columns'].str.upper()\n",
    "sales['columns'].str.contains('strings')\n",
    "sales['columns'].str.contains('strings').sum()\n",
    "\n",
    "# datetime method\n",
    "\n",
    "sales['Date'].dt.hour\n",
    "\n",
    "sales['Date'].dt.tz_localize('US/Central').dt.tz_convert('US/Eastern')\n",
    "\n",
    "population.resample(\"A\").first()\n",
    "population.resample('A').first().interpolate('linear')\n",
    "    # interpoating missing value as linear\n",
    "\n",
    "# strip extra whitesapce from col names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# extract data for which the destination\n",
    "dallas = df['Destination Airport'].str.contains('DAL')\n",
    "\n",
    "daily_departures = dallas.resample('D').sum()\n",
    "\n",
    "stats = daily_departures.describe()\n",
    "\n",
    "ts2_interp = ts2.reindex(ts1.index).interpolate(how='linear')\n",
    "    # reset index of ts to ts1, filling missing using linear\n",
    "\n",
    "diff = np.abs(ts1-ts2_interp)\n",
    "\n",
    "\n",
    "# BOOLEAN MASK TO FILTER\n",
    "mask = df['Destination Airport'] == 'LAX'\n",
    "la = df[mask]\n",
    "# Combine two columns of data to create a datetime series: times_tz_none \n",
    "times_tz_none = pd.to_datetime( la['Date (MM/DD/YYYY)'] + ' ' + la['Wheels-off Time'] )\n",
    "# Localize the time to US/Central: times_tz_central\n",
    "times_tz_central = times_tz_none.dt.tz_localize('US/Central')\n",
    "# Convert the datetimes from US/Central to US/Pacific\n",
    "times_tz_pacific = times_tz_central.dt.tz_convert('US/Pacific')\n",
    "\n",
    "# Convert the 'Date' column into a collection of datetime objects: df.Date\n",
    "df.Date = pd.to_datetime(df['Date'])\n",
    "# Set the index to be the converted 'Date' column\n",
    "df.set_index('Date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 465)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m465\u001b[0m\n\u001b[0;31m    labels=[[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2], [5, 3, 4, 0, 1, 5, 3, 4, 0, 2, 5, 3, 4, 2, 1]],\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# READING AND CLEANING DATA\n",
    "\n",
    "column_lables_list = column_labels.split(',')\n",
    "\n",
    "df.columns = column_labels_list\n",
    "\n",
    "df_dropped = df.drop(list_to_drop), axis='columns')\n",
    "\n",
    "\n",
    "# Convert the date column to string: df_dropped['date']\n",
    "df_dropped['date'] = df_dropped['date'].astype(str)\n",
    "# Pad leading zeros to the Time column: df_dropped['Time']\n",
    "df_dropped['Time'] = df_dropped['Time'].apply(lambda x:'{:0>4}'.format(x))\n",
    "# Concatenate the new date and Time columns: date_string\n",
    "date_string = df_dropped['date'] + df_dropped['Time']\n",
    "# Convert the date_string Series to datetime: date_times\n",
    "date_times = pd.to_datetime(date_string, format='%Y%m%d%H%M')\n",
    "# Set the index to be the new date_times container: df_clean\n",
    "df_clean = df_dropped.set_index(date_times)\n",
    "\n",
    "\n",
    "\n",
    "# Downsample df_clean by day and aggregate by mean: daily_mean_2011\n",
    "daily_mean_2011 = df_clean.resample('D').mean()\n",
    "# Extract the dry_bulb_faren column from daily_mean_2011 using .values: daily_temp_2011\n",
    "daily_temp_2011 = daily_mean_2011['dry_bulb_faren'].values\n",
    "# Downsample df_climate by day and aggregate by mean: daily_climate\n",
    "daily_climate = df_climate.resample('D').mean()\n",
    "# Extract the Temperature column from daily_climate using .reset_index(): daily_temp_climate\n",
    "daily_temp_climate = daily_climate.reset_index()['Temperature']\n",
    "# Compute the difference between the two arrays and print the mean difference\n",
    "difference = daily_temp_2011 - daily_temp_climate\n",
    "print(difference.mean())\n",
    "\n",
    "# Select days that are sunny: sunny\n",
    "sunny = df_clean.loc[df_clean['sky_condition']=='CLR']\n",
    "# Select days that are overcast: overcast\n",
    "overcast = df_clean.loc[df_clean['sky_condition'].str.contains('OVC')]\n",
    "\n",
    "\n",
    "# INDEXING\n",
    "# via bracket and col as attribute and row label\n",
    "df['col']['row']\n",
    "df.col['row']\n",
    "# BUT formal index .loc accessor\n",
    "df.loc['row','col']\n",
    "# nested list for some cols\n",
    "df[['col1','col2']]\n",
    "\n",
    "# Slicing col as Series\n",
    "df['col'][1:4] # col then unit of series\n",
    "# subtle distinguish between Series and DF\n",
    "df['col'] # series\n",
    "df[['col']] # DF of 1 series\n",
    "\t# many method/attr shared but series is 1D\n",
    "\n",
    "# Slice the row labels 'Potter' to 'Perry' in reverse order: p_counties_rev\n",
    "p_counties_rev = election.loc['Potter':'Perry':-1,:]\n",
    "# using predefined rows and cols\n",
    "three_counties = election.loc[rows, cols]\n",
    "\n",
    "# filters\n",
    "df[(df.col1 >= 50) & (df.col2) < 200]\n",
    "\t# operand applies | !=\n",
    "# selec col with Nonzero or any\n",
    "df.loc[:, df.all()]\n",
    "df.loc[:, df.any()]\n",
    "# select col with any NaN\n",
    "df.loc[:, df.isnull().any()]\n",
    "# without NaN\n",
    "df.loc[:, df.notnull().all()]\n",
    "# drop rows with any NaN\n",
    "df.dropna(how='any') \n",
    "\t# any of the cols per row NA dropped, 'all' requires all cols per row\n",
    "# cond.filter and modify\n",
    "df.col1[df.col2 > 55] += 5\n",
    "\n",
    "# Create the boolean array: too_close\n",
    "too_close = election['margin'] < 1\n",
    "# Assign np.nan to the 'winner' column where the results were too close to call\n",
    "election['winner'][too_close]  = np.nan\n",
    "\n",
    "# Call .dropna() with thresh=1000 and axis='columns' and print the output of .info() from titanic\n",
    "print(titanic.dropna(thresh=1000, axis='columns').info())\n",
    "\n",
    "# TRANSFORM\n",
    "# vectorised func arithmetic\n",
    "df.floordiv(12)\n",
    "np.floor_div(df, 12)\n",
    "\n",
    "# plain function definnition\n",
    "def dozen_cal(n):\n",
    "\treturn n//12\n",
    "df.apply(dozen)\n",
    "# lambda\n",
    "df.apply(lambda n: n//12)\n",
    "# apply func for index\n",
    "df.index = df.index.map(str.lower)\n",
    "\n",
    "\n",
    "def to_celsius(F):\n",
    "    return 5/9*(F - 32)\n",
    "df_celsius = weather[['Mean TemperatureF', 'Mean Dew PointF']].apply(to_celsius)\n",
    "df_celsius.columns = ['Mean TemperatureC', 'Mean Dew PointC']\n",
    "\n",
    "# Create the dictionary: red_vs_blue\n",
    "red_vs_blue = {'Obama':'blue', 'Romney':'red'}\n",
    "# Use the dictionary to map the 'winner' column to the new column: election['color']\n",
    "election['color'] = election['winner'].map(red_vs_blue)\n",
    "\n",
    "# When efficacy is paramount, avoid .apply() and .map() for they're looping\n",
    "# Vectorised func can loop over equating compiled code (C, Fortran)\n",
    "# such come with NumPy, SciPy and pandas, namely Universal Function\n",
    "\n",
    "# importing zscore from scipy - z-score = #of std by which sample is above/below the mean\n",
    "# UFunc takes a pdSeries returning NumPyArray, be assigned\n",
    "\n",
    "from scipy.stats import zscore\n",
    "# Call zscore with election['turnout'] as input: turnout_zscore\n",
    "turnout_zscore = zscore(election['turnout'])\n",
    "election['turnout_zscore'] = turnout_zscore\n",
    "\n",
    "# INDEXING AND LABELLING\n",
    "# pandas Data Structures\n",
    "\t# Index: seq of labels\n",
    "\t\t# Immutable (like DICT keys)\n",
    "\t\t# Homogenous in type (like NP arrays)\n",
    "\t# Series: 1D array with Index\n",
    "\t# DF: 2D array with Series as columns sharing same Index\n",
    "prices = [...]\n",
    "shares = pd.Series(prices)\n",
    "\t# indexed 0...\n",
    "days = ['Mon',...]\n",
    "shares = pd.Series(prices, index=days)\n",
    "\t# indexed Mon...\n",
    "shares.index.name = 'weekday'\n",
    "# default read_csv as RangeIndex\n",
    "data.index = data['identifier']\n",
    "\t# use existing col as index\n",
    "\t# delete duplicate\n",
    "\t\tdel data['identifier']\n",
    "\n",
    "# Create the list of new indexes: new_idx\n",
    "new_idx = [i.upper() for i in sales.index]\n",
    "\t# pithy 'list comprehension' func\n",
    "# Assign new_idx to sales.index\n",
    "sales.index = new_idx\n",
    "\n",
    "# Labelling index and columns\n",
    "data.index.name = \"name\"\n",
    "data.columns.name = \"name\"\n",
    "\n",
    "# Setting MultiIndex by 'tuple', or multiple identifiers\n",
    "stocks = stocks.set_index(['Symbol', 'Date'])\n",
    "# sorting \n",
    "stocks = stocks.sort_index()\n",
    "# slicing MultiIndex\n",
    "stocks.loc[(['AAPL', 'MSFT'], '2016-10-05'), 'Close']\n",
    "# slicing both indexes\n",
    "stocks.loc[(slice(None), slice('2016-10-03', '2016-10-04')), :]\n",
    "\t# Using IndexSlice class for more intuitive code:\n",
    "\t# Create alias for pd.IndexSlice: idx\n",
    "\tidx = pd.IndexSlice\n",
    "\tdf.loc[idx[:, 'col1':'col2'], :]\n",
    "\t# accessing inner levels most trickiest\n",
    "# Look up data for all states in month 2: all_month2\n",
    "all_month2 = sales.loc[(slice(None), 2), :]\n",
    "\n",
    "\n",
    "# PIVOTING DF\n",
    "trials.pivot(index='treatment', columns='gender', values='response')\n",
    "    # if values=none, all variables will atuo\n",
    "# unstacking a multi-index by moving it to columns\n",
    "trials.unstack(level='gender')\n",
    "    # for use by pivot()\n",
    "    # returning\n",
    "        trails.stack()\n",
    "    # swapping level\n",
    "        swapped = stacked.swaplevel(0,1)\n",
    "        sorted_trials = swapped.sort_index()\n",
    "    # verify equality\n",
    "        df1.equals(df2)\n",
    "\n",
    "# MELT columns into rows\n",
    "pd.melt(df, id_vars=['col1'], var_name='col2', value_name='newname')\n",
    "# Non-UNIQUE (or duplicate) entries INDEX fails pivot(), instead use pivot_table() MUST with 'reduction', default = mean()\n",
    "trials.pivot_table(index=, columns=, values=,\n",
    "    aggfunc='count')\n",
    "    aggfunc=sum, margins=True \n",
    "        # obtaining sum with total\n",
    "\n",
    "\n",
    "# CATEGORICALS AND GROUPBY\n",
    "# groupby 'category' of columns\n",
    "sales.groupby('weekday').count()\n",
    "\t# split-apply-combine syntax including std(), sum(), first(),last(), etc\n",
    "sales.groupby('weekday')[['bread','butter']].sum()\n",
    "\t# further cueing on specific column(s)\n",
    "# multi-index\n",
    "sales.groupby(['city','weekday']).mean()\n",
    "# converting 'object' into 'category'\n",
    "sales['weekday'] = sales['weekday'].astype('category')\n",
    "\t# Pros: less RAM and speeds up operations like groupby()\n",
    "# Multi-aggregation\n",
    "sales.groupby('city')[['bread','butter']].agg(['max', 'sum'])\n",
    "\t# agg() also accept DEF func\n",
    "\t\tdef data_range(series):\n",
    "\t\t\treturn series.max() - series.min()\n",
    "\t\t.agg(data_range)\n",
    "\t# agg() on DICT\n",
    "\t\t.agg({'bread':'sum', 'butter':data_range})\n",
    "\t# agg() method in-effect add as new cols into the DF\n",
    "\tagg_sales.loc[:, ('bread','max')]\n",
    "\t\t# returns both cols\n",
    "\n",
    "# For multi-level index, unit level can be used to groupby, allowing advanced agg on levels across cols\n",
    "by_year_region = gapminder.groupby(level=['Year','region'])\n",
    "# On transformed index values, e.g. DateTimeIndex; extracting portions of datetime over which to group\n",
    "# 'is there a day of the week more popular?'\n",
    "# Create a groupby object: by_day\n",
    "by_day = sales.groupby(sales.index.strftime('%a'))\n",
    "\t# '.strftime('%a') to transform the \n",
    "units_sum = by_day['Units'].sum()\n",
    "\n",
    "# Groupby + Transformation\n",
    "auto.groupby('yr')['mpg'].transform(zscore).head()\n",
    "# Apply transformation + aggregation\n",
    "def zscore_yearName(group):\n",
    "\tdf = pd.DataFrame(\n",
    "\t\t{'mpg':zscore(group['mpg']),\n",
    "\t\t'year':group['yr'], 'name':group['name']})\n",
    "\treturn df\n",
    "# this new Trans/Agg too complicated for transformation method, hence apply()\n",
    "auto.groupby('yr').apply(zscore_yearName)\n",
    "\n",
    "# Import zscore\n",
    "from scipy.stats import zscore\n",
    "# Group gapminder_2010: standardized\n",
    "standardized = gapminder_2010.groupby('region')['life','fertility'].transform(zscore)\n",
    "# Construct a Boolean Series to identify outliers: outliers\n",
    "outliers = (standardized['life'] < -3) | (standardized['fertility'] > 3)\n",
    "# Filter gapminder_2010 by the outliers: gm_outliers\n",
    "gm_outliers = gapminder_2010.loc[outliers]\n",
    "\n",
    "# Filling missing data (IMPUTATION) by group\n",
    "# many ML packages cannot determine the best action facing missing data, PANDAS deals naturally like .dropna()\n",
    "# Imputing missing wisely always trumps dropping\n",
    "\n",
    "# Create a groupby object: by_sex_class\n",
    "by_sex_class = titanic.groupby(['sex','pclass'])\n",
    "# Write a function that imputes median\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "# Impute age and assign to titanic['age']\n",
    "titanic.age = by_sex_class['age'].transform(impute_median)\n",
    "\n",
    "# .apply() + groupby() performs arbitrary func (agg, transf, or more complex workflow), then combined intelligently\n",
    "\n",
    "# Filtering\n",
    "# groupby object 'DataFrameGroupBy' is a DICT, key=groupby\n",
    "# type(splitting.groups) = dict\n",
    "\t# splitting.groups.keys()\n",
    "# it's 'iterable' \n",
    "\tfor group_name, group in splitting:\n",
    "\t\tavg = group['mpg'].mean()\n",
    "\t\tprint(group_name, avg)\n",
    "# iteration and filter\n",
    "\t\tavg = group.loc[group['name'].str.contains('chevrolet'), 'mpg'].mean()\n",
    "# DICT COMPREHENSION\n",
    "chevy_means = {year:group.loc[group['name'].str.contains('chevrolet'),'mpg'].mean() for year, group in splitting}\n",
    "# convert this DICT into Series\n",
    "pd.Series(chevy_means)\n",
    "# boolean groupby\n",
    "chevy = auto['name'].str.contains('chevrolet')\n",
    "auto.groupby(['yr', chevy])['mpg'].mean()\n",
    "\n",
    "# Read the CSV file into a DataFrame: sales\n",
    "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
    "# Group sales by 'Company': by_company\n",
    "by_company = sales.groupby('Company')\n",
    "# Filter 'Units' where the sum is > 35: by_com_filt\n",
    "by_com_filt = by_company.filter(lambda g:g['Units'].sum()>35)\n",
    "\n",
    "# Filter + Grouping with .map()\n",
    "\t# instead grouping by a func/transformation of a column\n",
    "\t# key being Series is indexed as DF, mix and match column grouping with Series grouping\n",
    "# Create the Boolean Series: under10\n",
    "under10 = (titanic['age'] < 10).map({True:'under 10', False:'over 10'})\n",
    "# Group by under10 and pclass and compute the survival rate\n",
    "survived_mean_2 = titanic.groupby([under10,'pclass'])['survived'].mean()\n",
    "print(survived_mean_2)\n",
    "\n",
    "# CASE STUDY - Olympic\n",
    "\t# INDEXING / PIVOTING / GROUPBY / AGG/TRANSFORMATING/FILTERING\n",
    "# ?? Total number of medals won US per edition ??\n",
    "\tUSA_edition = medals.loc['Country' == 'USA'].groupby('Edition')\n",
    "\tUSA_edition['Medal'].count()\n",
    "\tseries1.value_count()\n",
    "\t# method in Series unique value\n",
    "# ?? distinct medals won per country ??\n",
    "\t# Construct the pivot table: counted\n",
    "\tcounted = medals.pivot_table(index='NOC',values='Athlete',columns='Medal',aggfunc='count')\n",
    "\t# Create the new column: counted['totals']\n",
    "\tcounted['totals'] = counted.sum(axis='columns')\n",
    "\t# Sort counted by the 'totals' column\n",
    "\tcounted = counted.sort_values('totals', ascending=False)\n",
    "# Find unique/categorical pairs among columns\n",
    "\t# extract subject columns\n",
    "\tcol_unique = df.drop_duplicates()\n",
    "# Exploring Errors with groupby.() 'gender and event_gender'\n",
    "\t# Group medals by the two columns: medals_by_gender\n",
    "\tmedals_by_gender = medals.groupby(['Event_gender','Gender'])\n",
    "\t# Create a DataFrame with a group count: medal_count_by_gender\n",
    "\tmedal_count_by_gender = medals_by_gender.count()\n",
    "# Locating suspicious data\n",
    "\t# Create the Boolean Series: sus\n",
    "\tsus = (medals.Event_gender == 'W') & (medals.Gender == 'Men')\n",
    "\t# Create a DataFrame with the suspicious row: suspect\n",
    "\tsuspect = medals[sus]\n",
    "# Alternative rankings\n",
    "\t# new methods: 'idxmax() - returns row/col position with max value', 'idxmin()' the opposite\n",
    "\t\t# options 'axis=0 by default across rows, axis=1 for cols'\n",
    "# ?? which country won most distinct sports ??\n",
    "\t# Group medals by 'NOC': country_grouped\n",
    "\tcountry_grouped = medals.groupby('NOC')\n",
    "\t# Compute the number of distinct sports in which each country won medals: Nsports\n",
    "\tNsports = country_grouped['Sport'].nunique()\n",
    "\t# Sort the values of Nsports in descending order\n",
    "\tNsports = Nsports.sort_values(ascending=False)\n",
    "# Intriguing, USSR not in top 5, now compare with US during period\n",
    "\t# Extract all rows for which the 'Edition' is between 1952 & 1988: during_cold_war\n",
    "\tduring_cold_war = (medals['Edition'] >= 1952) & (medals['Edition'] <= 1988)\n",
    "\t# Extract rows for which 'NOC' is either 'USA' or 'URS': is_usa_urs\n",
    "\tis_usa_urs = medals.NOC.isin(['USA','URS'])\n",
    "\t# Use during_cold_war and is_usa_urs to create the DataFrame: cold_war_medals\n",
    "\tcold_war_medals = medals.loc[during_cold_war & is_usa_urs]\n",
    "\t# Group cold_war_medals by 'NOC'\n",
    "\tcountry_grouped = cold_war_medals.groupby('NOC')\n",
    "\t# Create Nsports\n",
    "\tNsports = country_grouped['Sport'].nunique().sort_values(ascending=False)\n",
    "# ?? who won most consistently over Cold-war\n",
    "\t# Create the pivot table: medals_won_by_country\n",
    "\tmedals_won_by_country = medals.pivot_table(index='Edition',columns='NOC',values='Athlete', aggfunc='count')\n",
    "\t# Slice medals_won_by_country: cold_war_usa_usr_medals\n",
    "\tcold_war_usa_usr_medals = medals_won_by_country.loc[1952:1988, ['USA','URS']]\n",
    "\t# Create most_medals \n",
    "\tmost_medals = cold_war_usa_usr_medals.idxmax(axis='columns')\n",
    "\t# Print most_medals.value_counts()\n",
    "\tprint(most_medals.value_counts())\n",
    "# Visualising by matlibplot, BUT on single indexing, hence need for 'unstack()' multi-indexing DF\n",
    "\t# Create the DataFrame: usa\n",
    "\tusa = medals[medals['NOC'] == 'USA']\n",
    "\t# Group usa by ['Edition', 'Medal'] and aggregate over 'Athlete'\n",
    "\tusa_medals_by_year = usa.groupby(['Edition', 'Medal'])['Athlete'].count()\n",
    "\t# Reshape usa_medals_by_year by unstacking\n",
    "\tusa_medals_by_year = usa_medals_by_year.unstack(level='Medal')\n",
    "\t# Plot the DataFrame usa_medals_by_year\n",
    "\tusa_medals_by_year.plot()\n",
    "\tplt.show()\n",
    "# Better visualise medal trend by 'area' plot\n",
    "\tusa_medals_by_year.plot.area()\n",
    "# Notice 'medal' sorted by lexicography, now sort it by Categorical self-defined\n",
    "\t# Redefine 'Medal' as an ordered categorical\n",
    "\tmedals.Medal = pd.Categorical(values=medals.Medal, categories=['Bronze', 'Silver', 'Gold'], ordered=True)\n",
    "\n",
    "\n",
    "# MERGING WITH PANDAS\n",
    "\tpd.read_excel()\n",
    "\tpd.read_html()\n",
    "\tpd.json()\n",
    "# Reading DF from multiple files in loop (List of DataFrames)\n",
    "# Create the list of file names: filenames\n",
    "filenames = ['Gold.csv', 'Silver.csv', 'Bronze.csv']\n",
    "# Create the list of three DataFrames: dataframes\n",
    "dataframes = []\n",
    "for filename in filenames:\n",
    "    dataframes.append(pd.read_csv(filename))\n",
    "# Indexing\n",
    "# Read 'monthly_max_temp.csv' into a DataFrame: weather1\n",
    "weather1 = pd.read_csv('monthly_max_temp.csv', index_col='Month')\n",
    "# Sort the index of weather1 in alphabetical order: weather2\n",
    "weather2 = weather1.sort_index()\n",
    "# Sort the index of weather1 in reverse alphabetical order: weather3\n",
    "weather3 = weather1.sort_index(ascending=False)\n",
    "# Sort weather1 numerically using the values of 'Max TemperatureF': weather4\n",
    "weather4 = weather1.sort_values('Max TemperatureF')\n",
    "\n",
    "# Reindex weather1 using the list year: weather2\n",
    "weather2 = weather1.reindex(year)\n",
    "# Reindex weather1 using the list year with forward-fill: weather3\n",
    "weather3 = weather1.reindex(year).ffill()\n",
    "\n",
    "names_1981 = pd.read_csv('names1981.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "# Reindexing in fact extract the entire dataset matching the index, if by df.index\n",
    "common_names = names_1981.reindex(names_1881.index)\n",
    "\tcommon_names.dropna()\n",
    "\t# change in names\n",
    "\n",
    "# Arithmetic with Series and DF\n",
    "\t# Convert dollars to pounds: pounds\n",
    "\tpounds = dollars.multiply(exchange['GBP/USD'],axis='rows')\n",
    "\tdf.divide(df_mean, axis='rows')\n",
    "\tdf.pct_change() * 100\n",
    "# Simple +Sum of Series = UNION of Series with common index values, leaving non-common as NaN\n",
    "# Broadcasting arithmetics\n",
    "# Extract selected columns from weather as new DataFrame: temps_f\n",
    "\ttemps_f = weather.loc[:,['Min TemperatureF', 'Mean TemperatureF', 'Max TemperatureF']]\n",
    "\t# Convert temps_f to celsius: temps_c\n",
    "\ttemps_c = (temps_f - 32) * 5/9\n",
    "\t# Rename 'F' in column names with 'C': temps_c.columns\n",
    "\ttemps_c.columns = temps_c.columns.str.replace('F','C')\n",
    "# Resample() of datetime-indexed Series\n",
    "\t index = pd.date_range('1/1/2000', periods=9, freq='T') # 'T', among other time offset alises \n",
    "     series = pd.Series(range(9), index=index)\n",
    "     # downsample the series into 3-minute bins and sum the values\n",
    "     series.resample('3T').sum()\n",
    "# Example\n",
    "\t# Read 'GDP.csv' into a DataFrame: gdp\n",
    "\tgdp = pd.read_csv('GDP.csv',parse_dates=True,index_col='DATE')\n",
    "\t# Slice all the gdp data from 2008 onward: post2008\n",
    "\tpost2008 = gdp.loc[\"2008\":,:]\n",
    "\t# Resample post2008 by year, keeping last(): yearly\n",
    "\tyearly = post2008.resample('A').last()\n",
    "\t# Compute percentage growth of yearly: yearly['growth']\n",
    "\tyearly['growth'] = yearly.pct_change()*100\n",
    "\n",
    "# Appending & Concatenating Series\n",
    "series1.append() # only 'row-wise'\n",
    "df.concat([series1, series2, series3]) # both-wise\n",
    "\t# appended index duplicates, so if cols differ it creates UNION with NaN on unique col-values\n",
    "\tnew_east = northeast.append(south).reset_index(drop=True)\n",
    "\t# concat too dupicates index\n",
    "\tnew_east = pd.concat([northeast, south], ignore_index=True)\n",
    "# Slicing via .loc() with datetime index is flexible\n",
    "# index == '2015-02-26'\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "\t# any form works '2015 01 27', etc.\n",
    "# append VS Concat\n",
    "\t# Build the list of Series\n",
    "\tfor month in [jan, feb, mar]:\n",
    "\t    units.append(month['Units'])\n",
    "\t# Concatenate the list: quarter1\n",
    "\tquarter1 = pd.concat(units, axis='rows')\n",
    "# Building DF from multiples files\n",
    "\tfor medal in medal_types:\n",
    "\t    # Create the file name: file_name\n",
    "\t    file_name = \"%s_top5.csv\" % medal\n",
    "\t    # Create list of column names: columns\n",
    "\t    columns = ['Country', medal]\n",
    "\t\t# Read file_name into a DataFrame: df\n",
    "\t    medal_df = pd.read_csv(file_name, header=0, index_col='Country', names=columns)\n",
    "\t    # Append medal_df to medals\n",
    "\t    medals.append(medal_df)\n",
    "\t# Concatenate medals horizontally: medals\n",
    "\tmedals = pd.concat(medals, axis='columns')\n",
    "\n",
    "# Keys & MultiIndexes in Concat()\n",
    "# Concat() using multi-index on rows assigning unique outer-index\n",
    "\train1314 = pd.concat([rain2013, rain2014], keys=[2013,2014], axis=0)\n",
    "\t\t# as usal, outer-index can be sliced by .loc[]\n",
    "\t\t# simiarly for 'column'-wise, resulting in multi-column\n",
    "\t# in case of DICT, key-val auto assigned, 'axis=columns'\n",
    "# Recap on MultiIndex DF\n",
    "\tmedals.index # output:\n",
    "\tOut[1]: MultiIndex(levels=[['bronze', 'silver', 'gold'], ['France', 'Germany', 'Italy', 'Soviet Union', 'United Kingdom', 'United States']],\n",
    "           labels=[[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2], [5, 3, 4, 0, 1, 5, 3, 4, 0, 2, 5, 3, 4, 2, 1]],\n",
    "           names=[None, 'Country'])\n",
    "\t\t# entire index data is wrapped as pandas.indexes.multi.MultiIndex\n",
    "\t\t# with each part as LoL\n",
    "# Example\n",
    "\t# Sort the entries of medals: medals_sorted\n",
    "\tmedals_sorted = medals.sort_index(level=0)\n",
    "\t# Print the number of Bronze medals won by Germany\n",
    "\tprint(medals_sorted.loc[('bronze','Germany')])\n",
    "\t# Create alias for pd.IndexSlice: idx\n",
    "\tidx = pd.IndexSlice\n",
    "\t# Print all the data on medals won by the United Kingdom\n",
    "\tprint(medals_sorted.loc[idx[:,'United Kingdom'], :])\n",
    "# MultiIndexes + Multiindexed-Columns, e.g. 'Date'/'Tier_1_Col'/'Tier_2_Col'\n",
    "# Concatenate dataframes: february\n",
    "\tfebruary = pd.concat(dataframes, axis=1, keys=['Hardware', 'Software', 'Service'])\n",
    "\t# Assign pd.IndexSlice: idx\n",
    "\tidx = pd.IndexSlice\n",
    "\t# Create the slice: slice_2_8\n",
    "\tslice_2_8 = february.loc['2015 02 02':'2015 02 08', idx[:, 'Company']]\n",
    "# Example\n",
    "\t# Make the list of tuples: month_list\n",
    "\tmonth_list = [('january', jan), ('february', feb), ('march', mar)]\n",
    "\t# Create an empty dictionary: month_dict\n",
    "\tmonth_dict = {}\n",
    "\tfor month_name, month_data in month_list:\n",
    "\t    # Group month_data: month_dict[month_name]\n",
    "\t    month_dict[month_name] = month_data.groupby('Company').sum()\n",
    "\t# Concatenate data in month_dict: sales\n",
    "\tsales = pd.concat(month_dict)\n",
    "\t# Print all sales by Mediacore\n",
    "\tidx = pd.IndexSlice\n",
    "\tprint(sales.loc[idx[:, 'Mediacore'], :])\n",
    "\n",
    "# Outer & Inner Joins\n",
    "# Stacking arrays horizontally\n",
    "\tnp.hstack([B,A]) == np.concatenate([B,A], axis=1)\n",
    "\t\t# Rows must equal, but cols can vary\n",
    "# Stacking arrays vertically\n",
    "\tnp.vstack([A,C]) == np.concatenate([A,C], axis=0)\n",
    "\t\t# Cols must equal, rows can differ\n",
    "# OUTER JOIN: Union of index sets (all labels, no repititaion), Missing fields as NaN\n",
    "# INNER JOIN: Intersection of index sets (only common labels)\n",
    "\tpd.concat([population, unemployment], join='inner', axis=0)\n",
    "\t\t# Empty, as no COMMON COLUMN index label\n",
    "# GDP Growth between US and China\n",
    "\t# Resample and tidy china: china_annual\n",
    "\tchina_annual = china.resample('A').pct_change(10).dropna()\n",
    "\t# Resample and tidy us: us_annual\n",
    "\tus_annual = us.resample('A').pct_change(10).dropna()\n",
    "\t# Concatenate china_annual and us_annual: gdp\n",
    "\tgdp = pd.concat([china_annual,us_annual], join='inner', axis=1)\n",
    "\t# Resample gdp and print\n",
    "\tprint(gdp.resample('10A').last())\n",
    "\n",
    "\n",
    "# MERGING DF (extending Concat() to align rows with multi-columns)\n",
    "# Default merge() 'INNER' joins columns with common key-column\n",
    "\t# Nil if no exact columns match\n",
    "\t# Instead, merge() single column \n",
    "\t\tpd.merge(bronze, gold, on='NOC') # appending columns to the right\n",
    "\t\tpd.merge(bronze, gold, on=['NOC', 'Country']) # allowing matching of multiple columns only\n",
    "\t# Change column label with suffixes=[]\n",
    "\t\tsuffixes=['_bronze', '_gold'] # adding to original label\n",
    "\t# If col-labels differ, must declare lables\n",
    "\t\tpd.merge(counties, cities, left_on='CITY NAME', right_on='City')\n",
    "\t\t\t# differing columns joined on are retained after merge\n",
    "# Merge(how='left or right'): retaining rest columns from left or right DF as NaN if missing\n",
    "# Merge(how='outer'): Union of rest of columns as NaN if missing\n",
    "\n",
    "# JOIN .join(how='left')\n",
    "\tpopulation.join(employment) # Default 'left' join by Index (i.e. population), missing filled as NaN\n",
    "\t\t# otherwise for how='right'\n",
    "\n",
    "\t# also for how='inner'/'outer'\n",
    "# Which to use ? - the simplest that works\n",
    "\t# append - stacking vertically only\n",
    "\t# concat - stacking many both-way + simple inner/outer on INDEX only\n",
    "\t# join - inner/outer/left/right on INDEXes only\n",
    "\t# merge - many joins on multiple columns, most powerful, NOT on INDEX\n",
    "sales_and_managers = pd.merge(sales, managers, how='left', left_on=['city','state'],  right_on=['branch', 'state'])\n",
    "\n",
    "# Order Merge: merge_ordered() : basically an ORDERED OUTER JOIN on common columns\n",
    "\t# similar args to merge()\n",
    "\tpd.merge_ordered(hardware, software, on=['Date','Company'], suffixes=['_hardware','_software'])\n",
    "\t# filling NaN, not helpful when beginning entries are NaN\n",
    "\tpd.merge_ordered(stocks, gdp, on='Date', fill_method='ffill')\n",
    "# merge_asof() : merge values in order using the 'on' column, but for each row in DF, only rows from the right DF\n",
    "\t# whose 'on' column values are LESS than the left value will be kept. Often used to align disparate datetime\n",
    "\t# frequencies without having to first resample\n",
    "\t\t# Merge auto and oil: merged\n",
    "\t\tmerged = pd.merge_asof(auto, oil, left_on='yr', right_on='Date')\n",
    "\t\t# Resample merged: yearly\n",
    "\t\tyearly = merged.resample('A',on='Date')[['mpg','Price']].mean()\n",
    "\n",
    "\n",
    "# CASE STUDY: Does host-country win more?\n",
    "# Loading DF\n",
    "\t# Create file path: file_path\n",
    "\tfile_path = 'Summer Olympic medallists 1896 to 2008 - EDITIONS.tsv'\n",
    "\t# Load DataFrame from file_path: editions\n",
    "\teditions = pd.read_csv(file_path, sep='\\t')\n",
    "\t# Extract the relevant columns: editions\n",
    "\teditions = editions[['Edition', 'Grand Total', 'City',  'Country']]\n",
    "# Building medals DF\n",
    "\t# Create empty dictionary: medals_dict\n",
    "\tmedals_dict = {}\n",
    "\tfor year in editions['Edition']:\n",
    "\t    # Create the file path: file_path\n",
    "\t    file_path = 'summer_{:d}.csv'.format(year)\n",
    "\t    # Load file_path into a DataFrame: medals_dict[year]\n",
    "\t    medals_dict[year] = pd.read_csv(file_path)\n",
    "\t    # Extract relevant columns: medals_dict[year]\n",
    "\t    medals_dict[year] = medals_dict[year][['Athlete', 'NOC', 'Medal']]\n",
    "\t    # Assign year to column 'Edition' of medals_dict\n",
    "\t    medals_dict[year]['Edition'] = year   \n",
    "\t# Concatenate medals_dict: medals\n",
    "\tmedals = pd.concat(medals_dict, ignore_index=True)\n",
    "# Counting Medals by Country per Edition\n",
    "\tmedal_counts = medals.pivot_table(index='Edition', values='Athlete', columns='NOC', aggfunc='count')\n",
    "# Counting Medals as Fraction of Total per Edition\n",
    "\t# Set Index of editions: totals\n",
    "\ttotals = editions.set_index('Edition')\n",
    "\t# Reassign totals['Grand Total']: totals\n",
    "\ttotals = totals['Grand Total']\n",
    "\t# Divide medal_counts by totals: fractions\n",
    "\tfractions = medal_counts.divide(totals, axis='rows')\n",
    "# Percentage change\n",
    "\t# Apply the expanding mean: mean_fractions\n",
    "\tmean_fractions = fractions.expanding().mean()\n",
    "\t# Compute the percentage change: fractions_change\n",
    "\tfractions_change = mean_fractions.pct_change()*100\n",
    "\t# Reset the index of fractions_change: fractions_change\n",
    "\tfractions_change = fractions_change.reset_index()\n",
    "# Reshape data\n",
    "\t# Left join editions and ioc_codes: hosts\n",
    "\thosts = pd.merge(editions, ioc_codes, how='left')\n",
    "\t# Extract relevant columns and set index: hosts\n",
    "\thosts = hosts[['Edition','NOC']].set_index('Edition')\n",
    "\t# Fix missing 'NOC' values of hosts\n",
    "\tprint(hosts.loc[hosts.NOC.isnull()])\n",
    "\thosts.loc[1972, 'NOC'] = 'FRG'\n",
    "\thosts.loc[1980, 'NOC'] = 'URS'\n",
    "\thosts.loc[1988, 'NOC'] = 'KOR'\n",
    "\t# Reset Index of hosts: hosts\n",
    "\thosts=hosts.reset_index()\n",
    "\t# Reshape fractions_change: reshaped\n",
    "\treshaped = pd.melt(fractions_change, id_vars='Edition', value_name='Change')\n",
    "\t# Extract rows from reshaped where 'NOC' == 'CHN': chn\n",
    "\tchn = reshaped.loc[reshaped['NOC']=='CHN']\n",
    "\t# Merge reshaped and hosts: merged\n",
    "\tmerged = pd.merge(reshaped, hosts, how='inner')\n",
    "\t# Set Index of merged and sort it: influence\n",
    "\tinfluence = merged.set_index('Edition').sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='NOTEFR.txt' mode='r' encoding='UTF-8'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_io.TextIOWrapper"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Note Mémoire\\n\\nthis researcher has a lot of knowledge in his field\\n\\t\\tCe chercheur a beaucoup de connaissance dans son domaine\\nI would like to inform you of a mistake \\n\\t\\tJe voudrais te signaler une erreur \\nWho is the target group of this product\\n\\t\\tQui est le groupe cible de ce produit\\nThis behaviour is not normal\\n\\t\\tCe comportement n’est pas normal\\nThe workers are on strike\\n\\t\\tLes ouvriers sont en grève\\nThere’s a pile of books on the table\\n\\t\\tIl y a un tas de livres sur la table\\nShe’s part of the human rights association\\n\\t\\tElle fait partie de la ligue des droits de l’homme\\nThe demonstration lasted 3 days\\n\\t\\tles manifestations ont duré 3 jours\\nThey have eaten a third of the cake\\n\\t\\tIls ont mangé un tiers du gâteau\\nThere are thousands of inhabitants in this town\\n\\t\\tIl y a des milliers d’inhabitants dans cette ville\\nThe police commissioner was replaced by a younger man\\n\\t\\tLe préfet de police a été remplacé par un homme plus jeune \\nThe production of goods and services has increased\\n\\t\\tLa production '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "des biens et services a augmenté\n",
      "Put your arms around my neck\n",
      "\t\tpassez vos bras autour mon cou\n",
      "I have to dash\n",
      "\t\tJe dois filer\n",
      "This tree is rotten inside\n",
      "\t\tCet arbre est pourri à l’intérieur\n",
      "Take off your shoes\n",
      "\t\tEnlève tes chaussures\n",
      "We are halfway there\n",
      "\t\tOn a fait la moitié du chemin\n",
      "If I could, I would do an easier job\n",
      "\t\tSi je pouvais, je ferais un travail plus facile\n",
      "This book is no longer topical\n",
      "\t\tCe livre n’est plus d’actualité\n",
      "playground\n",
      "\t\tLe terrain du jeu\n",
      "gunshots\n",
      "\t\tdes coups de feu\n",
      "The elected members discuss the big problems of society\n",
      "\t\tLes élus discutent des grands problèmes de société\n",
      "This screening takes two hours\n",
      "\t\tCette séance de cinéma dure deux heures\n",
      "He has got problems with the police\n",
      "\t\tIl a des ennuis avec la police\n",
      "This will attract him problems\n",
      "\t\tCela lui attirera des ennuis\n",
      "There is a lot of competition in this field\n",
      "\t\tIl y a beaucoup de concurrence dans ce domaine\n",
      "Clear your bag\n",
      "\t\tDégage ton sac\n",
      "you are wrong\n",
      "\t\tTu as tort (- tu as raison)\n",
      "I live by a very wid\n"
     ]
    }
   ],
   "source": [
    "# open file\n",
    "file_note = open('NOTEFR.txt', 'r')\n",
    "print(file_note)\n",
    "type(file_note)\n",
    "\n",
    "file_note.read(1000)\n",
    "print(file_note.read(1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note Mémoire\n",
      "\n",
      "\n",
      "\n",
      "this researcher has a lot of knowledge in his field\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "file_note.close()\n",
    "\n",
    "with open('NOTEFR.txt') as file_note:\n",
    "    print(file_note.readline())\n",
    "    print(file_note.readline())\n",
    "    print(file_note.readline())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-40b33d87ff19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'NOTEFR.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata_notefr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin)\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m                 \u001b[0mline_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m                 \u001b[0mline_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# importing FLAT files by numpy\n",
    "\n",
    "\n",
    "filename = 'NOTEFR.txt'\n",
    "\n",
    "data_notefr = np.loadtxt(filename, delimiter='\\t', skiprows=5, usecols=[0,2], dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fname' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-6b72443176ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecfromcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# auto-import poly-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fname' is not defined"
     ]
    }
   ],
   "source": [
    "np.genfromtxt(fname, dtype=None)\n",
    "np.recfromcsv(fname, dtype=None)\n",
    "    # auto-import poly-type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-bb5feebec4db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DF TO NumArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file' is not defined"
     ]
    }
   ],
   "source": [
    "# DF TO NumArray\n",
    "data =pd.read_csv(file, nrows=5, header=None)\n",
    "data_array = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'titanic_corrupt.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-997d80b6f959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'titanic_corrupt.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Import file: data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Nothing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Plot 'Age' variable in a histogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'titanic_corrupt.txt' does not exist"
     ]
    }
   ],
   "source": [
    "# case\n",
    "# Assign filename: file\n",
    "file = 'titanic_corrupt.txt'\n",
    "# Import file: data\n",
    "data = pd.read_csv(file, sep='\\t', comment='#', na_values='Nothing')\n",
    "# Plot 'Age' variable in a histogram\n",
    "pd.DataFrame.hist(data[['Age']])\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pickled files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'filename.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-79f51eb97371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filename.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Excel file via pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'filename.xlsx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'filename.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('filename.pkl', 'rb') as file:\n",
    "\tdata = pickle.load(file)\n",
    "# Excel file via pandas\n",
    "file = 'filename.xlsx'\n",
    "data = pd.ExcelFile(file)\n",
    "print(data.sheet_names)\n",
    "df1 = data.parse('1960-1966')\n",
    "\t# sheet name as string\n",
    "df2 = data.parse(0)\n",
    "\t# sheet index as float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DC_Stat_Thinking',\n",
       " 'STATS_PROBA.ipynb',\n",
       " '.DS_Store',\n",
       " 'trump_lies.csv',\n",
       " 'MATPLOTLIB.ipynb',\n",
       " 'test.csv',\n",
       " 'BASIC.ipynb',\n",
       " 'intro2stats',\n",
       " 'seaborn-data',\n",
       " 'cereal.csv',\n",
       " 'REPO_Python',\n",
       " 'WRANGLE.ipynb',\n",
       " 'train.csv',\n",
       " '.ipynb_checkpoints',\n",
       " 'Web-scrapping Trump Lies.ipynb',\n",
       " 'NOTEFR.txt',\n",
       " 'Jupyter_BASIC.ipynb',\n",
       " 'ModSimPy-master']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "wd = os.getcwd()\n",
    "\n",
    "os.listdir(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the first sheet and rename the columns: df1\n",
    "df1 = xl.parse(0, skiprows=[0], names=['Country','AAM due to War (2002)'])\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df2 = xl.parse(1, parse_cols=[0], skiprows=[1], names=['Country'])\n",
    "\t# KEY val passed must be list []\n",
    "\n",
    "# SAS files\n",
    "# Import sas7bdat package\n",
    "from sas7bdat import SAS7BDAT\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "\n",
    "# STATA\n",
    "# Load Stata file into a pandas DataFrame: df\n",
    "df = pd.read_stata('disarea.dta')\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n",
    "# Plot histogram of one column of the DataFrame\n",
    "pd.DataFrame.hist(df[['disa10']])\n",
    "plt.xlabel('Extent of disease')\n",
    "plt.ylabel('Number of coutries')\n",
    "\n",
    "# HDF5 files 'Hierachical Data Format version 5'\n",
    "# standard for storing big numerical up to exabytes\n",
    "import h5py\n",
    "filename = 'filename.hdf5'\n",
    "data = h5py.File(filename, 'r')\n",
    "for key in data.keys():\n",
    "\tprint(key)\n",
    "\t# meta, quality, strain\n",
    "\n",
    "# MatLab\n",
    "import scipy.io\n",
    "# Load MATLAB file: mat\n",
    "mat = scipy.io.loadmat('albeck_gene_expression.mat')\n",
    "print(mat.keys())\n",
    "print(np.shape(mat['CYratioCyt']))\n",
    "\n",
    "# Relational DB (PostgreSQL, MySQL, SQLite)\n",
    "SQLite\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "# Save the table names to a list: table_names\n",
    "table_names = engine.table_names()\n",
    "con = engine.connect()\n",
    "rs = con.execute(\"SELECT * FROM Orders\")\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "df.columns = rs.keys()\n",
    "con.close()\n",
    "# OR save troube of closing via Context Manager\n",
    "with engine.connect() as con:\n",
    "\trs = con.execute(\"SELECT Order ID, OrderDate, ShipName FROM Orders\")\n",
    "\tdf = pd.DataFrame(rs.fetchmany(size=5))\n",
    "\tdf.columns = rs.keys()\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Employee WHERE EmployeeID => 6\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "# direct query\n",
    "df = pd.read_sql_query(\"SELECT * FROM Orders\", engine)\n",
    "# check identity\n",
    "print(df.equals(df1))\n",
    "# joining\n",
    "\"INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\"\n",
    "\n",
    "# Accessing online files\n",
    "from urllib.request import urlretrieve\n",
    "url = \"http://....csv\"\n",
    "urlretrieve(url, 'filename.csv')\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('filename.csv', sep=';')\n",
    "print(df.head())\n",
    "\n",
    "# read via pandas without saving\n",
    "# skip urlretrieve()\n",
    "\n",
    "# online excel\n",
    "url = 'filename.xls'\n",
    "xl = pd.read_excel(url, sheetname=None)\n",
    "print(xl.keys())\n",
    "print(xl['sheetname'].head())\n",
    "\n",
    "# GET requests using urllib (slow)\n",
    "from urllib.request import urlopen, request\n",
    "url = 'https://www.wikipedia.org/'\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "html = response.read()\n",
    "response.close()\n",
    "\n",
    "# GET via Requests lib (fast)\n",
    "import requests\n",
    "url = '...'\n",
    "r = requests.get(url)\n",
    "text = r.text\n",
    "\n",
    "# Scraping the web\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://..'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\t# Prettified Soup\n",
    "print(soup.title)\n",
    "print(soup.get_text())\n",
    "for link in soup.find_all('a'):\n",
    "\t# all the hyperlinks\n",
    "\tprint(link.get('href'))\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# API and JSON (JS Object Notation)\n",
    "# human-readable, key-val pairs, need for real-time server2browser comm\n",
    "import json\n",
    "with open('snake.json','r') as json_file:\n",
    "\tjson_data = json.load(json_file)\n",
    "\t# type = Dict\n",
    "for key, val in json_data.items():\n",
    "\tprint(key+':',val)\n",
    "# or\n",
    "for k in json_data.keys():\n",
    "\tprint(k + ': ', json_data[k])\n",
    "\n",
    "json_data = r.json() # built-in\n",
    "# API syntax\n",
    "# http request, www.omdbapi.com - OMDB API addr., '?' = query string\n",
    "# the rest is based on specific documentation\n",
    "'http://www.omdbapi.com/?t=hackers' # title = hackers\n",
    "import requests\n",
    "url = 'http://www.omdbapi.com/?apikey=ff21610b&t=social+network'\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])\n",
    "# wiki\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "# DoD\n",
    "\n",
    "# Tweeter API (Public Stream)\n",
    "import tweepy, json\n",
    "access_token = '...'\n",
    "access_token_secret = '...'\n",
    "consumer_key = '...'\n",
    "consumer_secret = '...'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Tweet listener return 'tweets.txt' collecting streams as json for 100 tweets\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "\tdef __init__(self, api=None):\n",
    "\t\tsuper(MyStreamListener, self).__init__()\n",
    "\t\tself.num_tweets = 0\n",
    "\t\tself.file = open(\"tweets.txt\", 'w')\n",
    "\n",
    "\tdef on_status(self, status):\n",
    "\t\ttweet = status._json\n",
    "\t\tself.file.write(json.dumps(tweet) + '\\n')\n",
    "\t\tself.num_tweets += 1\n",
    "\t\tif self.num_tweets < 100:\n",
    "\t\t\treturn True \n",
    "\t\telse:\n",
    "\t\t\treturn False\n",
    "\t\tself.file.close()\n",
    "\n",
    "\tdef on_error(self,status):\n",
    "\t\tprint(status)\n",
    "\n",
    "# create streaming object and auth\n",
    "l = MyStreamListener()\n",
    "stream = tweepy.Stream(auth, l)\n",
    "# this line filters Streams to capture data by keywords:\n",
    "stream.filter(track=['apples','oranges'])\n",
    "\n",
    "# read the loaded txt\n",
    "tweets_data_path = 'tweets.txt'\n",
    "tweets_data = []\n",
    "# open connection to file\n",
    "tweets_file = open(tweets_data_path, 'r')\n",
    "# read in tweets and store in list\n",
    "for line in tweets_file:\n",
    "\ttweet = json.loads(line)\n",
    "\ttweets_data.append(tweet)\n",
    "tweets_file.close()\n",
    "# print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())\n",
    "\n",
    "# now tweets_data is a LoD containing tweets and features\n",
    "# to convert into DF \n",
    "# feature 'text' and 'lang'\n",
    "df = pd.DataFrame(tweets_data, columns=['text','lang'])\n",
    "\n",
    "# analysis of words\n",
    "import re\n",
    "def word_in_text(word, tweet):\n",
    "\tword = word.lower()\n",
    "\ttext = tweet.lower()\n",
    "\tmatch = re.search(word,tweet)\n",
    "\n",
    "\tif match:\n",
    "\t\treturn True\n",
    "\treturn False\n",
    " Initialize list to store tweet counts\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    clinton += word_in_text('clinton', row['text'])\n",
    "    trump += word_in_text('trump', row['text'])\n",
    "    sanders += word_in_text('sanders', row['text'])\n",
    "    cruz += word_in_text('cruz', row['text'])\n",
    "# Set seaborn style\n",
    "sns.set(color_codes=True)\n",
    "# Create a list of labels:cd\n",
    "cd = ['clinton', 'trump', 'sanders', 'cruz']\n",
    "# Plot histogram\n",
    "ax = sns.barplot(cd, [clinton, trump,sanders,cruz])\n",
    "ax.set(ylabel=\"count\")\n",
    "plt.show()\n",
    "\n",
    "# recap\n",
    "# importing txt, flat, others\n",
    "# SQL queries + RDB\n",
    "# scrapping web data via API/JSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLEAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'mfr', 'type', 'calories', 'protein', 'fat', 'sodium', 'fiber',\n",
       "       'carbo', 'sugars', 'potass', 'vitamins', 'shelf', 'weight', 'cups',\n",
       "       'rating'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77 entries, 0 to 76\n",
      "Data columns (total 16 columns):\n",
      "name        77 non-null object\n",
      "mfr         77 non-null object\n",
      "type        77 non-null object\n",
      "calories    77 non-null int64\n",
      "protein     77 non-null int64\n",
      "fat         77 non-null int64\n",
      "sodium      77 non-null int64\n",
      "fiber       77 non-null float64\n",
      "carbo       77 non-null float64\n",
      "sugars      77 non-null int64\n",
      "potass      77 non-null int64\n",
      "vitamins    77 non-null int64\n",
      "shelf       77 non-null int64\n",
      "weight      77 non-null float64\n",
      "cups        77 non-null float64\n",
      "rating      77 non-null float64\n",
      "dtypes: float64(5), int64(8), object(3)\n",
      "memory usage: 9.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77, 16)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mfr</th>\n",
       "      <th>type</th>\n",
       "      <th>calories</th>\n",
       "      <th>protein</th>\n",
       "      <th>fat</th>\n",
       "      <th>sodium</th>\n",
       "      <th>fiber</th>\n",
       "      <th>carbo</th>\n",
       "      <th>sugars</th>\n",
       "      <th>potass</th>\n",
       "      <th>vitamins</th>\n",
       "      <th>shelf</th>\n",
       "      <th>weight</th>\n",
       "      <th>cups</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100% Bran</td>\n",
       "      <td>N</td>\n",
       "      <td>C</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>280</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>68.402973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100% Natural Bran</td>\n",
       "      <td>Q</td>\n",
       "      <td>C</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>33.983679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All-Bran</td>\n",
       "      <td>K</td>\n",
       "      <td>C</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>320</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>59.425505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All-Bran with Extra Fiber</td>\n",
       "      <td>K</td>\n",
       "      <td>C</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>330</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>93.704912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Almond Delight</td>\n",
       "      <td>R</td>\n",
       "      <td>C</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>34.384843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name mfr type  calories  protein  fat  sodium  fiber  \\\n",
       "0                  100% Bran   N    C        70        4    1     130   10.0   \n",
       "1          100% Natural Bran   Q    C       120        3    5      15    2.0   \n",
       "2                   All-Bran   K    C        70        4    1     260    9.0   \n",
       "3  All-Bran with Extra Fiber   K    C        50        4    0     140   14.0   \n",
       "4             Almond Delight   R    C       110        2    2     200    1.0   \n",
       "\n",
       "   carbo  sugars  potass  vitamins  shelf  weight  cups     rating  \n",
       "0    5.0       6     280        25      3     1.0  0.33  68.402973  \n",
       "1    8.0       8     135         0      3     1.0  1.00  33.983679  \n",
       "2    7.0       5     320        25      3     1.0  0.33  59.425505  \n",
       "3    8.0       0     330        25      3     1.0  0.50  93.704912  \n",
       "4   14.0       8      -1        25      3     1.0  0.75  34.384843  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mfr</th>\n",
       "      <th>type</th>\n",
       "      <th>calories</th>\n",
       "      <th>protein</th>\n",
       "      <th>fat</th>\n",
       "      <th>sodium</th>\n",
       "      <th>fiber</th>\n",
       "      <th>carbo</th>\n",
       "      <th>sugars</th>\n",
       "      <th>potass</th>\n",
       "      <th>vitamins</th>\n",
       "      <th>shelf</th>\n",
       "      <th>weight</th>\n",
       "      <th>cups</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Triples</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>39.106174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Trix</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>27.753301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Wheat Chex</td>\n",
       "      <td>R</td>\n",
       "      <td>C</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3</td>\n",
       "      <td>115</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>49.787445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Wheaties</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>51.592193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Wheaties Honey Gold</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>36.187559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name mfr type  calories  protein  fat  sodium  fiber  \\\n",
       "72              Triples   G    C       110        2    1     250    0.0   \n",
       "73                 Trix   G    C       110        1    1     140    0.0   \n",
       "74           Wheat Chex   R    C       100        3    1     230    3.0   \n",
       "75             Wheaties   G    C       100        3    1     200    3.0   \n",
       "76  Wheaties Honey Gold   G    C       110        2    1     200    1.0   \n",
       "\n",
       "    carbo  sugars  potass  vitamins  shelf  weight  cups     rating  \n",
       "72   21.0       3      60        25      3     1.0  0.75  39.106174  \n",
       "73   13.0      12      25        25      2     1.0  1.00  27.753301  \n",
       "74   17.0       3     115        25      1     1.0  0.67  49.787445  \n",
       "75   17.0       3     110        25      1     1.0  1.00  51.592193  \n",
       "76   16.0       8      60        25      1     1.0  0.75  36.187559  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cereal.csv')\n",
    "df.columns\n",
    "df.info()\n",
    "df.shape\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Golden Grahams                            1\n",
       "Corn Chex                                 1\n",
       "Fruit & Fibre Dates; Walnuts; and Oats    1\n",
       "Shredded Wheat                            1\n",
       "Crispy Wheat & Raisins                    1\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name'].value_counts(dropna=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calories</th>\n",
       "      <th>protein</th>\n",
       "      <th>fat</th>\n",
       "      <th>sodium</th>\n",
       "      <th>fiber</th>\n",
       "      <th>carbo</th>\n",
       "      <th>sugars</th>\n",
       "      <th>potass</th>\n",
       "      <th>vitamins</th>\n",
       "      <th>shelf</th>\n",
       "      <th>weight</th>\n",
       "      <th>cups</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>106.883117</td>\n",
       "      <td>2.545455</td>\n",
       "      <td>1.012987</td>\n",
       "      <td>159.675325</td>\n",
       "      <td>2.151948</td>\n",
       "      <td>14.597403</td>\n",
       "      <td>6.922078</td>\n",
       "      <td>96.077922</td>\n",
       "      <td>28.246753</td>\n",
       "      <td>2.207792</td>\n",
       "      <td>1.029610</td>\n",
       "      <td>0.821039</td>\n",
       "      <td>42.665705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.484119</td>\n",
       "      <td>1.094790</td>\n",
       "      <td>1.006473</td>\n",
       "      <td>83.832295</td>\n",
       "      <td>2.383364</td>\n",
       "      <td>4.278956</td>\n",
       "      <td>4.444885</td>\n",
       "      <td>71.286813</td>\n",
       "      <td>22.342523</td>\n",
       "      <td>0.832524</td>\n",
       "      <td>0.150477</td>\n",
       "      <td>0.232716</td>\n",
       "      <td>14.047289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>18.042851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>33.174094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>110.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>40.400208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>110.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.828392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>93.704912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         calories    protein        fat      sodium      fiber      carbo  \\\n",
       "count   77.000000  77.000000  77.000000   77.000000  77.000000  77.000000   \n",
       "mean   106.883117   2.545455   1.012987  159.675325   2.151948  14.597403   \n",
       "std     19.484119   1.094790   1.006473   83.832295   2.383364   4.278956   \n",
       "min     50.000000   1.000000   0.000000    0.000000   0.000000  -1.000000   \n",
       "25%    100.000000   2.000000   0.000000  130.000000   1.000000  12.000000   \n",
       "50%    110.000000   3.000000   1.000000  180.000000   2.000000  14.000000   \n",
       "75%    110.000000   3.000000   2.000000  210.000000   3.000000  17.000000   \n",
       "max    160.000000   6.000000   5.000000  320.000000  14.000000  23.000000   \n",
       "\n",
       "          sugars      potass    vitamins      shelf     weight       cups  \\\n",
       "count  77.000000   77.000000   77.000000  77.000000  77.000000  77.000000   \n",
       "mean    6.922078   96.077922   28.246753   2.207792   1.029610   0.821039   \n",
       "std     4.444885   71.286813   22.342523   0.832524   0.150477   0.232716   \n",
       "min    -1.000000   -1.000000    0.000000   1.000000   0.500000   0.250000   \n",
       "25%     3.000000   40.000000   25.000000   1.000000   1.000000   0.670000   \n",
       "50%     7.000000   90.000000   25.000000   2.000000   1.000000   0.750000   \n",
       "75%    11.000000  120.000000   25.000000   3.000000   1.000000   1.000000   \n",
       "max    15.000000  330.000000  100.000000   3.000000   1.500000   1.500000   \n",
       "\n",
       "          rating  \n",
       "count  77.000000  \n",
       "mean   42.665705  \n",
       "std    14.047289  \n",
       "min    18.042851  \n",
       "25%    33.174094  \n",
       "50%    40.400208  \n",
       "75%    50.828392  \n",
       "max    93.704912  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf.shelf.plot(kind='hist', rot=70, logx=True, logy=True)\\ndf.boxplot(column='calories', by='mfr', rot=90)\\ndf.plot(kind='scatter', x=...)\\n\\nplt.xlim(20, 55)\\nplt.ylim(20, 55)\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df.shelf.plot(kind='hist', rot=70, logx=True, logy=True)\n",
    "df.boxplot(column='calories', by='mfr', rot=90)\n",
    "df.plot(kind='scatter', x=...)\n",
    "\n",
    "plt.xlim(20, 55)\n",
    "plt.ylim(20, 55)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRINCIPLES OF TIDY DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-73f032a1bcc1>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-73f032a1bcc1>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    Pivot - un-melting data\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# MELTING into variable-oriented columns\n",
    "pd.melt(frame=df, id_vars='name', \n",
    "\tvalue_vars=['treatment a', 'treatment b'],\n",
    "\tvar_name='treatment', value_name='result')\n",
    " Pivot - un-melting data\n",
    "weather_tidy = weather.pivot(index='date',\n",
    "\tcolumns='element', values='value')\n",
    "# this method cannot handle 'duplicate'\n",
    "# instead, pivot table mehtod\n",
    "weather2_tidy = weather.pivot_table(values='value',\n",
    "\tindex='date', columns='element', \n",
    "\taggfunc=np.mean)\n",
    "# turn this pivot table form into original\n",
    "print(airquality_pivot.index)\n",
    "\t# Multi-index DF resulted from pivot_table\n",
    "# reseting\n",
    "airquality_pivot = airquality_pivot.reset_index()\n",
    "\n",
    "# KEY, reporting data NOT analysing data\n",
    "# DA requires features = INDE-EXOGENEITY, hence no levels of the same feature\n",
    "# Melt tb: tb_melt\n",
    "tb_melt = pd.melt(frame=tb, id_vars=['country', 'year'])\n",
    "# Create the 'gender' column\n",
    "tb_melt['gender'] = tb_melt.variable.str[0]\n",
    "# Create the 'age_group' column\n",
    "tb_melt['age_group'] = tb_melt.variable.str[1:]\n",
    "\n",
    "# parsing delimited columns\n",
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')\n",
    "# Create the 'str_split' column\n",
    "ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')\n",
    "# Create the 'type' column\n",
    "ebola_melt['type'] = ebola_melt.str_split.str.get(0)\n",
    "# Create the 'country' column\n",
    "ebola_melt['country'] = ebola_melt.str_split.str.get(1)\n",
    "\t# 2 intermediate steps\n",
    "\n",
    "# concatenating data from subsets\n",
    "conc = pd.concat([dat1,dat2], ignore_index=True)\n",
    "\t# axis=0 'row-wise', axis=1 'column-wise'\n",
    "# concat multiple files\n",
    "import glob\n",
    "csv_files = glob.glob('*.csv')\n",
    "\t# wildcard ending .csv\n",
    "\t# saved as list\n",
    "list_data = []\n",
    "for filename in csv_files:\n",
    "\tdata = pd.read_csv(filename)\n",
    "\tlist_data.append(data)\n",
    "\t# list of DF\n",
    "pd.concat(list_data)\n",
    "\t# concat that list into single DF\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = '*.csv'\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "# Load the second file into a DataFrame: csv2\n",
    "csv2 = pd.read_csv(csv_files[1])\n",
    "# Create an empty list: frames\n",
    "frames = []\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)\n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)\n",
    "\n",
    "# Merge = INNER JOIN in SQL\n",
    "pd.merge(left=df1, right=df2, on=None,\n",
    "\tleft_on='key_col', right_on='key_col1')\n",
    "\t# on=None as both keys differ\n",
    "# Many2One - multiple entries per key\n",
    "# same code, different DF results\n",
    "print(m2m.head(20))\n",
    "\n",
    "# Converting dtype\n",
    "df['feature'] = df['feature'].astype(str)\n",
    "df['feature'] = df['feature'].astype('category')\n",
    "# cleaning numeric loaded as string due to non-num entries\n",
    "df['numeric'] = pd.to_numeric(df['feature'], errors='coerce')\n",
    "\n",
    "\t# converting categorical data into 'category' saves RAM and utility in libraries\n",
    "\n",
    "# clean strings by Regular Expression\n",
    "import re\n",
    "pattern = re.compile('\\$\\d*\\.\\d{2}')\n",
    "result = pattern.match('$17.89')\n",
    "prog = re.compile('\\d{3}-\\d{3}-\\d{4}')\n",
    "result = prog.match('123-456-7890')\n",
    "print(bool(result))\n",
    "\n",
    "# extracting numerical from string\n",
    "# Find the numeric values: matches\n",
    "matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana')\n",
    "\t# '+' set finding multiple digits repeatable\n",
    "# Print the matches\n",
    "print(matches)\n",
    "\n",
    "pattern1 = bool(re.match(pattern='\\d{3}-\\d{3}-\\d{4}', string='123-456-7890'))\n",
    "print(pattern1)\n",
    "pattern2 = bool(re.match(pattern='\\$\\d*\\.\\d{2}', string='$123.45'))\n",
    "print(pattern2)\n",
    "pattern3 = bool(re.match(pattern='[A-Z]\\w*', string='Australia'))\n",
    "print(pattern3)\n",
    "\n",
    "# complex cleaning via func\n",
    "# apply func\n",
    "df.apply(np.mean, axis=0)\n",
    "\t# 0 = byCol, 1 = byRow\n",
    "\n",
    "# Define recode_sex()\n",
    "def recode_sex(sex_value):\n",
    "    # Return 1 if sex_value is 'Male'\n",
    "    if sex_value == 'Male':\n",
    "        return 1\n",
    "    # Return 0 if sex_value is 'Female'    \n",
    "    elif sex_value == 'Female':\n",
    "        return 0\n",
    "    # Return np.nan    \n",
    "    else:\n",
    "        return np.nan\n",
    "# Apply the function to the sex column\n",
    "tips['sex_recode'] = tips.sex.apply(recode_sex)\n",
    "\n",
    "# Write the lambda function using replace\n",
    "tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))\n",
    "# Write the lambda function using regular expressions\n",
    "tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])\n",
    "\n",
    "\n",
    "# Dupicate and Missing Value\n",
    "\n",
    "# Create the new DataFrame: tracks\n",
    "tracks = billboard[['year','artist','track','time']]\n",
    "# Drop the duplicates: tracks_no_duplicates\n",
    "tracks_no_duplicates = tracks.drop_duplicates()\n",
    "\n",
    "# Calculate the mean of the Ozone column: oz_mean\n",
    "oz_mean = airquality['Ozone'].mean()\n",
    "# Replace all the missing values in the Ozone column with the mean\n",
    "airquality['Ozone'] = airquality['Ozone'].fillna(oz_mean)\n",
    "tips_dropped = tips_nan.dropna()\n",
    "\n",
    "# Check missing value with assert\n",
    "assert google['Close'].notnull().all()\n",
    "\t# chaining methods\n",
    "# fill missing value\n",
    "google_0 = google.fillna(value=0)\n",
    "\n",
    "# Assert that there are no missing values\n",
    "assert pd.notnull(ebola).all().all()\n",
    "# Assert that all values are >= 0\n",
    "assert (ebola >= 0).all().all()\n",
    "\n",
    "\n",
    "# All-in-all check func\n",
    "def check_null_or_valid(row_data):\n",
    "\t\"\"\"takes a row, drops missing, check rest num>=0 \"\"\"\n",
    "\tno_na = row_data.dropna()[1:-1]\n",
    "\t\t# assuming [0] is categorical data\n",
    "\tnumeric = pd.to_numeric(no_na)\n",
    "\tge0 = numeric >= 0\n",
    "\treturn ge0\n",
    "# check column \n",
    "assert g1800s.columns[0] == 'Life expectancy'\n",
    "# check val in rows valid\n",
    "assert g1800s.iloc[:,1:].apply(check_null_or_valid, axis=1).all().all()\n",
    "# check singleton of country\n",
    "assert g1800s['Life expectancy'].value_counts()[0] == 1\n",
    "\t# [0] specifies .value_counts() will contain the most frequently occuring value\n",
    "\t# for the list is sorted(desc), and [0] takes 1st value\n",
    "\n",
    "# save csv\n",
    "df.to_csv['filename.csv']\n",
    "\n",
    "# Convert the year column to numeric\n",
    "gapminder.year = pd.to_numeric(gapminder.year)\n",
    "# Test if country is of type object\n",
    "assert gapminder.country.dtypes == np.object\n",
    "# Test if year is of type int64\n",
    "assert gapminder.year.dtypes == np.int64\n",
    "# Test if life_expectancy is of type float64\n",
    "assert gapminder.life_expectancy.dtypes == np.float64\n",
    "\n",
    "# text matching\n",
    "# Create the series of countries: countries\n",
    "countries = gapminder['country']\n",
    "# Drop all the duplicates from countries\n",
    "countries = countries.drop_duplicates()\n",
    "# Write the regular expression: pattern\n",
    "pattern = '^[A-Za-z\\.\\s]*$'\n",
    "\t# ^ begins $ ends = EXACT MATCH\n",
    "# Create the Boolean vector: mask\n",
    "mask = countries.str.contains(pattern)\n",
    "# Invert the mask: mask_inverse\n",
    "mask_inverse = ~mask\n",
    "# Subset countries using mask_inverse: invalid_countries\n",
    "invalid_countries = countries.loc[mask_inverse]\n",
    "\n",
    "# visualising\n",
    "# Add first subplot (segmenting tableau)\n",
    "plt.subplot(2, 1, 1) # positioning\n",
    "# Create a histogram of life_expectancy\n",
    "gapminder.life_expectancy.plot(kind='hist')\n",
    "# Group gapminder: gapminder_agg\n",
    "gapminder_agg = gapminder.groupby('year')['life_expectancy'].mean()\n",
    "# Add second subplot\n",
    "plt.subplot(2, 1, 2)\n",
    "# Create a line plot of life expectancy per year\n",
    "gapminder_agg.plot()\n",
    "# Add title and specify axis labels\n",
    "plt.title('Life expectancy over the years')\n",
    "plt.ylabel('Life expectancy')\n",
    "plt.xlabel('Year')\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Save both DataFrames to csv files\n",
    "gapminder.to_csv('gapminder.csv')\n",
    "gapminder_agg.to_csv('gapminder_agg.csv')\n",
    "S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File IO\n",
    "\n",
    "- Read/Write Text\n",
    "```python\n",
    "    read_csv, read_table '\\t', read_fwf, read_clipboard\n",
    "    header=None, names=['columns'], index_col=list()\n",
    "    skiprows = list(), sep = '\\s\\t', na_values=['NULL']\n",
    "    parse_dates\n",
    "    date_parse = function, nrows, chunksize, encoding\n",
    "    # saving\n",
    "    .to_csv(filename) or sys.stdout, sep='|', na_rep='NULL', index=False, header=False\n",
    "    date = pd.date_range('1/1/2000', periods=7)\n",
    "    Series(np.arange(7), index=dates)\n",
    "    Series.from_csv(filename, parse_date=true)\n",
    "```\n",
    "\n",
    "- Read_CSV/READ_TABLE ARGUMENTS\n",
    "    - path / sep / header / index_col / names (list of column names with header=None)\n",
    "    - skiprows / na_values (sequence of values to repalce wtih NA) / comment (char to split)\n",
    "    - parse_dates (attempt to parse data to datetime; False default, If True, parse all columns. Else can specify a list of column numers or name; if element of list is tuple or list, will combine multiple columns and parse e.g. if date/time split across two columns)\n",
    "    - keep_date_col (if joining columns to parse date, drop the joined columns)\n",
    "    - converters (dict having col number of name mapping to fuc, {'foo': f} apply f to all values in 'foo' col)\n",
    "    - dayfirst (ambiguous dates, treat as international format)\n",
    "    - date_parser (fuc to use parse dates)\n",
    "    - nrows (number of rows to read from start)\n",
    "    - iterator (return a TextParser obj for reading file piecemeal)\n",
    "    - chunksize (for iteration, size of file chunks)\n",
    "    - skip_footer (lines to ignore at end) / verbose / encoding / squeeze (if parsed data only one col return a Series / thousands (sep for 000 , or .)\n",
    "\n",
    "```python\n",
    "    chunker = pd.read_csv(filename, chunksize=1000)\n",
    "    tot = Series([])\n",
    "    for piece in chunker:\n",
    "        tot = tot.add(piece['key'].value_counts(), fill_value=0)\n",
    "    tot = tot.order(ascending=False)\n",
    "\n",
    "    # WRITE\n",
    "    data.to_csv(filename or sys.stdout, sep='|', na_rep='NULL', index=False, header=False, cols=['a', 'c'])\n",
    "    dates = pd.date_range('1/1/2000', periods=7)\n",
    "    ts = Series(np.arange(7), index=dates)\n",
    "    ts.to_csv(filename)\n",
    "    # with wrangling, no header, 1st col as index, can read CSV of Series with read_csv but also from_csv easier\n",
    "    Series.from_csv(filename, parse_dates=True)\n",
    "    # Manual reading\n",
    "    import csv # open file\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        print line\n",
    "    lines = list(csv.reader(f))\n",
    "    header, values = lines[0], lines[1:]\n",
    "    data_dict = {h: v for h, v in zip(header, zip(*values))}\n",
    "    # CSV reader defined\n",
    "    class my_dialect(csv.Dialect):\n",
    "        lineterminator = '\\n'\n",
    "        delimitor = ';'\n",
    "        quotechar = '\"'\n",
    "    reader = csv.reader(f, dialect=my_dialect) \n",
    "    reader = csv.reader(f, delimiter='|') # override\n",
    "    # CSV dialect options\n",
    "    delimiter, lineterminator, quotechar, quoting, skiinitialspace, doublequote, escapechar\n",
    "    # JSON\n",
    "    import json\n",
    "    result = json.loads(obj)\n",
    "    asjson = json.dumps(result) # python to JSON\n",
    "    siblings = DataFrame(result['siblings'], columns=['names', 'age']) # JSON to DF\n",
    "    # Binary pickle serialisation\n",
    "    frame.save(pickle); pd.load(pickle)\n",
    "    # HDF5 binart form API : PyTables and h5py, pandas uses PyTables to store pandas obj\n",
    "    sotre = pd.HDFStore('mydata.h5')\n",
    "    sotre['obj1'] = frame\n",
    "    sotre['obj1_col'] = frame['a']\n",
    "    # many problems are IO-bound (not CPU-bound) HDF5 massively accelerating, used for WRITE-ONCE, USE-MANY\n",
    "    # EXCEL\n",
    "    xls_file = pd.ExcelFile('data.xls')\n",
    "    table = xls_file('Sheet1') # sheet to DF\n",
    "    # HTTP API\n",
    "    import requests\n",
    "    resp = requests.get(url)\n",
    "    data = json.loads(resp.text)\n",
    "    data.keys() # each as DICT\n",
    "    fields = ['col1', 'col2']\n",
    "    df = DataFrame(data['results'], columns = fields)\n",
    "    # DB\n",
    "    import sqlite3\n",
    "    query = \"\"\" CREATE TABLE test (a VARCHAR(20), b VARCHAR(20), c REAL, d INTEGER);\"\"\"\n",
    "    con = sqlite3.connect(':memory:')\n",
    "    con.execute(query)\n",
    "    con.commit()\n",
    "    data = [ ('Atlanta', 'Georgia', 1.25, 6), ... ]\n",
    "    stmt = \"INSERT INTO test VALUES(?, ?, ?, ?)\n",
    "    con.executemany(stmt, data)\n",
    "    con.commit()\n",
    "    cursor = con.execute('select * from test') # return list of tuples \n",
    "    rows = cursor.fetchall()\n",
    "    DataFrame(rows, columns=zip(*cursor.description)[0]) # important\n",
    "    import pandas.io.sql as sql # saving processing code\n",
    "    sql.read_frame('select * from test', con)\n",
    "    # MongoDB dict-like obj basic unit\n",
    "    import pymongo\n",
    "    con = pymongo.Connection('localhost', port=27017)\n",
    "    tweets = con.db.tweets # making collections\n",
    "    data = json.loads(requests.get(url).text)\n",
    "    for tweet in data['results']:\n",
    "        tweets.save(tweet)\n",
    "    cursor = tweets.find( {'from_user': 'wesmckinn'} ) # get from collection query\n",
    "    tweet_fields = ['created_at', 'from_user', 'id', 'text']\n",
    "    result = DateFrame(list(cursor), columns=tweet_fileds)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling (clean, transform, merge, reshape)\n",
    "- Combining / Merging\n",
    "```python\n",
    "    pandas.merge # connect rows per key\n",
    "    pandas.concat # glues / stacks obj along axis\n",
    "    combine_first # splicing overlapping data to fill missing\n",
    "```\n",
    "\n",
    "- DB style Merges\n",
    "```python\n",
    "    df1 = DataFrame('key': ['vaues'], 'data1': range(7)})\n",
    "    df2 # range 3\n",
    "    # Many to One merge\n",
    "    pd.merge(df1, df2) # many take over with 0 filled\n",
    "    pd.merge(df1, df2, on='key') # overlapping col as keys \n",
    "    df3 # different columns\n",
    "    pd.merge(df3, df4, left_on='lkey', right_on='rkey') # retaining both\n",
    "    pd.merge(df1, df2, how='outer') # default is INNER, OUTER is UNION\n",
    "    # Many to Many joins Cartesian product of rows, only affects distinct key\n",
    "    pd.merge(left, right, on=['key1', 'key2'], how='outer')\n",
    "    # Whick ey to join? depending on choice of merge method, think of multiple keys as forming an array of tuples used as single join key, though not actually done so\n",
    "    # joining col-col indexes on passed DF are discarded !!\n",
    "    pd.merge(left, right, on='key1', suffixes=('_left', '_right'))\n",
    "```\n",
    "\n",
    "- Merging Index\n",
    "```python\n",
    "    pd.merge(left1, right1, left_on='key', right_index=True) # index used as merge key\n",
    "    left2.join(right2, how='outer') # easy instance for joining by index \n",
    "```\n",
    "\n",
    "- Concatenating Along Axis\n",
    "```python\n",
    "    np.concatenate( [arr, arr], axis=1 ) # NumPy done on arrays\n",
    "    # pandas object havinglabeled axes enable further generalise array concat - if indexed differently on other axes, should collection axes union or intersected? do groups need ID in resul? does concat axis matter at all?\n",
    "    pd.concat( [s1, s2, s3] ) # glues values and indexes\n",
    "    pd.concat([s1, s4], axis=1, join='inner') # if no overlap on other axis (from sorted union of 'outer'), can intersect\n",
    "    pd.concat([s1, s4], axis=1, join_axes=[ ['a','c'] ])\n",
    "    # instead for hierarchical index on axis\n",
    "    result = pd.concat([s1, s2, s3], keys=['one','two','three']) # each retain own index\n",
    "    result.unstack() # combined and fill NaN\n",
    "    [objs, axis, join, keys, levels, names, verify_integratiy, ignore_index]\n",
    "```\n",
    "\n",
    "- Combining Overlaps\n",
    "```python\n",
    "    b[:-2].combine_first(a[2:]) # col by col pacthing missing data \n",
    "    # reshaping and pivoting\n",
    "    # STACK - ROTATES or pivot from col to row\n",
    "    # UNSTACK - pivot from rows to col\n",
    "    # DEFAULT innermost level is unstacked or stacked, different level arg\n",
    "    result.unstack(0); result.unstakc('column')\n",
    "    data2.unstack().stack(dropna=False) # keeping nan\n",
    "    # LONG or Stacked format to store multi-time series\n",
    "    # datetime often as PRIMARY KEY in DB, BUT not easy to work in LONG format, might prefer DF having one col per distinct ITEM value by timestamps in DATE col, hence PIVOT method \n",
    "    pivoted = longdata.pivot('date', 'item', 'value') # first two args are columns as ROW and COL INDEX, last optional arg to FILL DF; omitting VALUE will get HIERARCHIAL COLUMNS of values cols\n",
    "    # Note: pivot = easy creating hierarchical index using set_index + unstack\n",
    "    unstacked = longdata.set_index(['date', 'item']).unstack('item')\n",
    "```\n",
    "\n",
    "- Data Transformation\n",
    "```python\n",
    "    # Removing Dupe\n",
    "    df.duplicated() # boolean Series of ROW-wise dupe test\n",
    "    df.drop_duplicates() # dropping True above\n",
    "    df.drop_duplicates( ['col1', 'col2'], take_last=True) # return last one\n",
    "    # Func or Mapping\n",
    "    df['animal'] = df['food'].map(str.lower).map(meat_to_animal) # \n",
    "    df['food'].map(lambda x: meat_to_animal[x.lower()]) # equivalent\n",
    "    # Replacing value\n",
    "    df.replace(-999, np.nan)\n",
    "    df.repalce([-999, -1000], [np.nan, o]) # different per value\n",
    "    df.repalce( {-999 : np.nan, -1000 : 0} ) # or as dict\n",
    "    # Renameing Axis Indexes\n",
    "    df.index = df.index.map(str.upper) # need in-place mod\n",
    "    df.rename(index = str.title, columns = str.upper) # without mod original via rename\n",
    "    df.rename(index = {'OHIO' : 'INDIANA'}, columns = {'three' : 'peekaboo'}) # dict of values, !! rename saves having to copy DF manually and assign to its index, should mod data inplace\n",
    "    _ = df.rename(..., inplace=True) # mod original\n",
    "    # Discretizaiton and Binning\n",
    "    df_binned = pd.cut(df, bins) # bins = [10, 20, ... ] as CATEGORICAL special dtype\n",
    "    df_binned.labels ; df_binned.levels\n",
    "    pd.value_count(df_binned) # useful\n",
    "    pd.cut(df, bins, labels=group_names, right=False) # define group names per bin and () is exc and [] inc\n",
    "    pd.cut(df, 4, precision = 2) # cut integer # of bins up to 2-digit\n",
    "    pd.qcut(df, 4) # cut into quartiles\n",
    "    pd.qcut(df, [0, 0.1, 0.5, 0.9, 0.1]) # self-defined quantiles o and 1 inclusive\n",
    "    # Detecing and Filtering Outliers\n",
    "    df[(np.abs(df) > 100).any(1)] # select all rows exceeding 100\n",
    "    df[np.abs(df) > 3] = np.sign(data) * 3 \n",
    "    # Permutation and Random Sampling\n",
    "    sampler = np.random.permutation(5)\n",
    "    # Dummy\n",
    "    pd.get_dummies(df['cat'], prefix = 'key')\n",
    "    df_dummied = df[ ['df1'] ].join(dummies)\n",
    "    pd.get_dummies(pd.cut(values, bins)) # useful\n",
    "    # String Manuipulation\n",
    "    [count, endswith, startswith, join, index, find, rfind, repalce, strip, rstrip, lstrip, split, lower, ljust]\n",
    "```\n",
    "- GROUP and REDUCTION\n",
    "```python\n",
    "    df['data1'].groupby([df['key1'], df['key2']]).mean()\n",
    "    hier_df.groupby(level = 'cty', axis=1).count() # group by index levels\n",
    "    grouped['data1'].quantile(0.9)\n",
    "    grouped.agg(reduction_Func) # very useful !\n",
    "    people.groupby(key).transform(np.mean) # note use of transform and np.mean method\n",
    "    df.groupby('smoker').apply(func) # func applied to each piece of DF, then glued by concat, labeling pieces with group names !! hence hierarchical index whose inner level contains index values from original F\n",
    "    df.groupby(['smoker', 'day']).apply(func, n=1, column='total_bill') # specify arguments\n",
    "    # Examples\n",
    "    result = df.groupby('smoker')['col'].describe() \n",
    "    result.unstack('smoker') # transpose index and col !!\n",
    "    df.groupby('smoker', group_keys=False).apply(lambda x: x.max()) # disable hierarchical index from group keys along with indexes of each piece\n",
    "    df.col1.groupby(pd.cut(df.data1, 4)).apply(func).unstack() # cool?\n",
    "    group_key = ['East'] * 4 + ['West'] * 4; data[['some sstaetes']] = np.nan; \n",
    "        data.groupby(group_key).mean() or .apply(lambda g: g.fillna(g.mean());\n",
    "```\n",
    "                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2018, 11, 16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(926, 56700)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2010, 12, 14, 0, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('2011-01-03 00:00:00', '2011-01-03')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2011, 1, 3, 0, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2011, 7, 6, 0, 0), datetime.datetime(2011, 8, 6, 0, 0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2011, 1, 3, 0, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1997, 1, 31, 22, 45)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2011, 12, 6, 0, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2011-07-06', '2011-08-06'], dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2011-07-06', '2011-08-06', 'NaT'], dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "NaT"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([False, False,  True])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now();\n",
    "now.year, now.month, now.day # sotres both date and time down to microsecond\n",
    "# temporal diff\n",
    "delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8 ,15)\n",
    "delta.days, delta.seconds\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "datetime(2011, 1, 7) - 2 * timedelta(12) \n",
    "\n",
    "# Converting String and Datetime !\n",
    "    # pandas use Timestamp obj \n",
    "stamp = datetime(2011, 1, 3)\n",
    "str(stamp), stamp.strftime('%Y-%m-%d')\n",
    "\n",
    "value = '2011-01-03'\n",
    "datetime.strptime(value, '%Y-%m-%d')\n",
    "\n",
    "datestrs = ['7/6/2011', '8/6/2011']\n",
    "[datetime.strptime(x, '%m/%d/%Y') for x in datestrs]\n",
    "\n",
    "# Formatted as strings using str or strftime with spec\n",
    "\n",
    "# Same format codes can be sued to convert strings to dates using datetime.strptime\n",
    "\n",
    "# DATETIME.STRPTIME is the best way to parse date with known format. BUT annoying to write spec, so \n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "parse('2011-01-03')\n",
    "\n",
    "parse('Jan 31, 1997 10:45 PM')\n",
    "\n",
    "parse('6/12/2011', dayfirst=True)\n",
    "\n",
    "# PANDAS is generally geared towards arrays of dates, soit axis index soit column in DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.to_datetime(datestrs) # parse many kinds of date quickly\n",
    "\n",
    "idx = pd.to_datetime(datestrs + [None]) # hanndle missing\n",
    "idx\n",
    "idx[2]\n",
    "pd.isnull(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime forsmat specs\n",
    "- %Y  4-digit year\n",
    "- %y  2-digit year\n",
    "- %m  2-digit month\n",
    "- %d  2-digit day\n",
    "- %H  hour\n",
    "- %I  12-hour clock\n",
    "- %M  2-digit min\n",
    "- %S  second\n",
    "- %w  weekday as integer\n",
    "- %U  week number of year Sunday as first day of week\n",
    "- %W  week number Monday first\n",
    "- %z  UTC time zone offset as +HHMM or -HHMM empty if timezone naive\n",
    "- %F  short for %Y-%m-%d\n",
    "- %D  short for %m/%d/%y\n",
    "\n",
    "### Locale-spec formating\n",
    "- %a  abbre weekday name\n",
    "- %A  full weekday name\n",
    "- %b  abbre month name\n",
    "- %B  full month name\n",
    "- %c  full date time\n",
    "- %p  locale equi-am or pm\n",
    "- %x  locale-proper formatted date\n",
    "- %X  locale-proper time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011-01-02    1.188368\n",
       "2011-01-05    0.313255\n",
       "2011-01-07    0.026741\n",
       "2011-01-08    1.310502\n",
       "2011-01-10   -1.039549\n",
       "2011-01-12    0.430944\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2011-01-02', '2011-01-05', '2011-01-07', '2011-01-08',\n",
       "               '2011-01-10', '2011-01-12'],\n",
       "              dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2011-01-02    2.376735\n",
       "2011-01-05         NaN\n",
       "2011-01-07    0.053481\n",
       "2011-01-08         NaN\n",
       "2011-01-10   -2.079099\n",
       "2011-01-12         NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2011-01-02 00:00:00')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.026740595502435946"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-1.0395494687011155, -1.0395494687011155)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-01-01    1.755024\n",
       "2000-01-02    0.983606\n",
       "2000-01-03   -0.165140\n",
       "2000-01-04    0.195253\n",
       "2000-01-05   -0.030057\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2001-01-01    0.052687\n",
       " 2001-01-02   -0.501179\n",
       " 2001-01-03    0.433831\n",
       " 2001-01-04    0.905362\n",
       " 2001-01-05    0.336581\n",
       " Freq: D, dtype: float64, 2001-05-01    0.788311\n",
       " 2001-05-02    0.665669\n",
       " 2001-05-03    0.021811\n",
       " 2001-05-04    0.531980\n",
       " 2001-05-05   -1.313512\n",
       " Freq: D, dtype: float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2011-01-07    0.026741\n",
       "2011-01-08    1.310502\n",
       "2011-01-10   -1.039549\n",
       "2011-01-12    0.430944\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2011-01-07    0.026741\n",
       "2011-01-08    1.310502\n",
       "2011-01-10   -1.039549\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dates =  [datetime(2011,1,2), datetime(2011,1, 5), \n",
    "                           datetime(2011,1, 7), datetime(2011,1, 8),  datetime(2011,1,10),  datetime(2011,1,12)]\n",
    "\n",
    "ts = Series(np.random.randn(6), index=dates)\n",
    "ts\n",
    "type(ts)\n",
    "ts.index # Il faut PAS use TimeSeries explicitly, when creating Series with DateTimeIndex, pandas knows\n",
    "\n",
    "ts + ts[::2] # diff-indexed time series automatically align on the dates !!\n",
    "\n",
    "ts.index.dtype\n",
    "\n",
    "ts.index[0] # Timestamp obj, used for datetime obj\n",
    "ts[ts.index[2]] \n",
    "ts['1/10/2011'], ts['20110110']\n",
    "\n",
    "\n",
    "long_ts = Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))\n",
    "long_ts[:5]\n",
    "long_ts['2001'][:5], long_ts['2001-05'][:5]\n",
    "\n",
    "ts[datetime(2011,1,7):] # slicing with dates works justl ike regular Series as most TS data ordered \n",
    "\n",
    "ts['1/6/2011':'1/11/2011']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011-01-02    1.188368\n",
       "2011-01-05    0.313255\n",
       "2011-01-07    0.026741\n",
       "2011-01-08    1.310502\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Colorado</th>\n",
       "      <th>Taxas</th>\n",
       "      <th>NY</th>\n",
       "      <th>Ohio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-05-02</th>\n",
       "      <td>-0.639555</td>\n",
       "      <td>-1.087885</td>\n",
       "      <td>1.324169</td>\n",
       "      <td>-0.110692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-05-09</th>\n",
       "      <td>0.410760</td>\n",
       "      <td>0.072771</td>\n",
       "      <td>0.852234</td>\n",
       "      <td>0.550670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-05-16</th>\n",
       "      <td>-0.065198</td>\n",
       "      <td>-0.066211</td>\n",
       "      <td>-0.857632</td>\n",
       "      <td>0.415574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-05-23</th>\n",
       "      <td>-0.559778</td>\n",
       "      <td>0.940752</td>\n",
       "      <td>-1.531608</td>\n",
       "      <td>-0.682450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-05-30</th>\n",
       "      <td>0.467056</td>\n",
       "      <td>-1.483246</td>\n",
       "      <td>0.066533</td>\n",
       "      <td>-1.725065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Colorado     Taxas        NY      Ohio\n",
       "2001-05-02 -0.639555 -1.087885  1.324169 -0.110692\n",
       "2001-05-09  0.410760  0.072771  0.852234  0.550670\n",
       "2001-05-16 -0.065198 -0.066211 -0.857632  0.415574\n",
       "2001-05-23 -0.559778  0.940752 -1.531608 -0.682450\n",
       "2001-05-30  0.467056 -1.483246  0.066533 -1.725065"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2011-01-02    1.188368\n",
       " 2011-01-05    0.313255\n",
       " 2011-01-07    0.026741\n",
       " 2011-01-08    1.310502\n",
       " 2011-01-10   -1.039549\n",
       " 2011-01-12    0.430944\n",
       " dtype: float64, 2011-01-02    1\n",
       " 2011-01-05    1\n",
       " 2011-01-07    1\n",
       " 2011-01-08    1\n",
       " 2011-01-10    1\n",
       " 2011-01-12    1\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndexResampler [freq=<Day>, axis=0, closed=left, label=left, convention=start, base=0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2012-04-01', '2012-04-02', '2012-04-03', '2012-04-04',\n",
       "               '2012-04-05', '2012-04-06', '2012-04-07', '2012-04-08',\n",
       "               '2012-04-09', '2012-04-10', '2012-04-11', '2012-04-12',\n",
       "               '2012-04-13', '2012-04-14', '2012-04-15', '2012-04-16',\n",
       "               '2012-04-17', '2012-04-18', '2012-04-19', '2012-04-20',\n",
       "               '2012-04-21', '2012-04-22', '2012-04-23', '2012-04-24',\n",
       "               '2012-04-25', '2012-04-26', '2012-04-27', '2012-04-28',\n",
       "               '2012-04-29', '2012-04-30', '2012-05-01', '2012-05-02',\n",
       "               '2012-05-03', '2012-05-04', '2012-05-05', '2012-05-06',\n",
       "               '2012-05-07', '2012-05-08', '2012-05-09', '2012-05-10',\n",
       "               '2012-05-11', '2012-05-12', '2012-05-13', '2012-05-14',\n",
       "               '2012-05-15', '2012-05-16', '2012-05-17', '2012-05-18',\n",
       "               '2012-05-19', '2012-05-20', '2012-05-21', '2012-05-22',\n",
       "               '2012-05-23', '2012-05-24', '2012-05-25', '2012-05-26',\n",
       "               '2012-05-27', '2012-05-28', '2012-05-29', '2012-05-30',\n",
       "               '2012-05-31', '2012-06-01'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2012-04-01', '2012-04-02', '2012-04-03', '2012-04-04',\n",
       "               '2012-04-05', '2012-04-06', '2012-04-07', '2012-04-08',\n",
       "               '2012-04-09', '2012-04-10', '2012-04-11', '2012-04-12',\n",
       "               '2012-04-13', '2012-04-14', '2012-04-15', '2012-04-16',\n",
       "               '2012-04-17', '2012-04-18', '2012-04-19', '2012-04-20'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2012-05-13', '2012-05-14', '2012-05-15', '2012-05-16',\n",
       "               '2012-05-17', '2012-05-18', '2012-05-19', '2012-05-20',\n",
       "               '2012-05-21', '2012-05-22', '2012-05-23', '2012-05-24',\n",
       "               '2012-05-25', '2012-05-26', '2012-05-27', '2012-05-28',\n",
       "               '2012-05-29', '2012-05-30', '2012-05-31', '2012-06-01'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2000-01-31', '2000-02-29', '2000-03-31', '2000-04-28',\n",
       "               '2000-05-31', '2000-06-30', '2000-07-31', '2000-08-31',\n",
       "               '2000-09-29', '2000-10-31', '2000-11-30'],\n",
       "              dtype='datetime64[ns]', freq='BM')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2012-05-02 12:56:31', '2012-05-03 12:56:31',\n",
       "               '2012-05-04 12:56:31', '2012-05-05 12:56:31',\n",
       "               '2012-05-06 12:56:31'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2012-05-02', '2012-05-03', '2012-05-04', '2012-05-05',\n",
       "               '2012-05-06'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing any of string date / datetime / Timestamp; slicing = VIEW equivalent to truncate\n",
    "ts.truncate(after='1/9/2011')\n",
    "\n",
    "dates = pd.date_range('1/1/2000', periods=100, freq='W-WED')\n",
    "\n",
    "long_df = DataFrame(np.random.randn(100, 4), index=dates, columns=['Colorado', 'Taxas', 'NY', 'Ohio'])\n",
    "long_df.loc['5-2001']\n",
    "\n",
    "# DUPLICATE INDICES\n",
    "\n",
    "ts.index.is_unique # if not False\n",
    "\n",
    "ts_grouped = ts.groupby(level=0) # aggregate data having non-unique timestamps\n",
    "\n",
    "ts_grouped.mean(), ts_grouped.count()\n",
    "\n",
    "\n",
    "# DATE RANGE, FREQUENCIES, SHIFTING\n",
    "\n",
    "ts.resample('D') # is it a timestamp thing?\n",
    "\n",
    "index = pd.date_range('4/1/2012', '6/1/2012') # \n",
    "index\n",
    "    # default daily timestamps\n",
    "pd.date_range(start='4/1/2012', periods=20)\n",
    "pd.date_range(end='6/1/2012', periods= 20)\n",
    "    # disintct bounds \n",
    "\n",
    "pd.date_range('1/1/2000', '12/1/2000', freq='BM') # business end of month\n",
    "\n",
    "pd.date_range('/5/2/2012 12:56:31', periods=5)\n",
    "\n",
    "pd.date_range('5/2/2012 12:56:31', periods=5, normalize=True) # normed to midngiht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Hour>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<4 * Hours>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 04:00:00',\n",
       "               '2000-01-01 08:00:00', '2000-01-01 12:00:00',\n",
       "               '2000-01-01 16:00:00', '2000-01-01 20:00:00',\n",
       "               '2000-01-02 00:00:00', '2000-01-02 04:00:00',\n",
       "               '2000-01-02 08:00:00', '2000-01-02 12:00:00',\n",
       "               '2000-01-02 16:00:00', '2000-01-02 20:00:00',\n",
       "               '2000-01-03 00:00:00', '2000-01-03 04:00:00',\n",
       "               '2000-01-03 08:00:00', '2000-01-03 12:00:00',\n",
       "               '2000-01-03 16:00:00', '2000-01-03 20:00:00'],\n",
       "              dtype='datetime64[ns]', freq='4H')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<150 * Minutes>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 01:30:00',\n",
       "               '2000-01-01 03:00:00', '2000-01-01 04:30:00',\n",
       "               '2000-01-01 06:00:00', '2000-01-01 07:30:00',\n",
       "               '2000-01-01 09:00:00', '2000-01-01 10:30:00',\n",
       "               '2000-01-01 12:00:00', '2000-01-01 13:30:00'],\n",
       "              dtype='datetime64[ns]', freq='90T')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FREQUENCIES AND DATE OFFSETS\n",
    "\n",
    "from pandas.tseries.offsets import Hour, Minute\n",
    "\n",
    "hour = Hour() # base frequency and multiplier\n",
    "hour\n",
    "\n",
    "four_hours = Hour(4) # multiple of offset \n",
    "four_hours\n",
    "\n",
    "pd.date_range('1/1/2000', '1/3/2000 23:59', freq='4h') # never above, instead like so in FREQ\n",
    "\n",
    "Hour(2) + Minute(30)\n",
    "\n",
    "pd.date_range('1/1/2000', periods=10, freq='1h30min') # frequency strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Time Series Frequencies\n",
    "- D   day calendar daily\n",
    "- B   businessday \n",
    "- H   hour\n",
    "- T   (or min) minute\n",
    "- S  \n",
    "- L   (or ms) mili\n",
    "- U   micro\n",
    "- M   month end\n",
    "- BM   lastbusines day\n",
    "- MS   month begin\n",
    "- BMS   first weekday of month\n",
    "- W-MON   etc, weekly on given day of week\n",
    "- WOM-1MON   week of month , WOM-3FRI = 3rd Friday of each month\n",
    "- Q-JAN   quater end\n",
    "- BQ-JAN   quartelry dates anchored on last weekday\n",
    "- A-JAN   year end\n",
    "- BA-JAN   biz year end\n",
    "- AS-JAN   year begin\n",
    "- BAS-JAN   annual dates anchored on first weekday of given month businesswise- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2012-01-20 00:00:00', freq='WOM-3FRI'),\n",
       " Timestamp('2012-02-17 00:00:00', freq='WOM-3FRI'),\n",
       " Timestamp('2012-03-16 00:00:00', freq='WOM-3FRI'),\n",
       " Timestamp('2012-04-20 00:00:00', freq='WOM-3FRI'),\n",
       " Timestamp('2012-05-18 00:00:00', freq='WOM-3FRI'),\n",
       " Timestamp('2012-06-15 00:00:00', freq='WOM-3FRI'),\n",
       " Timestamp('2012-07-20 00:00:00', freq='WOM-3FRI'),\n",
       " Timestamp('2012-08-17 00:00:00', freq='WOM-3FRI')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2000-01-31    1.349511\n",
       " 2000-02-29    1.197860\n",
       " 2000-03-31    1.047459\n",
       " 2000-04-30   -0.522432\n",
       " Freq: M, dtype: float64, 2000-01-31         NaN\n",
       " 2000-02-29         NaN\n",
       " 2000-03-31    1.349511\n",
       " 2000-04-30    1.197860\n",
       " Freq: M, dtype: float64, 2000-01-31    1.047459\n",
       " 2000-02-29   -0.522432\n",
       " 2000-03-31         NaN\n",
       " 2000-04-30         NaN\n",
       " Freq: M, dtype: float64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-01-31         NaN\n",
       "2000-02-29   -0.112375\n",
       "2000-03-31   -0.125558\n",
       "2000-04-30   -1.498762\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-03-31    1.349511\n",
       "2000-04-30    1.197860\n",
       "2000-05-31    1.047459\n",
       "2000-06-30   -0.522432\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-02-03    1.349511\n",
       "2000-03-03    1.197860\n",
       "2000-04-03    1.047459\n",
       "2000-05-03   -0.522432\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-01-31 01:30:00    1.349511\n",
       "2000-02-29 01:30:00    1.197860\n",
       "2000-03-31 01:30:00    1.047459\n",
       "2000-04-30 01:30:00   -0.522432\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2011-11-20 00:00:00')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2011-11-30 00:00:00')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2011-12-31 00:00:00')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2011-11-30 00:00:00')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2011-10-31 00:00:00')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Week of month dates\n",
    "rng = pd.date_range('1/1/2012', '9/1/2012', freq='WOM-3FRI')\n",
    "\n",
    "list(rng) # traders of US equity options wil recog these dates as standarddates of monthly expiry\n",
    "\n",
    "# SHIFTING (LEADING AND LAGGING)\n",
    "ts = Series(np.random.randn(4), index=pd.date_range('1/1/2000', periods=4, freq='M'))\n",
    "\n",
    "ts, ts.shift(2), ts.shift(-2) # common use for computing pct chnage\n",
    "\n",
    "ts / ts.shift(1) - 1 # naive shift leaves index unmod some data discarded, thus if freq known, can pass to advance \n",
    "\n",
    "ts.shift(2, freq='M')\n",
    "\n",
    "ts.shift(1, freq='3D') \n",
    "\n",
    "ts.shift(1, freq='90T')\n",
    "\n",
    "from pandas.tseries.offsets import Day, MonthEnd\n",
    "\n",
    "now = datetime(2011, 11, 17)\n",
    "\n",
    "now + 3 * Day()\n",
    "\n",
    "now + MonthEnd()\n",
    "\n",
    "now + MonthEnd(2)\n",
    "\n",
    "offset = MonthEnd()\n",
    "\n",
    "offset.rollforward(now)\n",
    "\n",
    "offset.rollback(now) # anchored offsets roll dates using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000-01-31    0.257064\n",
       "2000-02-29    0.244444\n",
       "2000-03-31   -0.091533\n",
       "dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-01-31    0.257064\n",
       "2000-02-29    0.244444\n",
       "2000-03-31   -0.091533\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clever use of date offsets is to use with groupby\n",
    "ts = Series(np.random.randn(20), index=pd.date_range('1/15/2000', periods=20, freq='4d'))\n",
    "\n",
    "ts.groupby(offset.rollforward).mean()\n",
    "\n",
    "ts.resample('M').mean() # EASIER FASTER way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<DstTzInfo 'US/Eastern' LMT-1 day, 19:04:00 STD>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2012-03-09 09:30:00+00:00', '2012-03-10 09:30:00+00:00',\n",
       "               '2012-03-11 09:30:00+00:00', '2012-03-12 09:30:00+00:00',\n",
       "               '2012-03-13 09:30:00+00:00', '2012-03-14 09:30:00+00:00',\n",
       "               '2012-03-15 09:30:00+00:00', '2012-03-16 09:30:00+00:00',\n",
       "               '2012-03-17 09:30:00+00:00', '2012-03-18 09:30:00+00:00'],\n",
       "              dtype='datetime64[ns, UTC]', freq='D')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2012-03-09 09:30:00+00:00    0.009380\n",
       "2012-03-10 09:30:00+00:00   -1.934166\n",
       "2012-03-11 09:30:00+00:00    0.116831\n",
       "2012-03-12 09:30:00+00:00   -0.222390\n",
       "2012-03-13 09:30:00+00:00    1.165559\n",
       "2012-03-14 09:30:00+00:00   -0.898233\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2012-03-09 09:30:00+00:00', '2012-03-10 09:30:00+00:00',\n",
       "               '2012-03-11 09:30:00+00:00', '2012-03-12 09:30:00+00:00',\n",
       "               '2012-03-13 09:30:00+00:00', '2012-03-14 09:30:00+00:00'],\n",
       "              dtype='datetime64[ns, UTC]', freq='D')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2012-03-09 04:30:00-05:00    0.009380\n",
       "2012-03-10 04:30:00-05:00   -1.934166\n",
       "2012-03-11 05:30:00-04:00    0.116831\n",
       "2012-03-12 05:30:00-04:00   -0.222390\n",
       "2012-03-13 05:30:00-04:00    1.165559\n",
       "2012-03-14 05:30:00-04:00   -0.898233\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2012-03-09 14:30:00+00:00    0.009380\n",
       "2012-03-10 14:30:00+00:00   -1.934166\n",
       "2012-03-11 13:30:00+00:00    0.116831\n",
       "2012-03-12 13:30:00+00:00   -0.222390\n",
       "2012-03-13 13:30:00+00:00    1.165559\n",
       "2012-03-14 13:30:00+00:00   -0.898233\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2012-03-09 09:30:00+08:00', '2012-03-10 09:30:00+08:00',\n",
       "               '2012-03-11 09:30:00+08:00', '2012-03-12 09:30:00+08:00',\n",
       "               '2012-03-13 09:30:00+08:00', '2012-03-14 09:30:00+08:00'],\n",
       "              dtype='datetime64[ns, Asia/Shanghai]', freq='D')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2011-03-11 23:00:00-0500', tz='US/Eastern')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2011-03-12 04:00:00+0300', tz='Europe/Moscow')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1299902400000000000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2011-03-12 05:00:00')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Zone\n",
    "import pytz\n",
    "\n",
    "pytz.common_timezones[-5:]\n",
    "\n",
    "tz = pytz.timezone('US/Eastern')\n",
    "tz # accpet either tie zone names or object, just name\n",
    "\n",
    "# Localisation and Conversion\n",
    "rng = pd.date_range('3/9/2012 9:30', periods=6, freq='D')\n",
    "ts = Series(np.random.randn(len(rng)), index=rng)\n",
    "\n",
    "print(ts.index.tz)\n",
    "\n",
    "pd.date_range('3/9/2012 9:30', periods = 10, freq='D', tz='UTC')\n",
    "ts_utc = ts.tz_localize('UTC') # conversion for naive to localised \n",
    "ts_utc\n",
    "ts_utc.index\n",
    "ts_utc.tz_convert('US/Eastern')\n",
    "\n",
    "ts_eastern = ts.tz_localize('US/Eastern') # could localise to EST and convert to UTC\n",
    "ts_eastern.tz_convert('UTC')\n",
    "ts.index.tz_localize('Asia/Shanghai') # both are alsoe instance mehtods on DatetimeIndex\n",
    "\n",
    "# AWARE TIMESTAMP OBJ\n",
    "stamp = pd.Timestamp('2011-03-12 04:00')\n",
    "stamp_utc = stamp.tz_localize('utc')\n",
    "stamp_utc.tz_convert('US/Eastern') \n",
    "stamp_moscow = pd.Timestamp('2011-03-12 04:00', tz='Europe/Moscow')\n",
    "stamp_moscow\n",
    "\n",
    "stamp_utc.value\n",
    "\n",
    "# 30 minutes before \n",
    "stamp + Hour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2007', 'A-DEC')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(Period('2012', 'A-DEC'), Period('2005', 'A-DEC'))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PeriodIndex(['2001Q3', '2002Q2', '2003Q1'], dtype='period[Q-DEC]', freq='Q-DEC')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Period('2007-01', 'M')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Period('2007-12', 'M')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2006   -1.457623\n",
       "2007   -0.414635\n",
       "2008    0.963312\n",
       "2009   -1.262272\n",
       "Freq: A-DEC, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2006-01   -1.457623\n",
       "2007-01   -0.414635\n",
       "2008-01    0.963312\n",
       "2009-01   -1.262272\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Period('2012Q4', 'Q-JAN')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Period('2011-11-01', 'D')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2011Q3    0\n",
       "2011Q4    1\n",
       "2012Q1    2\n",
       "2012Q2    3\n",
       "2012Q3    4\n",
       "2012Q4    5\n",
       "Freq: Q-JAN, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2010-10-28 16:00:00    0\n",
       "2011-01-28 16:00:00    1\n",
       "2011-04-28 16:00:00    2\n",
       "2011-07-28 16:00:00    3\n",
       "2011-10-28 16:00:00    4\n",
       "2012-01-30 16:00:00    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PERIOD ARITHMETIC\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "p = pd.Period('2007', freq='A-DEC')\n",
    "p\n",
    "\n",
    "p + 5, p - 2\n",
    "\n",
    "values = ['2001Q3', '2002Q2', '2003Q1']\n",
    "\n",
    "index = pd.PeriodIndex(values, freq='Q-DEC')\n",
    "index\n",
    "\n",
    "p.asfreq('M', how='start')\n",
    "p.asfreq('M', how='end') # freq conversion\n",
    "\n",
    "rng = pd.period_range('2006', '2009', freq='A-DEC')\n",
    "\n",
    "ts = Series(np.random.randn(len(rng)), index=rng)\n",
    "\n",
    "ts\n",
    "\n",
    "ts.asfreq('M', how='start')\n",
    "\n",
    "# Quarterly Period freq\n",
    "\n",
    "p = pd.Period('2012Q4', freq='Q-JAN')\n",
    "p\n",
    "p.asfreq('D', 'start')\n",
    "\n",
    "# e.g. get timestamp at 4PM on 2nd to last BD of the Q\n",
    "p4pm = (p.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60\n",
    "\n",
    "# Generating Q ranges the same\n",
    "rng = pd.period_range('2011Q3', '2012Q4', freq='Q-JAN')\n",
    "\n",
    "ts = Series(np.arange(len(rng)), index=rng)\n",
    "\n",
    "ts\n",
    "\n",
    "new_rng = (rng.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60\n",
    "\n",
    "ts.index = new_rng.to_timestamp()\n",
    "\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000-01-31   -0.538384\n",
       " 2000-02-29    1.286765\n",
       " 2000-03-31   -0.252885\n",
       " Freq: M, dtype: float64, 2000-01   -0.538384\n",
       " 2000-02    1.286765\n",
       " 2000-03   -0.252885\n",
       " Freq: M, dtype: float64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-01   -1.895860\n",
       "2000-01    1.796974\n",
       "2000-01   -0.495974\n",
       "2000-02   -0.820071\n",
       "2000-02   -0.136071\n",
       "2000-02    0.177865\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-01   -0.538384\n",
       "2000-02    1.286765\n",
       "2000-03   -0.252885\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-01-31   -0.538384\n",
       "2000-02-29    1.286765\n",
       "2000-03-31   -0.252885\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-01   -0.538384\n",
       "2000-02    1.286765\n",
       "2000-03   -0.252885\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Timestamps to Periods\n",
    "rng = pd.date_range('1/1/2000', periods=3, freq='M')\n",
    "ts = Series(np.random.randn(3), index=rng)\n",
    "pts = ts.to_period()\n",
    "ts, pts\n",
    "\n",
    "rng = pd.date_range('1/29/2000', periods=6, freq='D')\n",
    "ts2 = Series(np.random.randn(6), index=rng)\n",
    "ts2.to_period('M')\n",
    "\n",
    "pts = ts.to_period()\n",
    "pts\n",
    "pts.to_timestamp(how='end')\n",
    "pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a PeridIndex from Arrays\n",
    "\n",
    "from statsmodels.datasets import macrodata\n",
    "\n",
    "data = macrodata.load_pandas()\n",
    "data = data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    1959.0\n",
       " 1    1959.0\n",
       " 2    1959.0\n",
       " 3    1959.0\n",
       " 4    1960.0\n",
       " Name: year, dtype: float64, 0    1.0\n",
       " 1    2.0\n",
       " 2    3.0\n",
       " 3    4.0\n",
       " 4    1.0\n",
       " Name: quarter, dtype: float64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PeriodIndex(['1959Q1', '1959Q2', '1959Q3', '1959Q4', '1960Q1', '1960Q2',\n",
       "             '1960Q3', '1960Q4', '1961Q1', '1961Q2',\n",
       "             ...\n",
       "             '2007Q2', '2007Q3', '2007Q4', '2008Q1', '2008Q2', '2008Q3',\n",
       "             '2008Q4', '2009Q1', '2009Q2', '2009Q3'],\n",
       "            dtype='period[Q-DEC]', length=203, freq='Q-DEC')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1959Q1    0.00\n",
       "1959Q2    2.34\n",
       "1959Q3    2.74\n",
       "1959Q4    0.27\n",
       "1960Q1    2.31\n",
       "1960Q2    0.14\n",
       "1960Q3    2.70\n",
       "1960Q4    1.21\n",
       "1961Q1   -0.40\n",
       "1961Q2    1.47\n",
       "1961Q3    0.80\n",
       "1961Q4    0.80\n",
       "1962Q1    2.26\n",
       "1962Q2    0.13\n",
       "1962Q3    2.11\n",
       "1962Q4    0.79\n",
       "1963Q1    0.53\n",
       "1963Q2    2.75\n",
       "1963Q3    0.78\n",
       "1963Q4    2.46\n",
       "1964Q1    0.13\n",
       "1964Q2    0.90\n",
       "1964Q3    1.29\n",
       "1964Q4    2.05\n",
       "1965Q1    1.28\n",
       "1965Q2    2.54\n",
       "1965Q3    0.89\n",
       "1965Q4    2.90\n",
       "1966Q1    4.99\n",
       "1966Q2    2.10\n",
       "          ... \n",
       "2002Q2    1.56\n",
       "2002Q3    2.66\n",
       "2002Q4    3.08\n",
       "2003Q1    1.31\n",
       "2003Q2    1.09\n",
       "2003Q3    2.60\n",
       "2003Q4    3.02\n",
       "2004Q1    2.35\n",
       "2004Q2    3.61\n",
       "2004Q3    3.58\n",
       "2004Q4    2.09\n",
       "2005Q1    4.15\n",
       "2005Q2    1.85\n",
       "2005Q3    9.14\n",
       "2005Q4    0.40\n",
       "2006Q1    2.60\n",
       "2006Q2    3.97\n",
       "2006Q3   -1.58\n",
       "2006Q4    3.30\n",
       "2007Q1    4.58\n",
       "2007Q2    2.75\n",
       "2007Q3    3.45\n",
       "2007Q4    6.38\n",
       "2008Q1    2.82\n",
       "2008Q2    8.53\n",
       "2008Q3   -3.16\n",
       "2008Q4   -8.79\n",
       "2009Q1    0.94\n",
       "2009Q2    3.37\n",
       "2009Q3    3.56\n",
       "Freq: Q-DEC, Name: infl, Length: 203, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.year[:5], data.quarter[:5]\n",
    "\n",
    "index = pd.PeriodIndex(year=data.year, quarter=data.quarter, freq='Q-DEC') # passing these arrays to PeriodIndex with a freq to combin an index for DF\n",
    "index\n",
    "\n",
    "data.index = index\n",
    "\n",
    "data.infl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000-01-31    0.140221\n",
       "2000-02-29   -0.000831\n",
       "2000-03-31   -0.083460\n",
       "2000-04-30    0.727592\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-01    0.140221\n",
       "2000-02   -0.000831\n",
       "2000-03   -0.083460\n",
       "2000-04    0.727592\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resampling and Frequency Conversion \n",
    "# Aggregtaing hier feq to lower is downsampling, inversely upsampling\n",
    "\n",
    "rng = pd.date_range('1/1/2000', periods=100, freq='D')\n",
    "\n",
    "ts = Series(np.random.randn(len(rng)), index=rng)\n",
    "ts.resample('M').mean()\n",
    "ts.resample('M', kind='period').mean()\n",
    "\n",
    "# resample is flexible and high-perforamce mehotd used to process very large TS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample Method Arguments\n",
    "- freq string or dateoffset indicating desired freq\n",
    "- how='mean' func name or array func making value 'mean', 'ohlc', np.max, 'first', last, median\n",
    "- axis=0 axis to resample on\n",
    "- fill_method=None how to interpolate when upsampling 'ffill' bfill'\n",
    "- clsed='right' in downsampling, which end of each interval is closed\n",
    "- label='right' in downing, how to label aggre result\n",
    "- loffset=None time adjustmentto bin labels such as '-1s'\n",
    "- limit=None when forward or back filing, max number of periods to fill\n",
    "- kind=None aggr periods 'peroid' or timestamps'timestaamps\n",
    "- convention=None when resampling erpoids convetions start or end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000-01-01 00:00:00     0\n",
       "2000-01-01 00:01:00     1\n",
       "2000-01-01 00:02:00     2\n",
       "2000-01-01 00:03:00     3\n",
       "2000-01-01 00:04:00     4\n",
       "2000-01-01 00:05:00     5\n",
       "2000-01-01 00:06:00     6\n",
       "2000-01-01 00:07:00     7\n",
       "2000-01-01 00:08:00     8\n",
       "2000-01-01 00:09:00     9\n",
       "2000-01-01 00:10:00    10\n",
       "2000-01-01 00:11:00    11\n",
       "Freq: T, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-01-01 00:00:00    10\n",
       "2000-01-01 00:05:00    35\n",
       "2000-01-01 00:10:00    21\n",
       "Freq: 5T, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-01-01 00:00:00     2.0\n",
       "2000-01-01 00:05:00     7.0\n",
       "2000-01-01 00:10:00    10.5\n",
       "Freq: 5T, dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000-01-01 00:00:00     2.0\n",
       "2000-01-01 00:05:00     7.0\n",
       "2000-01-01 00:10:00    10.5\n",
       "Freq: 5T, dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1999-12-31 23:59:59     2.0\n",
       "2000-01-01 00:04:59     7.0\n",
       "2000-01-01 00:09:59    10.5\n",
       "Freq: 5T, dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:05:00</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:10:00</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     open  high  low  close\n",
       "2000-01-01 00:00:00     0     4    0      4\n",
       "2000-01-01 00:05:00     5     9    5      9\n",
       "2000-01-01 00:10:00    10    11   10     11"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1    5.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5    5.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsampling\n",
    "rng = pd.date_range('1/1/2000', periods=12, freq='T')\n",
    "ts = Series(np.arange(12), index=rng)\n",
    "ts\n",
    "ts.resample('5min').sum()\n",
    "\n",
    "ts.resample('5min', closed='left').mean()\n",
    "\n",
    "ts.resample('5min', closed='left', label='left').mean()\n",
    "\n",
    "ts.resample('5min', loffset='-1s').mean()\n",
    "\n",
    "ts.resample('5min').ohlc() # Open-High-Low-Close four values\n",
    "\n",
    "ts.groupby(lambda x: x.month).mean()\n",
    "ts.groupby(lambda x: x.weekday).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"close = pd.read_csv(file, parse_dates=True, index_col=0)\\nclose = close[['APPL']]\\nclose = close.resample('B', fill_method='ffill')\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Series Plotting\n",
    "'''close = pd.read_csv(file, parse_dates=True, index_col=0)\n",
    "close = close[['APPL']]\n",
    "close = close.resample('B', fill_method='ffill')'''\n",
    "\n",
    "# pd.rolling_mean(close, 250).plot()\n",
    "\n",
    "# define expanding mean in rolling\n",
    "# expanding_mean = lambda x: rolling_mean(x, len(x), min_periods=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Window and Exp weighted func\n",
    "- rolling_count returns num of non_NA obser in each trailing window\n",
    "- rolling_sum moving window sum\n",
    "- rolling_mean moving window mean\n",
    "- rolling_median \n",
    "- rolling_var\n",
    "- rolling_std\n",
    "- rolling_skew\n",
    "- rolling_min\n",
    "- rolling_quantile\n",
    "- rolling_corr\n",
    "- rolling_cov\n",
    "- rolling_apply apply generic array func over a moving window\n",
    "- ewma expoential--weight moving average\n",
    "- ewmvar\n",
    "- ewmstd\n",
    "- ewmcorr\n",
    "- ewmcov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Time Series Examples in FinEcon\n",
    "\n",
    "```python\n",
    "    prices.align(volume, join='inner')\n",
    "    # different indexed serise\n",
    "    DataFrame( {'one':s1, 'two':s2, 'three':s3}, index=list('abc'))\n",
    "    ts1.resample('B', fill_method='ffill')\n",
    "    rng = pd.date_range('2012-06-01 09:30', '2012-06-01 15:59', freq='T')\n",
    "    rng = rng.append([rng + pd.offsets.BDay(i) for i in range(1, 4)])\n",
    "    ts = Series(np.arange(len(rng), dtype=float), index=rng)\n",
    "    ts[time(10, 0)]\n",
    "    ts.at_time(time(10, 0))\n",
    "    ts.between_time(time(10, 0), time(10, 1))\n",
    "    indexer = np.sort(np.random.permutation(len(ts))[700:])\n",
    "    irr_ts = ts.copy()\n",
    "    irr_ts[indexer] = np.nan\n",
    "    irr_ts['2012-06-01 09:50' : '2012-06-01 10:00']\n",
    "    irr_ts.asof(pd.date_range('2012-06-01 10:00', periods=4, freq='B'))\n",
    "    # switching from one data source to another at specific point in time\n",
    "    # patching missing in ts at begining, midle or end using antoher time series\n",
    "    # completely repalcing data for a subset of symbols \n",
    "    spliced = pd.concat( [data1.loc[:'2012-06-14'], date2.loc['2012-06-15':]])\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NumPy\n",
    "- inside ndarray: \n",
    "    1. pointer to data block of memory\n",
    "    2. data type or dtype\n",
    "    3. tuple of shape\n",
    "    4. strides or step (C order, 3 x 4 x 5 array of float64 => 160, 40, 9)\n",
    "- Dtype Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=uint16)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[numpy.float64,\n",
       " numpy.floating,\n",
       " numpy.inexact,\n",
       " numpy.number,\n",
       " numpy.generic,\n",
       " float,\n",
       " object]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([700, 100, 200, 600])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([700, 100, 200, 600])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([  0,  42,  42, 300, 400, 500,  42,  42, 800, 900])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1.31154256,  1.99379374,  0.77039495,  0.36458795,\n",
       "          0.12549787],\n",
       "        [ 0.34454258,  0.17207022, -1.23042637,  0.1318841 ,\n",
       "          0.40378059],\n",
       "        [-1.09019235, -0.29280908, -0.51573333,  1.52229317,\n",
       "         -0.54518269],\n",
       "        [ 0.33469729, -1.64628257, -0.65118491, -1.91055421,\n",
       "         -0.38778737]],\n",
       "\n",
       "       [[-1.38474643, -1.09323791, -0.1676661 ,  1.11112204,\n",
       "          1.90544239],\n",
       "        [ 1.49031506, -0.07640619, -0.53539701,  1.2747054 ,\n",
       "         -0.97363284],\n",
       "        [ 0.53476783, -0.10980149,  1.36915008,  0.78058609,\n",
       "         -0.13143435],\n",
       "        [-0.30247655,  1.13734837, -1.01472368, -0.78595544,\n",
       "         -1.79714398]],\n",
       "\n",
       "       [[-1.31471734, -0.77588119,  0.21530978, -0.45232531,\n",
       "         -1.61386856],\n",
       "        [-1.08154781, -0.69287944, -0.29117042, -0.23483119,\n",
       "         -0.72436936],\n",
       "        [-1.89085229, -0.47793184, -0.27766927,  0.13672821,\n",
       "          0.3926533 ],\n",
       "        [ 1.25400424, -1.02945757, -0.04133799, -1.28378372,\n",
       "         -1.10741507]]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.91316341, -0.03562978, -0.18432486, -0.85222235],\n",
       "       [ 0.0741828 ,  0.23591688,  0.48865363, -0.55259026],\n",
       "       [-0.78829652, -0.60495965, -0.42341438, -0.44159802]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8.88178420e-17,  2.22044605e-17, -2.22044605e-17,\n",
       "        -1.33226763e-16],\n",
       "       [-4.44089210e-17, -4.44089210e-17,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-1.11022302e-16, -1.11022302e-17, -1.11022302e-16,\n",
       "         8.88178420e-17]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[5., 5., 5.],\n",
       "       [5., 5., 5.],\n",
       "       [5., 5., 5.],\n",
       "       [5., 5., 5.]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.28,  1.28,  1.28],\n",
       "       [-0.42, -0.42, -0.42],\n",
       "       [ 0.44,  0.44,  0.44],\n",
       "       [ 1.6 ,  1.6 ,  1.6 ]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.37 , -1.37 , -1.37 ],\n",
       "       [ 0.509,  0.509,  0.509],\n",
       "       [ 0.44 ,  0.44 ,  0.44 ],\n",
       "       [ 1.6  ,  1.6  ,  1.6  ]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(10, dtype=np.uint16)\n",
    "np.float64.mro() # parent classese of specif dtype\n",
    "\n",
    "arr = np.arange(8)\n",
    "arr.ravel()\n",
    "arr.flatten()\n",
    "\n",
    "arr = np.arange(12).reshape((3,4))\n",
    "arr\n",
    "arr.ravel()\n",
    "arr.ravel('F')\n",
    "\n",
    "arr = np.arange(3)\n",
    "arr.repeat(3)\n",
    "arr.repeat([2,3,4])\n",
    "\n",
    "arr = np.arange(10) * 100\n",
    "inds = [7 , 1, 2, 6]\n",
    "arr[inds]\n",
    "\n",
    "arr.take(inds)\n",
    "\n",
    "arr.put(inds, 42)\n",
    "arr\n",
    "arr.put(inds, [40, 41, 42, 43])\n",
    "\n",
    "# 3D array and to demean axis-2\n",
    "arr = np.random.randn(3, 4, 5)\n",
    "arr\n",
    "depth_means = arr.mean(2)\n",
    "depth_means\n",
    "demeaned = arr - depth_means[:, :, np.newaxis]\n",
    "demeaned.mean(2)\n",
    "\n",
    "# Same broadcasting rule governing arithmetic operations alsoe applies to setting values via arrray indexing \n",
    "arr =np.zeros((4, 3))\n",
    "arr[:] = 5\n",
    "arr\n",
    "col = np.array([1.28, -0.42, 0.44, 1.6])\n",
    "arr[:] = col[:, np.newaxis]\n",
    "arr\n",
    "arr[:2] = [[-1.37], [0.509]]\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.1152765 , -0.95106854,  0.93735221, -0.11047933, -1.11633504],\n",
       "       [ 1.88595961,  1.36151629, -1.43444323,  0.2071479 , -1.12641314],\n",
       "       [ 1.47518677, -1.33484591,  0.88124331, -0.28778382, -1.78494483],\n",
       "       [-0.38073419, -0.14440296,  0.24930142,  0.34310015, -0.98660859],\n",
       "       [-0.93676498, -0.28959674,  0.6272069 ,  1.01480108,  0.10753883]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True],\n",
       "       [False, False,  True, False],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True, False],\n",
       "       [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True, False,  True])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  3,  6, 10],\n",
       "       [ 5, 11, 18, 26, 35],\n",
       "       [10, 21, 33, 46, 60]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 2, 2])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 1, 2, 3, 4],\n",
       "       [0, 1, 2, 3, 4],\n",
       "       [0, 2, 4, 6, 8],\n",
       "       [0, 2, 4, 6, 8]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3, 4, 5)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([10, 18, 17])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  2,  3,  4],\n",
       "       [ 0,  2,  4,  6,  8],\n",
       "       [ 0,  3,  6,  9, 12]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [ 1,  5,  4],\n",
       "       [ 2, 10,  8],\n",
       "       [ 3, 15, 12]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 6, 8, 10, 12, 14], dtype=object)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  2.,  4.,  6.,  8., 10., 12., 14.])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.58 ms ± 706 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "6.58 µs ± 306 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Advacned ufunc usage : \n",
    "arr = np.arange(10)\n",
    "arr\n",
    "np.add.reduce(arr)\n",
    "arr.sum()\n",
    "\n",
    "arr = np.random.randn(5, 5)\n",
    "arr\n",
    "arr[::2].sort(1) \n",
    "arr[:, :-1] < arr[:, 1:]\n",
    "\n",
    "np.logical_and.reduce(arr[:, :-1] < arr[:, 1:], axis=1)\n",
    "\n",
    "arr = np.arange(15).reshape((3,5))\n",
    "\n",
    "np.add.accumulate(arr, axis=1)\n",
    "arr = np.arange(3).repeat([1, 2, 2])\n",
    "\n",
    "arr\n",
    "np.multiply.outer(arr, np.arange(5))\n",
    "\n",
    "result = np.subtract.outer(np.random.randn(3,4), np.random.randn(5))\n",
    "\n",
    "result.shape\n",
    "\n",
    "\n",
    "arr = np.arange(10)\n",
    "np.add.reduceat(arr, [0, 5, 8])\n",
    "\n",
    "arr = np.multiply.outer(np.arange(4), np.arange(5))\n",
    "arr\n",
    "np.add.reduceat(arr, [0, 2, 4], axis=1)\n",
    "\n",
    "# custom ufuncs\n",
    "def add_elements(x,y):\n",
    "    return x+y\n",
    "add_them = np.frompyfunc(add_elements, 2, 1)\n",
    "add_them(np.arange(8), np.arange(8))\n",
    "\n",
    "\n",
    "add_them = np.vectorize(add_elements, otypes=[np.float64])\n",
    "add_them(np.arange(8), np.arange(8))\n",
    "\n",
    "arr = np.random.randn(10000)\n",
    "%timeit add_them(arr, arr)\n",
    "%timeit np.add(arr, arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Tips\n",
    "- array typically replace otherwise slow pure pythnon loops\n",
    "- convert Phyton loops and conditional logic to array operaitons and boolean array opertions\n",
    "- use broadcasting as possible !\n",
    "- avoid copying data using array views (slicing)\n",
    "- use UFUNCS and UFUNCS methods\n",
    "\n",
    "### Cython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "343px",
    "width": "352px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Synopsis",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
