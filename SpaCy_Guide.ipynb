{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TddRiwSPYouV"
   },
   "source": [
    "**Snippet: Self-made 3 Versions of Preprocessing Text**\n",
    "\n",
    "1. Bare Token: Sans grammer, sans semantics\n",
    "\n",
    "```python\n",
    "def token_cleaner(text):\n",
    "    text = strip_multiple_whitespaces(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = strip_numeric(text) \n",
    "    text = strip_non_alphanum(text)\n",
    "    text = strip_punctuation(text)\n",
    "    text = strip_short(text, minsize=3)\n",
    "    text = [ tok.lemma_.lower().strip() for tok in nlp(text, disable=['tagger', 'parser', 'ner']) ]\n",
    "    text = [ tok for tok in text if tok not in SYMBOLS and tok not in STOPLIST ]\n",
    "    return ' '.join(text)\n",
    "```\n",
    "\n",
    "2. Lemmas: Retain grammar and semantcis\n",
    "\n",
    "```python\n",
    "def token_cleaner(text):\n",
    "    text = strip_multiple_whitespaces(text)\n",
    "    text = strip_non_alphanum(text)\n",
    "    text = strip_punctuation(text)\n",
    "    text = strip_short(text, minsize=3) # optional\n",
    "    text = [ tok.lemma_.lower().strip() for tok in nlp(text, disable=['tagger', 'parser', 'ner']) ]\n",
    "    text = [ tok for tok in text if tok not in SYMBOLS ]\n",
    "    return ' '.join(text)\n",
    "```\n",
    "\n",
    "3. Clean Text: Remove non-text only\n",
    "\n",
    "```python\n",
    "def token_cleaner(text):\n",
    "    text = strip_multiple_whitespaces(text)\n",
    "    text = strip_non_alphanum(text)\n",
    "    text = strip_punctuation(text)\n",
    "    text = strip_short(text, minsize=3) # optional\n",
    "    text = [ tok.text.lower().strip() for tok in nlp(text, disable=['tagger', 'parser', 'ner']) ]\n",
    "    text = [ tok for tok in text if tok not in SYMBOLS ]\n",
    "    return ' '.join(text)\n",
    "```\n",
    "\n",
    "### SpaCy Trick to speed up above by applying on whole text\n",
    "\n",
    "```python\n",
    "def doc_to_spans(list_of_texts, join_string=' ||| '):\n",
    "    all_docs = nlp(' ||| '.join(list_of_texts))\n",
    "    split_inds = [i for i, token in enumerate(all_docs) if token.text == '|||'] + [len(all_docs)]\n",
    "    new_docs = [all_docs[(i + 1 if i > 0 else i):j] for i, j in zip([0] + split_inds[:-1], split_inds)]\n",
    "    return new_docs \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU**\n",
    "\n",
    "```python\n",
    "def prefer_gpu():\n",
    "    used = spacy.util.use_gpu(0)\n",
    "    if used is None:\n",
    "        return False\n",
    "    else:\n",
    "        import cupy.random\n",
    "\n",
    "        cupy.random.seed(0)\n",
    "        return True\n",
    "random.seed(0)\n",
    "numpy.random.seed(0)\n",
    "use_gpu = prefer_gpu()\n",
    "print(\"Using GPU?\", use_gpu)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# COURSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token, Span, Lexical Attr, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellow\n",
      "world\n",
      "!\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "doc = nlp(\"Hellow world!\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "token = doc[1]\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "span = doc[1:4]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  [0, 1, 2, 3, 4]\n",
      "Text:  ['It', 'costs', '$', '5', '.']\n",
      "is_alpha: [True, True, False, False, False]\n",
      "is_punct: [False, False, False, False, True]\n",
      "like_num: [False, False, False, True, False]\n",
      "like_url [False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It costs $5.\")\n",
    "\n",
    "print('Index: ', [token.i for token in doc])\n",
    "print('Text: ', [token.text for token in doc])\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "print('like_num:', [token.like_num for token in doc])\n",
    "print('like_url', [token.like_url for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme proverty. \"\n",
    "          \"Now less than 4% are.\")\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        next_token = doc[token.i + 1]\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Model\n",
    "- POS\n",
    "- Syntatic Dep\n",
    "- NE\n",
    "\n",
    "1. **Training on Labelled Data**\n",
    "2. **Can be updated with more examples to fine-tune**\n",
    "\n",
    "**Model packages built-in**\n",
    "- Binary weights\n",
    "- Vocab\n",
    "- Metadata (language, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'direct object'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")\n",
    "spacy.explain(\"NNP\")\n",
    "spacy.explain('dobj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based Matching\n",
    "\n",
    "**Why not REGEX**\n",
    "1. Match on `Doc` not just strings\n",
    "2. Match on tokens and token attributes\n",
    "3. Use model's predictions\n",
    "4. e.g. \"duck\" (verb) vs. \"duck\" (noun)\n",
    "\n",
    "**Match patterns**\n",
    "- Lists of dictionaries, one per token\n",
    "- Match extact\n",
    "`[{'TEXT': 'iPhone'}, {'TEXT': 'X'}]`\n",
    "- Match lexical attributes\n",
    "`[{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "- Match any token attributes\n",
    "`[{'LEMMA': 'buy'}, {'POS': 'NOUN'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "# INIT matcher with SHARED VOCAB\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add pattern to matcher\n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "\n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "for matche_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matching Lexical**\n",
    "\n",
    "```python\n",
    "pattern = [\n",
    "    {'IS_DIGIT': True},\n",
    "    {'LOWER': 'fifa'},\n",
    "    {'LOWER': 'world'},\n",
    "    {'LOWER': 'cup'},\n",
    "    {'IS_PUNCT': 'True'}\n",
    "]\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "# 2018 FIFA World Cup:\n",
    "```\n",
    "\n",
    "**Matching other Token Attributes**\n",
    "\n",
    "```python\n",
    "pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "# loved dogs \n",
    "# love cats\n",
    "```\n",
    "\n",
    "**Operators and Quantifiers**\n",
    "\n",
    "```python\n",
    "pattern = [\n",
    "    {'LEMMA': 'buy'}.\n",
    "    {'POS': 'DET', 'OP': '?'}, # optional: match 0 or 1 times\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "# bought a smartphone\n",
    "# buying apps\n",
    "```\n",
    "\n",
    "- `{'OP': '!'}` Negation: match 0 times\n",
    "- `'+'` Match 1 or more times\n",
    "- `'*'` Match 0 or more times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Large-scale Data Analysis \n",
    "\n",
    "**Data Structure: Vocab, Lexemes and StringStore**\n",
    "\n",
    "- `Vocab` stores data shared across multiple documents\n",
    "- To save memory, spaCy encodes all strings to **hash value**\n",
    "- Strings are only stored once in the `StringStore` via `nlp.vocab.strings`\n",
    "- String store: **lookup table** in both directions\n",
    "\n",
    "```python\n",
    "coffee_hash = nlp.vocab.strings['coffee']\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "```\n",
    "\n",
    "- Hashes cannot be reversed - hence need to provide shared vocab!!!\n",
    "\n",
    "```python\n",
    "# Raises error if not seen string before\n",
    "string = nlp.vocab.strings[319792845301844401]\n",
    "```\n",
    "\n",
    "- Look up string and hash in `nlp.vocab.strings`\n",
    "\n",
    "```python\n",
    "doc = nlp(\"I love coffee\")\n",
    "nlp.vocab.strings['coffee'] # 319792745301814401\n",
    "nlp.vocab.strings[hash] # coffee\n",
    "\n",
    "# doc also exposes vocab and strings\n",
    "doc.vocab.strings['coffee']\n",
    "```\n",
    "\n",
    "- `Lexeme` obj is entry in vocab\n",
    "\n",
    "```python\n",
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "lexeme.text, lexeme.orth, lexeme.is_alpha # coffee 3179... True\n",
    "```\n",
    "\n",
    "- Contains **context-independent** info about word\n",
    "    - Word text: `lexeme.text` and `lexeme.orth` (the hash)\n",
    "    - Lexical attributes like `lexeme.is_alpha`\n",
    "    - NOT context-dependent POS, DEPs or NE\n",
    "    \n",
    "**DATA STRUCTURE**\n",
    "\n",
    "VOCAB, HASHES, LEXEMES\n",
    "\n",
    "- DOC (Token : I : PRON) <-nsubj- (Token : love : VERB) -dobj-> (Token : coffee : NOUN)\n",
    "- VOCAB (Lexeme : 46904...) (Lexeme : 37020...) (Lexeme : 31979...)\n",
    "- STRINGSTORE (4905... : \"I\") (37020... : \"love\") (31979... : \"coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5439657043933447811\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# look up hash for 'cat'\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# loop up cat_hash to get string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Docs, Spans, NE from Scratch**\n",
    "- spaCy under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "# manual creation doc\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world GREETING\n",
      "[('Hello world', 'GREETING')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "print(span_with_label.text, span_with_label.label_)\n",
    "\n",
    "doc.ents = [span_with_label]\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEST PRACTICES**\n",
    "\n",
    "- CONVERT RESULT TO STRINGS AS LATE AS POSSIBLE\n",
    "- USE TOKEN ATTRIBUTE IF AVAILABLE e.g. `token.i` for token index\n",
    "- ALWAYS PASS IN SHARED `vocab`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectors**\n",
    "\n",
    "`Doc.simiarlity(), Span.similarity(), Token.similarity()`\n",
    "\n",
    "**THREE WAY COMPARISON POSSIBLE**\n",
    "\n",
    "- Cosine default can be changed\n",
    "- `Doc` and `Span` default to average of `Token` vectors\n",
    "- Short phrases are better than long documents with many irrelevant words\n",
    "\n",
    "- Useful for many app: recomm, flagging duplicates, etc\n",
    "- Depends on context and what app needs to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Models and Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exact Match**\n",
    "\n",
    "COUNTRY a list of string names from json file\n",
    "\n",
    "```python\n",
    "# faster version of [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# create doc and find matches in it\n",
    "doc = nlp(TEXT) # some new text\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # create a Span with label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "    # overwrite doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "    # get span's root and head token\n",
    "    span_root_head = span.root.head\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "print([\n",
    "    (ent.text, ent.label_) for ent in doc.ents if ent.label_ ==\n",
    "    \"GPE\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Pipeline\n",
    "\n",
    "**Built-in Components**\n",
    "- **tagger** POS Token.tag\n",
    "- **parser** Dependency parser Token.dep, Token.head, Doc.sents, Doc.noun_chuncks\n",
    "- **ner** Doc.ents, Token.ent_iob, Token.ent_type\n",
    "- **textcat** Doc.cats\n",
    "\n",
    "**Custom Components**\n",
    "- Func takes `doc` and modifies it then returns it\n",
    "- can be added using `nlp.add_pipe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'animal_compoenent']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "def animal_compoenent(doc):\n",
    "    matches = matcher(doc)\n",
    "    # create Span for each match and assign label \n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # OVERWRITE doc.ents with matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(animal_compoenent, after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp(\"I have a cat and a Golden Retriever in my home in Montreal\")\n",
    "print([ (ent.text, ent.label_) for ent in doc.ents ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extension Attributes**\n",
    "\n",
    "- Add custom metadata to documents, tokens and spans\n",
    "- Accessible via `._` property\n",
    "\n",
    "```python\n",
    "doc._.title = \"My document\"\n",
    "token._.is_color = True\n",
    "span._.has_color = False\n",
    "```\n",
    "\n",
    "- Registered on global `Doc, Token, Span` using `set_extension`\n",
    "\n",
    "```python\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling and Performance**\n",
    "\n",
    "- SLOW `docs = [nlp(text) for text in BIG_TEXT]`\n",
    "- FAST `docs = list(nlp.pipe(BIG_TEXT))`\n",
    "- Passing in context \n",
    "    - `for doc, context in nlp.pipe(data, as_tuples=True): doc.text, context['page_number'] # self-defined`\n",
    "    - combined with `set_extension` in for loop to make `doc._.id and doc._.page_number` \n",
    "- Use ONLY `Tokenizer` disable other pipelines\n",
    "    - BAD: `doc = nlp(\"Hellow world')`\n",
    "    - GOOD: `doc = nlp.make_doc(\"Hellow world\")`\n",
    "    - `with nlp.disable_pipes('tagger', 'parser'): doc = nlp(text)'\n",
    "    \n",
    "```python\n",
    "# Example of pipe\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Network\n",
    "\n",
    "- Essential for custom Textcat and NER\n",
    "- Less critical for POS tagging and Dep parsing\n",
    "\n",
    "**FLOW**\n",
    "1. INIT model weights randomly with `nlp.begin_training`\n",
    "2. Predict a few examples with current weights by calling `nlp.update`\n",
    "3. Compare prediction with true labels\n",
    "4. Calculate how to change weights to improve predictions\n",
    "5. Update weigths slightly\n",
    "6. Go back to 2\n",
    "\n",
    "**NER Trainer**\n",
    "- NER tags words and phrases\n",
    "- Each token in mutually exclusive NE\n",
    "- Examples need CONTEXT!!\n",
    "    - `(\"iPhone X is coming\", {\"entities\": [(0, 8), 'GADGET')]})`\n",
    "\n",
    "**Texts with no entities also important!!**\n",
    "- `(\"I need a new phone! Any tipes?\", {\"entities\": []})`\n",
    "\n",
    "**Training Data**\n",
    "\n",
    "- Updating existing model = 00s to 000s examples\n",
    "- New category = 000s to 1,000,000 examples\n",
    "- Manual human annotators\n",
    "- Can be semi-automated - **Matcher**\n",
    "\n",
    "```python\n",
    "# make patterns for Matcher\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2) # two patterns for 'iPhone x and op ?`\n",
    "TRAINING_DATA = []\n",
    "\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # match on doc and create list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # get (start char, end char, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    # format matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # append to training\n",
    "    TRAINING_DATA.append(training_example)\n",
    "```\n",
    "\n",
    "**Training Loop**\n",
    "- Loop for times\n",
    "- Shuffle training data\n",
    "- Divide data into batches\n",
    "- Update model per batch\n",
    "- Save model\n",
    "\n",
    "```python\n",
    "for i in range(10):\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        nlp.update(texts, annotations)\n",
    "nlp.to_disk(path_to_model)\n",
    "```\n",
    "\n",
    "```python\n",
    "# new pipeline from scratch\n",
    "nlp = spacy.blank('en')\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "ner.add_label('GADGET')\n",
    "\n",
    "nlp.begin_training()\n",
    "for itn in range(10):\n",
    "    random.shuffle(examples)\n",
    "    losses = {}\n",
    "    for batch in spacy.util.minibatch(examples, size=2):\n",
    "        ...\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "        print(losses)\n",
    "```\n",
    "\n",
    "**BEST PRACTICES**\n",
    "- Existing model can overfit new data\n",
    "    - e.g. if only update with `WEBSITE`, it can 'unlearn' what a `PERSON` is\n",
    "    - Aka 'catastrophic forgetting' problem\n",
    "- Solution 1: Mix in previously correct preditions\n",
    "    - also include `PERSON` examples\n",
    "    - Run existing spaCy model over data and extract all other relevant entities !!\n",
    "\n",
    "- Models cannot learn everything\n",
    "    - predictions based on **local context**\n",
    "    - model can struggle to learn if decision hard to make based on context\n",
    "    - **label scheme needs be consistent and not too specific**\n",
    "        - e.g. `CLOTHING` is better than `ADULT_CLOTHING` and etc\n",
    "- Solution 2: Plan label scheme carefully\n",
    "    - pick categories reflecting local context\n",
    "    - more generic better than specific\n",
    "    - **use rules to go from generic labels to specific categories**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# GUIDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Linguistic Features\n",
    "Raw text to Doc, rich annotated object using linguistic features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging (Need Model)\n",
    "\n",
    "- After tokenizing, spaCy can **parse, tag** a given `Doc`, in comes stats-model prediction\n",
    "- Model consists of **binary** data, e.g. \"the\" + NOUN in English\n",
    "- Ling-annos under `Token`\n",
    "- spaCy encodes all strings to hash values to reduce MEM and speed\n",
    "- `_` + name give string repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text    \tlemma   \tpos     \ttag     \tdep     \tshape   \tis_alpha\tis_stop \n",
      "\n",
      "Apple   \tApple   \tPROPN   \tNNP     \tnsubj   \tXxxxx   \t1       \t0       \n",
      "is      \tbe      \tAUX     \tVBZ     \taux     \txx      \t1       \t1       \n",
      "looking \tlook    \tVERB    \tVBG     \tROOT    \txxxx    \t1       \t0       \n",
      "at      \tat      \tADP     \tIN      \tprep    \txx      \t1       \t1       \n",
      "buying  \tbuy     \tVERB    \tVBG     \tpcomp   \txxxx    \t1       \t0       \n",
      "U.K.    \tU.K.    \tPROPN   \tNNP     \tcompound\tX.X.    \t0       \t0       \n",
      "startup \tstartup \tNOUN    \tNN      \tdobj    \txxxx    \t1       \t0       \n",
      "for     \tfor     \tADP     \tIN      \tprep    \txxx     \t1       \t1       \n",
      "$       \t$       \tSYM     \t$       \tquantmod\t$       \t0       \t0       \n",
      "1       \t1       \tNUM     \tCD      \tcompound\td       \t0       \t0       \n",
      "billion \tbillion \tNUM     \tCD      \tpobj    \txxxx    \t1       \t0       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "print(\"{:<8}\\t{:<8}\\t{:<8}\\t{:<8}\\t{:<8}\\t{:<8}\\t{:<8}\\t{:<8}\".format('text', \n",
    "                                                                      'lemma', \n",
    "                                                                      'pos',\n",
    "                                                                      'tag',\n",
    "                                                                      'dep', \n",
    "                                                                      'shape',\n",
    "                                                                      'is_alpha',\n",
    "                                                                      'is_stop'))\n",
    "print()\n",
    "for token in doc:\n",
    "    print(\"{:<8}\\t{:<8}\\t{:<8}\\t{:<8}\\t{:<8}\\t{:<8}\\t{:<8}\\t{:<8}\".format(\n",
    "        token.text, \n",
    "        token.lemma_, # base form\n",
    "        token.pos_, # simple POS tag\n",
    "        token.tag_, # detailed POS tag\n",
    "        token.dep_, # syntactic dependency or relation between tokens\n",
    "        token.shape_, # word shape - cap, punc, digit\n",
    "        token.is_alpha, \n",
    "        token.is_stop))\n",
    "    \n",
    "spacy.explain('ADP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"bae031bf066443caa6671f1cf78e886c-0\" class=\"displacy\" width=\"1700\" height=\"362.0\" direction=\"ltr\" style=\"max-width: none; height: 362.0px; color: white; background: #09a3d5; font-family: Source Sans Pro; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1550\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1550\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bae031bf066443caa6671f1cf78e886c-0-0\" stroke-width=\"2px\" d=\"M62,227.0 62,177.0 347.0,177.0 347.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bae031bf066443caa6671f1cf78e886c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,229.0 L58,221.0 66,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bae031bf066443caa6671f1cf78e886c-0-1\" stroke-width=\"2px\" d=\"M212,227.0 212,202.0 344.0,202.0 344.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bae031bf066443caa6671f1cf78e886c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M212,229.0 L208,221.0 216,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bae031bf066443caa6671f1cf78e886c-0-2\" stroke-width=\"2px\" d=\"M362,227.0 362,202.0 494.0,202.0 494.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bae031bf066443caa6671f1cf78e886c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M494.0,229.0 L498.0,221.0 490.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bae031bf066443caa6671f1cf78e886c-0-3\" stroke-width=\"2px\" d=\"M512,227.0 512,202.0 644.0,202.0 644.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bae031bf066443caa6671f1cf78e886c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M644.0,229.0 L648.0,221.0 640.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bae031bf066443caa6671f1cf78e886c-0-4\" stroke-width=\"2px\" d=\"M812,227.0 812,202.0 944.0,202.0 944.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bae031bf066443caa6671f1cf78e886c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M812,229.0 L808,221.0 816,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bae031bf066443caa6671f1cf78e886c-0-5\" stroke-width=\"2px\" d=\"M662,227.0 662,177.0 947.0,177.0 947.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bae031bf066443caa6671f1cf78e886c-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M947.0,229.0 L951.0,221.0 943.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bae031bf066443caa6671f1cf78e886c-0-6\" stroke-width=\"2px\" d=\"M662,227.0 662,152.0 1100.0,152.0 1100.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bae031bf066443caa6671f1cf78e886c-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,229.0 L1104.0,221.0 1096.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bae031bf066443caa6671f1cf78e886c-0-7\" stroke-width=\"2px\" d=\"M1262,227.0 1262,177.0 1547.0,177.0 1547.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bae031bf066443caa6671f1cf78e886c-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1262,229.0 L1258,221.0 1266,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bae031bf066443caa6671f1cf78e886c-0-8\" stroke-width=\"2px\" d=\"M1412,227.0 1412,202.0 1544.0,202.0 1544.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bae031bf066443caa6671f1cf78e886c-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1412,229.0 L1408,221.0 1416,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bae031bf066443caa6671f1cf78e886c-0-9\" stroke-width=\"2px\" d=\"M1112,227.0 1112,152.0 1550.0,152.0 1550.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bae031bf066443caa6671f1cf78e886c-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1550.0,229.0 L1554.0,221.0 1546.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options = {\"compact\": True, \"bg\": \"#09a3d5\",\n",
    "           \"color\": \"white\", \"font\": \"Source Sans Pro\"}\n",
    "\n",
    "spacy.displacy.render(doc, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e118f4302af74dcba6a635b373be6c60-0\" class=\"displacy\" width=\"1800\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">In</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">ancient</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Rome,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">some</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">neighbors</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">live</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">three</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">adjacent</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">houses.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 925.0,2.0 925.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-0-2\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,354.0 L398.0,342.0 382.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-0-6\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,177.0 1615.0,177.0 1615.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-0-7\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,354.0 L1462,342.0 1478,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-0-8\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,89.5 1620.0,89.5 1620.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1620.0,354.0 L1628.0,342.0 1612.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e118f4302af74dcba6a635b373be6c60-1\" class=\"displacy\" width=\"5300\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">In</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">center</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">house</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Senex,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">who</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">lives</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">there</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">wife</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">Domina,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">son</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">Hero,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">several</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">slaves,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">including</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">head</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">slave</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">Hysterium</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4075\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4075\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4425\">musical</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4425\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4600\">'s</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4600\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4775\">main</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4775\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4950\">character</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5125\">Pseudolus.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5125\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,177.0 565.0,177.0 565.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-1\" stroke-width=\"2px\" d=\"M245,439.5 C245,352.0 380.0,352.0 380.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,441.5 L237,429.5 253,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-2\" stroke-width=\"2px\" d=\"M70,439.5 C70,264.5 385.0,264.5 385.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M385.0,441.5 L393.0,429.5 377.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-3\" stroke-width=\"2px\" d=\"M770,439.5 C770,352.0 905.0,352.0 905.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-4\" stroke-width=\"2px\" d=\"M595,439.5 C595,264.5 910.0,264.5 910.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910.0,441.5 L918.0,429.5 902.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-5\" stroke-width=\"2px\" d=\"M945,439.5 C945,352.0 1080.0,352.0 1080.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1080.0,441.5 L1088.0,429.5 1072.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-6\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,352.0 1255.0,352.0 1255.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1255.0,441.5 L1263.0,429.5 1247.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-7\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,352.0 1605.0,352.0 1605.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,441.5 L1462,429.5 1478,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-8\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,264.5 1610.0,264.5 1610.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1610.0,441.5 L1618.0,429.5 1602.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-9\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,352.0 1780.0,352.0 1780.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1780.0,441.5 L1788.0,429.5 1772.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-10\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,264.5 1960.0,264.5 1960.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1960.0,441.5 L1968.0,429.5 1952.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-11\" stroke-width=\"2px\" d=\"M2170,439.5 C2170,352.0 2305.0,352.0 2305.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2170,441.5 L2162,429.5 2178,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-12\" stroke-width=\"2px\" d=\"M1995,439.5 C1995,264.5 2310.0,264.5 2310.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2310.0,441.5 L2318.0,429.5 2302.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-13\" stroke-width=\"2px\" d=\"M2520,439.5 C2520,352.0 2655.0,352.0 2655.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2520,441.5 L2512,429.5 2528,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-14\" stroke-width=\"2px\" d=\"M2345,439.5 C2345,264.5 2660.0,264.5 2660.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2660.0,441.5 L2668.0,429.5 2652.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-15\" stroke-width=\"2px\" d=\"M2345,439.5 C2345,177.0 2840.0,177.0 2840.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2840.0,441.5 L2848.0,429.5 2832.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-16\" stroke-width=\"2px\" d=\"M3045,439.5 C3045,352.0 3180.0,352.0 3180.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3045,441.5 L3037,429.5 3053,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-17\" stroke-width=\"2px\" d=\"M2345,439.5 C2345,89.5 3195.0,89.5 3195.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3195.0,441.5 L3203.0,429.5 3187.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-18\" stroke-width=\"2px\" d=\"M3220,439.5 C3220,352.0 3355.0,352.0 3355.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3355.0,441.5 L3363.0,429.5 3347.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-19\" stroke-width=\"2px\" d=\"M3570,439.5 C3570,264.5 3885.0,264.5 3885.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3570,441.5 L3562,429.5 3578,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-20\" stroke-width=\"2px\" d=\"M3745,439.5 C3745,352.0 3880.0,352.0 3880.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3745,441.5 L3737,429.5 3753,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-21\" stroke-width=\"2px\" d=\"M3395,439.5 C3395,177.0 3890.0,177.0 3890.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3890.0,441.5 L3898.0,429.5 3882.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-22\" stroke-width=\"2px\" d=\"M3920,439.5 C3920,352.0 4055.0,352.0 4055.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4055.0,441.5 L4063.0,429.5 4047.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-23\" stroke-width=\"2px\" d=\"M4270,439.5 C4270,352.0 4405.0,352.0 4405.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4270,441.5 L4262,429.5 4278,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-24\" stroke-width=\"2px\" d=\"M4445,439.5 C4445,177.0 4940.0,177.0 4940.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4445,441.5 L4437,429.5 4453,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-25\" stroke-width=\"2px\" d=\"M4445,439.5 C4445,352.0 4580.0,352.0 4580.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-25\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4580.0,441.5 L4588.0,429.5 4572.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-26\" stroke-width=\"2px\" d=\"M4795,439.5 C4795,352.0 4930.0,352.0 4930.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-26\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4795,441.5 L4787,429.5 4803,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-27\" stroke-width=\"2px\" d=\"M3920,439.5 C3920,2.0 4950.0,2.0 4950.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-27\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4950.0,441.5 L4958.0,429.5 4942.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e118f4302af74dcba6a635b373be6c60-1-28\" stroke-width=\"2px\" d=\"M4970,439.5 C4970,352.0 5105.0,352.0 5105.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e118f4302af74dcba6a635b373be6c60-1-28\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5105.0,441.5 L5113.0,429.5 5097.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# long text each sent\n",
    "\n",
    "text = \"\"\"In ancient Rome, some neighbors live in three adjacent houses. In the center is the house of Senex, who lives there with wife Domina, son Hero, and several slaves, including head slave Hysterium and the musical's main character Pseudolus. \"\"\"\n",
    "#A slave belonging to Hero, Pseudolus wishes to buy, win, or steal his freedom. One of the neighboring houses is owned by Marcus Lycus, who is a buyer and seller of beautiful women; the other belongs to the ancient Erronius, who is abroad searching for his long-lost children (stolen in infancy by pirates). One day, Senex and Domina go on a trip and leave Pseudolus in charge of Hero. Hero confides in Pseudolus that he is in love with the lovely Philia, one of the courtesans in the House of Lycus (albeit still a virgin).\"\"\"\n",
    "doc = nlp(text)\n",
    "sentence_spans = list(doc.sents)\n",
    "spacy.displacy.render(sentence_spans, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">When \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sebastian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " Thrun started working on self-driving cars at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2007\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", few people outside of the company took him seriously.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n",
    "doc = nlp(text)\n",
    "spacy.displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based Morphology\n",
    "\n",
    "- a **lemma** (root) is **inflected** (modified/combined) with one or more **morphological features** to create a surface form.\n",
    "- I read the paper yesterday.\n",
    "    - SURFACE: read\n",
    "    - LEMMA: read\n",
    "    - POS: verb\n",
    "    - MORPHOLOGICAL FEAT.: VerbForm=Fin, Mood=Ind, Tense=Past\n",
    "- VerbForm=Ger (gerand), Mood=Subj, Tense=Future, etc\n",
    "- English is simple morphologically, spaCy uses rules that can be **keyed by the token, POS tag, or mix**\n",
    "- Logic\n",
    "    - tokenizer lookup **MAPPING TABLE** `TOKENIZER_EXCEPTIONS`, allowing seq of char be mapped to multiple tokens. Each token may be assigned a POS and >=1 morpho-features\n",
    "    - POS tagger then assigns each token **extended POS tag** - `Token.tag` expressing POS and some morpho-info, e.g. Verbe Tense=Past\n",
    "    - Words POS not set, a **MAPPING TABLE** `TAG_MAP` maps the tags to a POS and a set of morpho-features\n",
    "    - **rule-based deterministic lemmatizer** maps the surface form, to a lemma in light of the assigned extended POS and morpho info, without consulting the context of the token! (lemmatizer also accepts list-based exception files, acquired from WordNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing (Need Model)\n",
    "\n",
    "- fast navigating the dep-tree\n",
    "- sentence boundary detection\n",
    "- iterate over base noun phrases or \"chunks\"\n",
    "- `doc.is_parsed` -> bool\n",
    "\n",
    "**NOUN CHUNCKS**\n",
    "\n",
    "- \"base noun phrases\" - flat phrases having a noun as HEAD - sort of \"noun plus words describing it\" (the lavish green grass or the world's largest tech fund)\n",
    "- root.text - original text connecting them\n",
    "    - cars\n",
    "    - liability\n",
    "    - manufacturers\n",
    "- root.dep - dep relation connecting root to its head\n",
    "    - nsubj\n",
    "    - dobj\n",
    "    - proj\n",
    "- root.head.text - root token's head text\n",
    "    - shift\n",
    "    - shift\n",
    "    - toward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, \n",
    "          chunk.root.text, \n",
    "          chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NAVIGATING PARSE TREE**\n",
    "\n",
    "- spaCy uses **head, child** to repr words **connected by single arc** in dep-tree\n",
    "- `.dep` is hash and `.dep_` string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text    \tdep     \t head-text\thead-POS\tchildren\n",
      "\n",
      "Autonomous\tamod    \tcars    \tNOUN    \t[]\n",
      "cars    \tnsubj   \tshift   \tVERB    \t[Autonomous]\n",
      "shift   \tROOT    \tshift   \tVERB    \t[cars, liability]\n",
      "insurance\tcompound\tliability\tNOUN    \t[]\n",
      "liability\tdobj    \tshift   \tVERB    \t[insurance, toward]\n",
      "toward  \tprep    \tliability\tNOUN    \t[manufacturers]\n",
      "manufacturers\tpobj    \ttoward  \tADP     \t[]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "print(\"{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\".format('text', 'dep', ' head-text', 'head-POS', 'children'))\n",
    "print()\n",
    "for token in doc:\n",
    "    print(\"{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\\t{}\".format(token.text,\n",
    "                                      token.dep_, \n",
    "                                      token.head.text, \n",
    "                                      token.head.pos_,\n",
    "                                      [child for child in token.children]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dep: syntactic link connecting child to head\n",
    "- head.text: token head text\n",
    "- head.pos: POS tag of token head\n",
    "- children: immediate syntactic deps of token\n",
    "\n",
    "> BECAUSE SYNTATIC RELATIONS FROM A TREE - EACH WORD ONE HEAD\n",
    "\n",
    "- hence iterate over the arcs in tree by words in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{shift}\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "\n",
    "# Finding a verb with a subject from below — good\n",
    "verbs = set()\n",
    "for possible_subject in doc:\n",
    "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "        verbs.add(possible_subject.head)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-b9794b3ed655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpossible_subject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossible_verb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpossible_subject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnsubj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0mverbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossible_verb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# less good \n",
    "# iterate twice, once for the head, then again via children\n",
    "for possible_verb in doc:\n",
    "    if possible_verb.pos == VERB:\n",
    "        for possible_subject in possible_verb.children:\n",
    "            if possible_subject.dep == nsubj:\n",
    "                verbs.append(possible_verb)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterating around local tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bright', 'red']\n",
      "['on']\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"bright red apples on the tree\")\n",
    "print([token.text for token in doc[2].lefts])  # ['bright', 'red']\n",
    "print([token.text for token in doc[2].rights])  # ['on']\n",
    "print(doc[2].n_lefts)  # 2\n",
    "print(doc[2].n_rights)  # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **get whole phrase by head using `Token.subtree` returning an ordered seq tokens - up the tree with `Token.ancestors` and dominance with `Token.is_ancestor`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT    \tDEP     \tN_LEFTS \tN_RIGHETS\tANCESTORS\n",
      "\n",
      "Credit  \tnmod    \t0       \t2       \t['account', 'holders', 'submit']\n",
      "and     \tcc      \t0       \t0       \t['Credit', 'account', 'holders', 'submit']\n",
      "mortgage\tconj    \t0       \t0       \t['Credit', 'account', 'holders', 'submit']\n",
      "account \tcompound\t1       \t0       \t['holders', 'submit']\n",
      "holders \tnsubj   \t1       \t0       \t['submit']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Credit and mortgage account holders must submit their requests\")\n",
    "print(\"{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\".format(\n",
    "    'TEXT', 'DEP', 'N_LEFTS', 'N_RIGHETS', 'ANCESTORS'))\n",
    "print()\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "subject = list(root.lefts)[0]\n",
    "for descendant in subject.subtree:\n",
    "    assert subject is descendant or subject.is_ancestor(descendant)\n",
    "    print(\"{:<8s}\\t{:<8s}\\t{:<8}\\t{:<8}\\t{}\".format(\n",
    "        descendant.text, descendant.dep_, descendant.n_lefts,\n",
    "            descendant.n_rights,\n",
    "            [ancestor.text for ancestor in descendant.ancestors]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `.left_edge` and right esp. useful as giving first and last token of the subtree - **easiest way to create a `Span` for a syntactic phrase`** (RIGHT_EDGE IS WITHIN SUBTREE, +1 AS END-POINT OF RANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT\tPOS\tDEP\tHEAD TEXT\n",
      "\n",
      "Credit and mortgage account holders\tNOUN\tnsubj\tsubmit\n",
      "must\tVERB\taux\tsubmit\n",
      "submit\tVERB\tROOT\tsubmit\n",
      "their\tDET\tposs\trequests\n",
      "requests\tNOUN\tdobj\tsubmit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Credit and mortgage account holders must submit their requests\")\n",
    "span = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(span)\n",
    "print(\"{:8<}\\t{:8<}\\t{:8<}\\t{:8<}\".format(\n",
    "    'TEXT', 'POS', 'DEP', 'HEAD TEXT'))\n",
    "print()\n",
    "for token in doc:\n",
    "    print(\"{:8<}\\t{:8<}\\t{:8<}\\t{:8<}\".format(\n",
    "        token.text, token.pos_, token.dep_, token.head.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISABLING PARSER**\n",
    "\n",
    "- If no need any syntactic info, should disable parser\n",
    "- Load and run faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity\n",
    "\n",
    "- stats-model prediction\n",
    "\n",
    "**ACCESS**\n",
    "\n",
    "- `doc.ents` => `Span` sequence\n",
    "- accessed either as hash or string `ent.label, ent.label_`\n",
    "- also at `token.ent_iob` and `token.ent_type` \n",
    "- IOB indicates whether an NE starts, continues or ends on the tag\n",
    "    - I inside an NE\n",
    "    - O outside \n",
    "    - B beginning of NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('San Francisco', 0, 13, 'GPE')]\n",
      "['San', 'B', 'GPE']\n",
      "['Francisco', 'I', 'GPE']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "\n",
    "# token level\n",
    "ent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]\n",
    "ent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]\n",
    "print(ent_san)  # ['San', 'B', 'GPE']\n",
    "print(ent_francisco)  # ['Francisco', 'I', 'GPE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting NE Annotations\n",
    "\n",
    "- ensure consistency, set at **document level**\n",
    "- BUT CANNOT write directly to `token.ent_iob, token.ent_type` \n",
    "- SO easiest to set by assigning to `doc.ents` and create new NE as `Span`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before []\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"fb is hiring a new vice president of global policy\")\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('Before', ents)\n",
    "# the model didn't recognise \"fb\" as an entity :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After [('fb', 0, 2, 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "fb_ent = Span(doc, 0, 1, label=\"ORG\")\n",
    "doc.ents = list(doc.ents) + [fb_ent]\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) \n",
    "       for e in doc.ents]\n",
    "print('After', ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fb"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting NE from array\n",
    "\n",
    "- `doc.from_array` with both `ENT_TYPE, ENT_IOB` in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ()\n",
      "After (London,)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from spacy.attrs import ENT_IOB, ENT_TYPE\n",
    "\n",
    "doc = nlp.make_doc(\"London is a big city in the United Kingdom.\")\n",
    "print(\"Before\", doc.ents)  # []\n",
    "\n",
    "header = [ENT_IOB, ENT_TYPE]\n",
    "attr_array = numpy.zeros((len(doc), len(header)))\n",
    "attr_array[0, 0] = 3  # B\n",
    "attr_array[0, 1] = doc.vocab.strings[\"GPE\"]\n",
    "doc.from_array(header, attr_array)\n",
    "print(\"After\", doc.ents)  # [London]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting NE in Cython\n",
    "\n",
    "- writing to underlying struc\n",
    "- efficient native code\n",
    "\n",
    "```cython\n",
    "# cython: infer_types=True\n",
    "from spacy.tokens.doc cimport Doc\n",
    "\n",
    "cpdef set_entity(Doc doc, int start, int end, int ent_type):\n",
    "    for i in range(start, end):\n",
    "        doc.c[i].ent_type = ent_type\n",
    "    doc.c[start].ent_iob = 3\n",
    "    for i in range(start+1, end):\n",
    "        doc.c[i].ent_iob = 2\n",
    "```\n",
    "\n",
    "- if writing to `TokenC*` structs, responsible for ensuring data is left in a consistent state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Linking\n",
    "\n",
    "- resolve a textual entity to UID from KB\n",
    "- (processing scripts)[https://github.com/explosion/spaCy/tree/master/bin/wiki_entity_linking] use WikiData identifiers but can create own KB and train new EL model using custom-made KB (see KB API and Training later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acessing Entity Identifiers\n",
    "\n",
    "- either a hash or string using `ent.kb_id, ent.kb_id_` of a Span, or `ent_kb_id/_` on Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "- input string, output `Doc`\n",
    "- make `Doc` requires `Vocab` instance, a seq of word strings and optional seq of spaces booleans\n",
    "- non-destructive, `doc.text == input_text`\n",
    "- first segment into words, punc, etc by applying rules specific to each lang\n",
    "- `text.split(' ')`\n",
    "- on each substring\n",
    "    1. **Does the substring match a tokenizer exception rule?** e.g. 'don't' does not contian whitespace, but should be split into two tokens, 'do' and 'n't' while U.K. should always remain one token\n",
    "    2. **Can a prefix, suffix or infix be split off?** e.g. puncs like commas, periods, hypens or quotes\n",
    "- if matched, the rule is applied and continues its loop starting with the newly split substrings\n",
    "- spaCy can split complex, nested tokens like combinations of abbreviations and multiple punctuation marks\n",
    "\n",
    "![exmaple](https://spacy.io/tokenization-57e618bd79d933c4ccd308b5739062d6.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer Data\n",
    "\n",
    "- Global and lang-specific tokenizer data is supplied via lang-data\n",
    "\n",
    "![Language data](https://spacy.io/language_data-ef63e6a58b7ec47c073fb59857a76e5f.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Tokenisation Rules\n",
    "\n",
    "- very certain expressions, or abbreviations only used there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gimme', 'that']\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(\"gimme that\")\n",
    "print([w.text for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gim', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "\n",
    "print([w.text for w in nlp(\"gimme that\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> doesn't have to match entire whitespace-delimited substring, tokenizer will incrementally split off puncs and keep looking up remaining substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.text for w in nlp(\"gimme!\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(', '\"', '...', 'gim', 'me', '...', '?', '\"', ')']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.text for w in nlp('(\"...gimme...?\")')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How it works?\n",
    "\n",
    "- handle \"don't\" as well as \"(don't)!\" via splitting off the open bracket, then the exclamation, then close bracket, finally matching special case\n",
    "\n",
    "```python\n",
    "def tokenizer_pseudo_code(self, special_cases, prefix_search, suffix_search,\n",
    "                          infix_finditer, token_match):\n",
    "    tokens = []\n",
    "    for substring in text.split():\n",
    "        suffixes = []\n",
    "        while substring:\n",
    "            while prefix_search(substring) or suffix_search(substring):\n",
    "                if substring in special_cases:\n",
    "                    tokens.extend(special_cases[substring])\n",
    "                    substring = ''\n",
    "                    break\n",
    "                if prefix_search(substring):\n",
    "                    split = prefix_search(substring).end()\n",
    "                    tokens.append(substring[:split])\n",
    "                    substring = substring[split:]\n",
    "                    if substring in special_cases:\n",
    "                        continue\n",
    "                if suffix_search(substring):\n",
    "                    split = suffix_search(substring).start()\n",
    "                    suffixes.append(substring[split:])\n",
    "                    substring = substring[:split]\n",
    "            if substring in special_cases:\n",
    "                tokens.extend(special_cases[substring])\n",
    "                substring = ''\n",
    "            elif token_match(substring):\n",
    "                tokens.append(substring)\n",
    "                substring = ''\n",
    "            elif list(infix_finditer(substring)):\n",
    "                infixes = infix_finditer(substring)\n",
    "                offset = 0\n",
    "                for match in infixes:\n",
    "                    tokens.append(substring[offset : match.start()])\n",
    "                    tokens.append(substring[match.start() : match.end()])\n",
    "                    offset = match.end()\n",
    "                if substring[offset:]:\n",
    "                    tokens.append(substring[offset:])\n",
    "                substring = ''\n",
    "            elif substring:\n",
    "                tokens.append(substring)\n",
    "                substring = ''\n",
    "        tokens.extend(reversed(suffixes))\n",
    "    return tokens\n",
    "```\n",
    "\n",
    "1. iterate over whitespace-separated substrings\n",
    "2. check if an explicitly defined rule for this substring, if so use it\n",
    "3. else try to consume one predix, if so go back to 2) so that special cases always get priority\n",
    "4. if didn't consume predix, try to consume a suffix then go to 2)\n",
    "5. if can't consume either, look for special case\n",
    "6. look for token match\n",
    "7. look for \"infixes\" - stuff like hypens etc and split the substring into tokens on all infixes\n",
    "8. once can't consume any more of the string, handle it as a single token\n",
    "\n",
    "#### Debugging Tokenizer V2.2.3\n",
    "\n",
    "- above pseudo-code is at `nlp.tokenizer.explain(text)`\n",
    "- returns a list of tuples showing which tokenizer rule or pattern was matched for each token\n",
    "- tokens produced are same as `nlp.tokenizer()` except for whitespace tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokenizer.Tokenizer' object has no attribute 'explain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8708ce5c8d4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'''\"Let's go!\"'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtok_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtok_exp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokenizer.Tokenizer' object has no attribute 'explain'"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "text = '''\"Let's go!\"'''\n",
    "doc = nlp(text)\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "print(tok_exp)\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\t\", t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customising Tokenizer Class\n",
    "\n",
    "Let’s imagine you wanted to create a tokenizer for a new language or specific domain. There are five things you would need to define:\n",
    "\n",
    "1. A dictionary of special cases. This handles things like contractions, units of measurement, emoticons, certain abbreviations, etc.\n",
    "2. A function prefix_search, to handle preceding punctuation, such as open quotes, open brackets, etc.\n",
    "3. A function suffix_search, to handle succeeding punctuation, such as commas, periods, close quotes, etc.\n",
    "4. A function infixes_finditer, to handle non-whitespace separators, such as hyphens etc.\n",
    "5. An optional boolean function token_match matching strings that should never be split, overriding the infix rules. Useful for things like URLs or numbers. Note that prefixes and suffixes will be split off before token_match is applied.\n",
    "\n",
    "> shouldn't usually need to create a Tokenizer subclass, standard usage is to use `re.compile()` to build a regex object, pass its `.search()` and `.finditer()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oceanbao/miniconda3/envs/dev/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Possible nested set at position 2\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'world.', ':)']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "prefix_re = re.compile(r'''^[[(\"']''')\n",
    "suffix_re = re.compile(r'''[])\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases,\n",
    "                                prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=simple_url_re.match)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "doc = nlp(\"hello-world. :)\")\n",
    "print([t.text for t in doc]) # ['hello', '-', 'world.', ':)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you need to subclass the tokenizer instead, the relevant methods to specialize are find_prefix, find_suffix and find_infix.\n",
    "\n",
    "Important note\n",
    "When customizing the prefix, suffix and infix handling, remember that you’re passing in functions for spaCy to execute, e.g. prefix_re.search – not just the regular expressions. This means that your functions also need to define how the rules should be applied. For example, if you’re adding your own prefix rules, you need to make sure they’re only applied to characters at the beginning of a token, e.g. by adding ^. Similarly, suffix rules should only be applied at the end of a token, so your expression should end with a $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EntityRuler\n",
    "\n",
    "- new component allows adding NE based on pattern dict - combining rules and statistical NER\n",
    "\n",
    "#### Entity Patterns\n",
    "\n",
    "- dict with \"label\" and \"pattern\" \n",
    "- **phrase patterns** exact string\n",
    "    - `{\"label\": \"ORG\", \"pattern\": \"Apple\"}`\n",
    "- **token patterns** describing one token (list)\n",
    "    - `{\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}`\n",
    "    \n",
    "#### Usage\n",
    "\n",
    "- typically added to doc.ents after match found\n",
    "- designed to integrate with stats-model, if added BEFORE \"ner\", it will respect existing NE spans and adjust its predictions around it, if added AFTER \"ner\", will only add spans to `doc.ents` if NO OVERLAP \n",
    "- to OVERWRITE NEs, set `overwrite_ents=True` on init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG'), ('San Francisco', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = English()\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MyCorp Inc.', 'ORG'), ('U.S.', 'GPE')]\n",
      "['tagger', 'parser', 'ner', 'entity_ruler']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(\"MyCorp Inc. is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating and debugging patterns\n",
    "\n",
    "- validate agasint JSON schema\n",
    "\n",
    "`ruler = EntityRuler(nlp, validate=True)`\n",
    "\n",
    "#### Adding IDs to patterns\n",
    "\n",
    "- also accept an `id` attribute for each pattern\n",
    "- allows multiple patterns to be associated with SAME NE !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG', 'apple'), ('San Francisco', 'GPE', 'san-francisco')]\n",
      "[('Apple', 'ORG', 'apple'), ('San Fran', 'GPE', 'san-francisco')]\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}], \"id\": \"san-francisco\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"fran\"}], \"id\": \"san-francisco\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc1 = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])\n",
    "\n",
    "doc2 = nlp(\"Apple is opening its first big office in San Fran.\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc2.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> if `id` included, `ent_id` property of the matched NE is set to `id` !!! so in the example above its easy to identitfy that both patterns mapped to same NE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pattern files\n",
    "\n",
    "- serialisation let saving and loading patterns to and fro JSONL files, one pattern per line\n",
    "\n",
    "```jsonl\n",
    "# patterns.jsonl\n",
    "{\"label\": \"ORG\", \"pattern\": \"Apple\"}\n",
    "{\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}\n",
    "```\n",
    "\n",
    "```python\n",
    "ruler.to_disk('./patterns.jsonl')\n",
    "new_ruler = EntityRuler(nlp).from_disk('./patterns.jsonl')\n",
    "```\n",
    "\n",
    "- serialisation is auto-exported to disk dir\n",
    "- same as dir contianing the jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = EntityRuler(nlp)\n",
    "ruler.add_patterns([{\"label\": \"ORG\", \"pattern\": \"Apple\"}])\n",
    "nlp.add_pipe(ruler)\n",
    "nlp.to_disk(\"/path/to/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Model and Rule\n",
    "\n",
    "- rules augments stats-model by **presetting tags, NE or sentence boundaries for specific tokens**\n",
    "- stats-model can sometimes improves the accuracy of OTHER DECISIONS!!\n",
    "- post-model correction of common errors\n",
    "- referenced attributes set by models, to implement more abstract logic ?!\n",
    "\n",
    "#### Example: Expanding NE\n",
    "\n",
    "- maybe only partial NE - either incorrect prediction or if NE type defined in original corppose mismatch context\n",
    "- e.g. Mr. Dr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dr Alex Smith', 'PERSON'), ('Acme Corp Inc.', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.tokens import Span\n",
    "\n",
    "def expand_person_entities(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and ent.start != 0:\n",
    "            prev_token = doc[ent.start - 1]\n",
    "            if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
    "                new_ents.append(new_ent)\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "# Add the component after the named entity recognizer\n",
    "nlp.add_pipe(expand_person_entities, after='ner')\n",
    "\n",
    "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- alternative **extension attribute** `._.person_title` adding to Span \n",
    "- advantage being NE text stays intact and can still be used to look up the name in a KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dr Alex Smith', 'PERSON', None), ('Acme Corp Inc.', 'ORG', None)]\n"
     ]
    }
   ],
   "source": [
    "def get_person_title(span):\n",
    "    if span.label_ == \"PERSON\" and span.start != 0:\n",
    "        prev_token = span.doc[span.start - 1]\n",
    "        if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "            return prev_token.text\n",
    "\n",
    "# Register the Span extension as 'person_title'\n",
    "Span.set_extension(\"person_title\", getter=get_person_title)\n",
    "\n",
    "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_, ent._.person_title) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: POS Tags and Depen-Parse with NE\n",
    "\n",
    "- case: past and present occupation NER !!\n",
    "- trigger word: \"work\" being PASTE TENSE or PRESENT TENSE, \n",
    "- whether company names are attached to it and whether person is the subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alex Smith', 'PERSON'), ('Acme Corp Inc.', 'ORG')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"25dba222282f4a59b94e110f6fdb45c5-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Alex</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Smith</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">worked</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VBD</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">IN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Acme</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Corp</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Inc.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-25dba222282f4a59b94e110f6fdb45c5-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-25dba222282f4a59b94e110f6fdb45c5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-25dba222282f4a59b94e110f6fdb45c5-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-25dba222282f4a59b94e110f6fdb45c5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-25dba222282f4a59b94e110f6fdb45c5-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-25dba222282f4a59b94e110f6fdb45c5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-25dba222282f4a59b94e110f6fdb45c5-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-25dba222282f4a59b94e110f6fdb45c5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,179.0 L762,167.0 778,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-25dba222282f4a59b94e110f6fdb45c5-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-25dba222282f4a59b94e110f6fdb45c5-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-25dba222282f4a59b94e110f6fdb45c5-0-5\" stroke-width=\"2px\" d=\"M595,177.0 C595,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-25dba222282f4a59b94e110f6fdb45c5-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "spacy.displacy.render(doc, options={'fine_grained': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"worked\" is ROOT of sentence and past tense verb\n",
    "- subj is \"AS\", the person who worked at \"Acme Corp Inc.\" the **prepositional phrase** attached to verb \"worked\"\n",
    "- workflow\n",
    "    - find predicted PERSON NE,\n",
    "    - find their HEAD and check if attached to trigger word \"work\"\n",
    "    - check for prepo phrases attached to HEAD and if containing ORG NE\n",
    "    - find out if company affiliation is current by HEAD's POS TAG tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': Alex Smith, 'orgs': [Inc.], 'past': True}\n"
     ]
    }
   ],
   "source": [
    "person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "for ent in person_entities:\n",
    "    # Because the entity is a spans, we need to use its root token. The head\n",
    "    # is the syntactic governor of the person, e.g. the verb\n",
    "    head = ent.root.head\n",
    "    if head.lemma_ == \"work\":\n",
    "        # Check if the children contain a preposition\n",
    "        preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "        for prep in preps:\n",
    "            # Check if tokens part of ORG entities are in the preposition's\n",
    "            # children, e.g. at -> Acme Corp Inc.\n",
    "            orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n",
    "            # If the verb is in past tense, the company was a previous company\n",
    "            print({'person': ent, 'orgs': orgs, 'past': head.tag_ == \"VBD\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create custom pipeline \n",
    "- above logic expects NE are merged into single tokens !!! `merge_entities`\n",
    "- write to custom attributes on entity span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': Alex Smith, 'orgs': [Acme Corp Inc.], 'past': True}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5b28529acbc340dabe7fa8dbdb7b39d4-0\" class=\"displacy\" width=\"750\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Alex Smith</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">worked</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VBD</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">IN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Acme Corp Inc.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5b28529acbc340dabe7fa8dbdb7b39d4-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5b28529acbc340dabe7fa8dbdb7b39d4-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5b28529acbc340dabe7fa8dbdb7b39d4-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5b28529acbc340dabe7fa8dbdb7b39d4-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5b28529acbc340dabe7fa8dbdb7b39d4-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5b28529acbc340dabe7fa8dbdb7b39d4-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy.pipeline import merge_entities\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_person_orgs(doc):\n",
    "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    for ent in person_entities:\n",
    "        head = ent.root.head\n",
    "        if head.lemma_ == \"work\":\n",
    "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "            for prep in preps:\n",
    "                orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n",
    "                print({'person': ent, 'orgs': orgs, 'past': head.tag_ == \"VBD\"})\n",
    "    return doc\n",
    "\n",
    "# To make the entities easier to work with, we'll merge them into single tokens\n",
    "nlp.add_pipe(merge_entities)\n",
    "nlp.add_pipe(extract_person_orgs)\n",
    "\n",
    "doc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n",
    "# If you're not in a Jupyter / IPython environment, use displacy.serve\n",
    "displacy.render(doc, options={'fine_grained': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person_orgs(doc):\n",
    "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    for ent in person_entities:\n",
    "        head = ent.root.head\n",
    "        if head.lemma_ == \"work\":\n",
    "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "            for prep in preps:\n",
    "                orgs = [t for t in prep.children if t.ent_type_ == \"ORG\"]\n",
    "                aux = [token for token in head.children if token.dep_ == \"aux\"]                \n",
    "                past_aux = any(t.tag_ == \"VBD\" for t in aux)                \n",
    "                past = head.tag_ == \"VBD\" or head.tag_ == \"VBG\" and past_aux                \n",
    "                print({'person': ent, 'orgs': orgs, 'past': past})\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "- `Tokenizer` \"tokenizer\" -> `Doc` tokens\n",
    "- `Tagger` \"tagger\" -> `Doc[i].tag` POS tags\n",
    "- `DependencyParser` \"parser\" -> `Doc[i].head, Doc[i].dep, Doc.sents, Doc.noun_chunks`\n",
    "- etc\n",
    "- ALWAYS DEPENDS ON STATS-MODEL and its capabilities\n",
    "\n",
    "**ORDER OF PIPELINE**\n",
    "> v2.x, stats-models like tagger or parser are INDEPENDENT and don't share any data between themselves. E.g. NER doesn't use any features set by tagger and parser, and so on. This means CAN SWAP, REMOVE without affecting others\n",
    "\n",
    "> BUT, custom componeents depending on annotations set by others - e.g. a custom lemmatier may need tags, so order matteres. \n",
    "\n",
    "**TOKENIZER SPECIAL**\n",
    "> \"tokenizer\" hidden since there can ONLY be one tokenizer, and while all other pipeline components take `Doc` and return it, the tokenizer takes a **string of text** and turns it into `Doc` - still customisable via `nlp.tokenizer` writable\n",
    "\n",
    "### Processing Text\n",
    "\n",
    "- when processing large volumes of text, the **STATS-MODELS ARE MORE EFFICIENT IF LET THEM WORK ON BATCHES OF TEXTS** via `nlp.pipe` batching internally\n",
    "\n",
    "### How it works\n",
    "\n",
    "- when loading, spaCy consults `meta.json`\n",
    "- load language class and data via `get_lang_class` and init\n",
    "- **`Language` class contains shared vocab, tokenization rules and lang-specific annotations schemes**\n",
    "- add each pipeline component in order via `add_pipe`\n",
    "- make model data available to `Language` class via `from_disk` from dir\n",
    "\n",
    "- e.g.\n",
    "    - \"en\"\n",
    "    - [\"tagger\", \"parser\", \"ner\"]\n",
    "    - `spacy.lang.en.English`\n",
    "    \n",
    "> **FUNDAMENTALLY, SPACY MODEL HAS 3 COMPONENTS: WEIGHTS, i.e. binary data loaded from dir, PIPELINE of functions called in order, LANG DATA like tokenization rules and annotation schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.load under-hood\n",
    "\n",
    "lang = \"en\"\n",
    "pipeline = [\"tagger\", \"parser\", \"ner\"]\n",
    "data_path = \"path/to/encore\"\n",
    "\n",
    "cls = spacy.util.get_lang_class(lang) # == English()\n",
    "nlp = cls() # init\n",
    "for name in pipeline:\n",
    "    component = nlp.create_pipe(name)\n",
    "    nlp.add_pipe(component)\n",
    "# nlp.from_disk(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E109] Model for component 'tagger' not initialized. Did you forget to load a model, or forget to call begin_training()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-41fbbf3773a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is a sentence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Pipe.require_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E109] Model for component 'tagger' not initialized. Did you forget to load a model, or forget to call begin_training()?"
     ]
    }
   ],
   "source": [
    "# pipeline under-hood\n",
    "doc = nlp.make_doc(\"This is a sentence.\")\n",
    "for name, proc in nlp.pipeline:\n",
    "    doc = proc(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- built-in components also available in `Language.factories` - meaning that can init via `nlp.create_pipe` with string names\n",
    "\n",
    "FACTORIES\n",
    "\n",
    "- tagger, parser, ner, \n",
    "- \"entity_linker\" `from spacy.pipeline import EntityLinker`\n",
    "- \"sentencizer\"\n",
    "- \"merge_noun_chunks\" - merge all noun chunks into single token - should be after tagger and parser\n",
    "- \"merge_entities\" - after NER\n",
    "- \"merge_subtokens\" - merge subtokens predicted by parser into single tokens, after parser\n",
    "\n",
    "### Custom Pipeline\n",
    "\n",
    "```python\n",
    "def my_component(doc):\n",
    "    # do doc\n",
    "    return doc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'print_info']\n",
      "After tokenization, this doc has 5 tokens.\n",
      "The part-of-speech tags are: ['DET', 'AUX', 'DET', 'NOUN', 'PUNCT']\n",
      "This is a pretty short document.\n"
     ]
    }
   ],
   "source": [
    "def my_component(doc):\n",
    "    print(\"After tokenization, this doc has {} tokens.\".format(len(doc)))\n",
    "    print(\"The part-of-speech tags are:\", [token.pos_ for token in doc])\n",
    "    if len(doc) < 10:\n",
    "        print(\"This is a pretty short document.\")\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(my_component, name=\"print_info\", last=True)\n",
    "print(nlp.pipe_names)  # ['tagger', 'parser', 'ner', 'print_info']\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> wrapping component as class to allow INIT with custom settings and hold state within the component !!!\n",
    "\n",
    "- stateful components, especially ones depending on shared data !!!\n",
    "- e.g. custom `EntityMatcher` init with nlp, a term list and NE label, using `PhraseMatcher` it then matches terms in `Doc` and adds them to NEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barack Obama', 'LOC'), ('tree kangaroo', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "class EntityMatcher(object):\n",
    "    name = \"entity_matcher\"\n",
    "    \n",
    "    def __init__(self, nlp, terms, label):\n",
    "        patterns = [nlp.make_doc(text) for text in terms]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add(label, None, *patterns)\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]\n",
    "        return doc\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "terms = (\"cat\", \"dog\", \"tree kangaroo\", \"giant sea spider\")\n",
    "entity_matcher = EntityMatcher(nlp, terms, \"ANIMAL\")\n",
    "\n",
    "nlp.add_pipe(entity_matcher, after=\"ner\")\n",
    "\n",
    "doc = nlp(\"This is a text about Barack Obama and a tree kangaroo\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Custom sentence segmentation logic\n",
    "\n",
    "- after tokenization but BEFORE \"parser\" - take advantage of sentence boundaries at parser stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is. A sentence. |\n",
      "This is. Another sentence.\n"
     ]
    }
   ],
   "source": [
    "def custom_sentencizer(doc):\n",
    "    for i, token in enumerate(doc[:-2]):\n",
    "        # Define sentence start if pipe + titlecase token\n",
    "        if token.text == \"|\" and doc[i+1].is_title:\n",
    "            doc[i+1].is_sent_start = True\n",
    "        else:\n",
    "            # Explicitly set sentence start to False otherwise to\n",
    "            # tell parser to leave those tokens alone\n",
    "            doc[i+1].is_sent_start = False\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(custom_sentencizer, before=\"parser\")  # Insert before the parser\n",
    "doc = nlp(\"This is. A sentence. | This is. Another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Entity Matching and Tagging with Custom Attributes\n",
    "\n",
    "- taking a term list (firm names) matches occurrences as ORG\n",
    "- merge tokens \n",
    "- set custom `._.is_tech_org` attriutes\n",
    "- `PhraseMatcher` applies to Doc\n",
    "\n",
    "[link to github](https://github.com/explosion/spaCy/tree/master/examples/pipeline/custom_component_entities.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding factories\n",
    "\n",
    "- lookup string in internal factories to init\n",
    "- custom component won't trigger this\n",
    "- have to tell spaCy where to find components via `Language.factories`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "Language.factories[\"entity_matcher\"] = lambda nlp, **cfg: EntityMatcher(nlp, **cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ship above code and custom component in package model's `__init__.py`, so it's exec when loading, `**cfg` are passed all the way down from `load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"custom_model\", terms=[\"tree kangaroo\"], label=\"ANIMAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension attributes\n",
    "\n",
    "- storage of additional info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RsJPhaFPFWrE",
    "toc-hr-collapsed": true
   },
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKGKYh2LFWrF"
   },
   "source": [
    "### Containers\n",
    "\n",
    "- DOC \n",
    "    - container of sequence of tokens & annotations\n",
    "    - owns data\n",
    "    - made via `Tokenizer`\n",
    "    - mod (inplace) via COMPONENTs of PIPELINE\n",
    "        - `Language` object coordinates COMPONENTS\n",
    "        - raw text -> pipeline -> annotated document\n",
    "        - orchestrate training & serialisation\n",
    "    - SPAN\n",
    "        - view pointer\n",
    "        - slice of `Doc`\n",
    "    - TOKEN\n",
    "        - view pointer\n",
    "        - {word, punctuation, symbol, whitespace, etc}\n",
    "\n",
    "- VOCAB\n",
    "    - look-up tables / meta \n",
    "    - LEXEME\n",
    "        - entry in vocabulary\n",
    "        - word type without annotations (opposed to token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB 0 2 ORG\n"
     ]
    }
   ],
   "source": [
    "# Overriding Labels in Span()\n",
    "\n",
    "from spacy.tokens import Span\n",
    "\n",
    "doc = nlp(u\"FB is hiring a new VP of global policy\")\n",
    "doc.ents = [Span(doc, 0, 1, label=doc.vocab.strings[u\"ORG\"])]\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kxh35Df93aj"
   },
   "source": [
    "### Pipeline\n",
    "\n",
    "- `Language`\n",
    "    - text-processing pipe\n",
    "    - load once per process as `nlp`\n",
    "    - pass instance around application\n",
    "- `Tokenizer`\n",
    "    - segment text & create `Doc`\n",
    "- `Lemmatizer`\n",
    "    - determine base forms of words\n",
    "- `Morphology`\n",
    "    - assign linguistic features \n",
    "    - {lemmas, noun case, verb tense, etc}\n",
    "    - based on word, POS\n",
    "- `Tagger`\n",
    "    - annotate POS on `Doc`\n",
    "- `DependencyParser`\n",
    "    - annotate syntactic dependencies on `Doc`\n",
    "- `EntityRecognizer`\n",
    "    - annotate NER\n",
    "- `TextCategorizer`\n",
    "    - assign categories or labels to `Doc`\n",
    "- `Matcher` \n",
    "    - match seq of tok based on pattern rules (similar to Regex)\n",
    "- `PhraseMatcher`\n",
    "    - match seq of tok based on prahses\n",
    "- `EntityRuler`\n",
    "    - add entity `span` to `Doc` using token-based rules or exact phrase mathces\n",
    "- `Sentencizer`\n",
    "    - custom sent boundary detection logic (no need dependency parsing)\n",
    "- Other func\n",
    "    - auto-apply sth to `Doc`, e.g. merge spans of tokens\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGBdydOHFWrY"
   },
   "source": [
    "### Models and Training Data\n",
    "\n",
    "#### JSON input format for training\n",
    "\n",
    "`convert` commmand converts `.conllu` format used by the Universal Dependencies corpora to training format\n",
    "\n",
    "```json\n",
    "[{\n",
    "    \"id\": int,                      # ID of the document within the corpus\n",
    "    \"paragraphs\": [{                # list of paragraphs in the corpus\n",
    "        \"raw\": string,              # raw text of the paragraph\n",
    "        \"sentences\": [{             # list of sentences in the paragraph\n",
    "            \"tokens\": [{            # list of tokens in the sentence\n",
    "                \"id\": int,          # index of the token in the document\n",
    "                \"dep\": string,      # dependency label\n",
    "                \"head\": int,        # offset of token head relative to token index\n",
    "                \"tag\": string,      # part-of-speech tag\n",
    "                \"orth\": string,     # verbatim text of the token\n",
    "                \"ner\": string       # BILUO label, e.g. \"O\" or \"B-ORG\"\n",
    "            }],\n",
    "            \"brackets\": [{          # phrase structure (NOT USED by current models)\n",
    "                \"first\": int,       # index of first token\n",
    "                \"last\": int,        # index of last token\n",
    "                \"label\": string     # phrase label\n",
    "            }]\n",
    "        }]\n",
    "    }]\n",
    "}]\n",
    "```\n",
    "> EXAMPLE: dep, POS, NER from Wall Street Journal portion of Penn Treebank\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "      \"id\": 42,\n",
    "      \"paragraphs\": [\n",
    "        {\n",
    "          \"raw\": \"In an Oct. 19 review of \\\"The Misanthrope\\\" at Chicago's Goodman Theatre (\\\"Revitalized Classics Take the Stage in Windy City,\\\" Leisure & Arts), the role of Celimene, played by Kim Cattrall, was mistakenly attributed to Christina Haag. Ms. Haag plays Elianti.\",\n",
    "          \"sentences\": [\n",
    "            {\n",
    "              \"tokens\": [\n",
    "                {\n",
    "                  \"head\": 44,\n",
    "                  \"dep\": \"prep\",\n",
    "                  \"tag\": \"IN\",\n",
    "                  \"orth\": \"In\",\n",
    "                  \"ner\": \"O\",\n",
    "                  \"id\": 0\n",
    "                },\n",
    "                {\n",
    "                  \"head\": 3,\n",
    "                  \"dep\": \"det\",\n",
    "                  \"tag\": \"DT\",\n",
    "                  \"orth\": \"an\",\n",
    "                  \"ner\": \"O\",\n",
    "                  \"id\": 1\n",
    "                },\n",
    "                {\n",
    "                  \"head\": 2,\n",
    "                  \"dep\": \"nmod\",\n",
    "                  \"tag\": \"NNP\",\n",
    "                  \"orth\": \"Oct.\",\n",
    "                  \"ner\": \"B-DATE\",\n",
    "                  \"id\": 2\n",
    "                },\n",
    "```\n",
    "\n",
    "#### Lexical data for Vocab\n",
    "\n",
    "- CLI `spacy init-model` to populate vocab loading in **JSONL** file `--jsonl-loc` option\n",
    "- first line defines lang and setting\n",
    "- rest of lines JSON objects desc lexemes\n",
    "- attr set as attributes `Lexeme`\n",
    "- `vocab` output ready-to-use model with `Vocab` containing lexical data\n",
    "\n",
    "**FIRST LINE**\n",
    "\n",
    "```json\n",
    "{\"lang\": \"en\", \"settings\": {\"oov_prob\": -20.502029418945312}}\n",
    "{\"orth\": \".\", \"id\": 1, \"lower\": \".\", \"norm\": \".\", \"shape\": \".\", \"prefix\": \".\", \"suffix\": \".\", \"length\": 1, \"cluster\": \"8\", \"prob\": -3.0678977966308594, \"is_alpha\": false, \"is_ascii\": true, \"is_digit\": false, \"is_lower\": false, \"is_punct\": true, \"is_space\": false, \"is_title\": false, \"is_upper\": false, \"like_url\": false, \"like_num\": false, \"like_email\": false, \"is_stop\": false, \"is_oov\": false, \"is_quote\": false, \"is_left_punct\": false, \"is_right_punct\": false}\n",
    "{\"orth\": \",\", \"id\": 2, \"lower\": \",\", \"norm\": \",\", \"shape\": \",\", \"prefix\": \",\", \"suffix\": \",\", \"length\": 1, \"cluster\": \"4\", \"prob\": -3.4549596309661865, \"is_alpha\": false, \"is_ascii\": true, \"is_digit\": false, \"is_lower\": false, \"is_punct\": true, \"is_space\": false, \"is_title\": false, \"is_upper\": false, \"like_url\": false, \"like_num\": false, \"like_email\": false, \"is_stop\": false, \"is_oov\": false, \"is_quote\": false, \"is_left_punct\": false, \"is_right_punct\": false}\n",
    "{\"orth\": \"the\", \"id\": 3, \"lower\": \"the\", \"norm\": \"the\", \"shape\": \"xxx\", \"prefix\": \"t\", \"suffix\": \"the\", \"length\": 3, \"cluster\": \"11\", \"prob\": -3.528766632080078, \"is_alpha\": true, \"is_ascii\": true, \"is_digit\": false, \"is_lower\": true, \"is_punct\": false, \"is_space\": false, \"is_title\": false, \"is_upper\": false, \"like_url\": false, \"like_num\": false, \"like_email\": false, \"is_stop\": false, \"is_oov\": false, \"is_quote\": false, \"is_left_punct\": false, \"is_right_punct\": false}\n",
    "{\"orth\": \"I\", \"id\": 4, \"lower\": \"i\", \"norm\": \"I\", \"shape\": \"X\", \"prefix\": \"I\", \"suffix\": \"I\", \"length\": 1, \"cluster\": \"346\", \"prob\": -3.791565179824829, \"is_alpha\": true, \"is_ascii\": true, \"is_digit\": false, \"is_lower\": false, \"is_punct\": false, \"is_space\": false, \"is_title\": true, \"is_upper\": true, \"like_url\": false, \"like_num\": false, \"like_email\": false, \"is_stop\": false, \"is_oov\": false, \"is_quote\": false, \"is_left_punct\": false, \"is_right_punct\": false}\n",
    "{\"orth\": \"to\", \"id\": 5, \"lower\": \"to\", \"norm\": \"to\", \"shape\": \"xx\", \"prefix\": \"t\", \"suffix\": \"to\", \"length\": 2, \"cluster\": \"12\", \"prob\": -3.8560216426849365, \"is_alpha\": true, \"is_ascii\": true, \"is_digit\": false, \"is_lower\": true, \"is_punct\": false, \"is_space\": false, \"is_title\": false, \"is_upper\": false, \"like_url\": false, \"like_num\": false, \"like_email\": false, \"is_stop\": false, \"is_oov\": false, \"is_quote\": false, \"is_left_punct\": false, \"is_right_punct\": false}\n",
    "...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-QRhjEV96FA"
   },
   "source": [
    "### Other Classes\n",
    "\n",
    "- `Vocab`\t\n",
    "    - A lookup table for the vocabulary that allows you to access Lexeme objects.\n",
    "- `StringStore`\t\n",
    "    - Map strings to and from hash values.\n",
    "- `Vectors`\t\n",
    "    - Container class for vector data keyed by string.\n",
    "- `GoldParse`\t\n",
    "    - Collection for training annotations.\n",
    "- `GoldCorpus`\t\n",
    "    - An annotated corpus, using the JSON file format. Manages annotations for tagging, dependency parsing and NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmpwObT5FWrG"
   },
   "outputs": [],
   "source": [
    "# Example Text Pipeline\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tokens = nlp(u\"Some\\nspaces  and\\ttab characters\")\n",
    "tokens_text = [t.text for t in tokens]\n",
    "assert tokens_text == [\"Some\", \"\\n\", \"spaces\", \" \", \"and\", \"\\t\", \"tab\", \"characters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-KlUDIdXFWrJ",
    "outputId": "2972c169-ef1c-4c06-fc3f-9f1edbb5b641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adverb'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use spacy.explain to get the description for the string representation of a tag. \n",
    "import spacy\n",
    "spacy.explain(\"RB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7yktxlYFWrP",
    "outputId": "2af53fe6-51c4-4848-d0eb-cfb3787fcd26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'particle'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can also use spacy.explain to get the description for the string representation of a label.\n",
    "spacy.explain(\"prt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ym5AZWhsFWrU",
    "outputId": "0b04962a-1a2c-4de2-98fc-13159079c5de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Any named language'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dependency\n",
    "spacy.explain(\"LANGUAGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A look-up table that allows you to access `Lexeme` objects. The `Vocab`\n",
      "    instance also provides access to the `StringStore`, and owns underlying\n",
      "    C-data that is shared between `Doc` objects.\n",
      "\n",
      "    DOCS: https://spacy.io/api/vocab\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from spacy.vocab import Vocab\n",
    "\n",
    "print(Vocab.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<spacy.vocab.Vocab at 0x7f04bffbc7a0>, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab(strings=['Hello', 'world'])\n",
    "vocab, vocab.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('eng_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.vectors_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.6391e-01,  4.3771e-01, -2.0447e-01, -2.2889e-01, -1.4227e-01,\n",
       "        2.7396e-01, -1.1435e-02, -1.8578e-01,  3.7361e-01,  7.5339e-01,\n",
       "       -3.0591e-01,  2.3741e-02, -7.7876e-01, -1.3802e-01,  6.6992e-02,\n",
       "       -6.4303e-02, -4.0024e-01,  1.5309e+00, -1.3897e-02, -1.5657e-01,\n",
       "        2.5366e-01,  2.1610e-01, -3.2720e-01,  3.4974e-01, -6.4845e-02,\n",
       "       -2.9501e-01, -6.3923e-01, -6.2017e-02,  2.4559e-01, -6.9334e-02,\n",
       "       -3.9967e-01,  3.0925e-02,  4.9033e-01,  6.7524e-01,  1.9481e-01,\n",
       "        5.1488e-01, -3.1149e-01, -7.9939e-02, -6.2096e-01, -5.3277e-03,\n",
       "       -1.1264e-01,  8.3528e-02, -7.6947e-03, -1.0788e-01,  1.6628e-01,\n",
       "        4.2273e-01, -1.9009e-01, -2.9035e-01,  4.5630e-02,  1.0120e-01,\n",
       "       -4.0855e-01, -3.5000e-01, -3.6175e-01, -4.1396e-01,  5.9485e-01,\n",
       "       -1.1524e+00,  3.2424e-02,  3.4364e-01, -1.9209e-01,  4.3255e-02,\n",
       "        4.9227e-02, -5.4258e-01,  9.1275e-01,  2.9576e-01,  2.3658e-02,\n",
       "       -6.8737e-01, -1.9503e-01, -1.1059e-01, -2.2567e-01,  2.4180e-01,\n",
       "       -3.1230e-01,  4.2700e-01,  8.3952e-02,  2.2703e-01,  3.0581e-01,\n",
       "       -1.7276e-01,  3.2536e-01,  5.4696e-03, -3.2745e-01,  1.9439e-01,\n",
       "        2.2616e-01,  7.4742e-02,  2.2033e-01, -4.0301e-01, -3.1594e-01,\n",
       "       -2.8910e-02,  9.7858e-01,  7.1860e-01,  1.4995e-01,  6.3421e-02,\n",
       "        2.8332e-01, -1.5231e-01,  3.9330e-04,  1.8076e-01, -4.0199e-01,\n",
       "        6.0187e-02, -2.7543e-02,  1.6590e-01, -2.5774e-01,  1.6150e-01,\n",
       "        3.7247e-01, -3.8273e-01,  2.4012e-01, -4.2617e-02, -6.6785e-01,\n",
       "       -9.4437e-01,  2.7916e-01,  1.0476e-01,  1.3952e+00, -1.4296e-01,\n",
       "       -5.5049e-01,  5.3982e-02, -7.7524e-01, -2.8255e-01, -2.3323e-02,\n",
       "        2.4801e-01,  2.2855e-01, -3.7408e-01,  7.6012e-02,  2.4031e-01,\n",
       "        1.0746e-01,  1.2411e-01, -2.0676e-01, -2.5804e-01, -1.6791e-01,\n",
       "        4.3499e-01,  6.1762e-01, -2.9955e-02,  1.6196e-01, -2.9001e-01,\n",
       "       -3.1159e-01, -8.7262e-01,  4.3167e-01, -1.5071e-01, -4.1420e-01,\n",
       "       -5.3730e-01, -1.9910e-01,  1.3270e-01, -1.5018e-01, -4.9335e-01,\n",
       "       -2.5127e+00,  3.1660e-01,  3.6396e-01, -5.9248e-02,  3.1120e-02,\n",
       "        4.1071e-02,  1.6917e-02,  5.8410e-01, -2.0201e-01,  7.0238e-02,\n",
       "        8.7547e-01, -2.0114e-01,  5.1920e-01,  2.6786e-01, -5.5643e-01,\n",
       "       -3.1247e-01, -3.7992e-01,  4.2857e-01,  4.1780e-01,  3.0608e-01,\n",
       "       -2.1657e-01,  7.2464e-01,  6.1734e-01,  5.8085e-02, -6.2708e-01,\n",
       "        5.2895e-02, -2.5628e-01, -3.2688e-01, -6.1280e-01,  6.2609e-01,\n",
       "       -1.7965e-01,  8.8925e-01,  2.1963e-01, -3.4052e-03, -7.8663e-02,\n",
       "        3.4799e-01, -2.6062e-01,  8.0410e-03,  1.1721e-01, -4.5147e-01,\n",
       "       -1.2178e-01, -5.7030e-01,  4.6602e-01,  2.5059e-02,  5.3986e-02,\n",
       "       -7.6693e-01,  1.3173e-01, -2.8776e-02, -4.1915e-01, -2.4415e-01,\n",
       "       -4.0295e-01, -4.1520e-01,  3.7643e-02, -1.4843e-01,  2.6094e-02,\n",
       "        1.5315e-01,  3.8310e-01, -5.5825e-01, -3.3433e-01, -2.7939e-02,\n",
       "       -4.3712e-01, -3.1802e-01, -3.1731e-01,  9.2891e-02, -9.9397e-02,\n",
       "       -1.8846e-01,  5.2270e-02,  2.9061e-01,  1.0639e+00,  9.9584e-02,\n",
       "       -5.6775e-01,  2.9446e-01,  3.7797e-01, -2.1905e-01, -5.2616e-01,\n",
       "       -4.1744e-01, -6.5951e-01, -4.0820e-01, -6.0945e-01,  1.1759e-02,\n",
       "       -2.9122e-01, -3.1457e-01,  5.7076e-02,  4.1503e-01,  3.7345e-01,\n",
       "       -4.7119e-02, -7.1996e-02,  1.4587e-01, -3.0763e-01,  1.0759e-01,\n",
       "       -5.9447e-01, -4.0205e-01,  3.0677e-01, -1.9891e-01, -7.0775e-01,\n",
       "       -1.1513e-01,  3.0866e-01, -6.9235e-01,  2.1219e-01,  1.0554e-01,\n",
       "        2.2617e-01, -2.6145e-01, -3.9298e-01, -2.3585e-01,  3.0795e-02,\n",
       "       -1.0193e-01,  3.2070e-01,  3.0505e-01, -5.3470e-01, -7.9272e-02,\n",
       "       -1.6817e-01, -2.2115e-01, -3.5143e-01, -9.2376e-02,  1.4686e-01,\n",
       "       -1.9859e-01,  2.0460e-01,  2.0276e-01,  3.6144e-01, -3.5867e-01,\n",
       "        4.0095e-01,  6.3686e-02, -1.2763e-01, -1.6226e-01, -3.1763e-01,\n",
       "       -5.8732e-01, -5.4009e-01, -4.9035e-01, -4.6035e-01, -1.9794e-01,\n",
       "       -2.5209e-01,  2.5706e-01,  4.0110e-01,  5.2830e-02, -3.2079e-01,\n",
       "        3.9563e-01, -4.4512e-01, -9.1862e-02, -1.9243e-01,  1.5397e-01,\n",
       "       -2.8923e-01,  6.0561e-01,  5.8133e-01,  3.2268e-01,  6.3892e-02,\n",
       "        8.5438e-02,  1.4956e-01,  3.8134e-01, -1.1820e-01, -2.3951e-01,\n",
       "       -6.7731e-01,  2.8090e-01, -5.1770e-01, -4.1098e-01, -4.1292e-01,\n",
       "       -6.7856e-02, -3.3721e-02, -7.2958e-01, -4.7891e-01,  7.2956e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.get_vector('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.to_disk('DELE_VOCAB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mDELE_VOCAB/\u001b[00m\n",
      "├── [9.0M]  key2row\n",
      "├── [123M]  lexemes.bin\n",
      "├── [1.6M]  lookups.bin\n",
      "├── [ 22M]  strings.json\n",
      "└── [784M]  vectors\n",
      "\n",
      "0 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "!tree DELE_VOCAB/ -sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dele_bytestrings = nlp.vocab.to_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dele_bytestrings.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "979847654"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dele_bytestrings.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'\\x84\\xa7strings\\xdb\\x01\\x169\\xc3[\"\\\\\"\\\\\"\",\"#\",\"$\",\"\\'\\'\",\",\",\"-LRB-\",\"-RRB-\",\".\",\":\",\"ADD\",\"AFX\",\"BES\",\"CC\",\"CD\",\"DT\",\"EX\"',\n",
       " b'elling\\x91\\xa5yodel\\xa6zapped\\x91\\xa3zap\\xa7zapping\\x91\\xa3zap\\xa9zigzagged\\x91\\xa6zigzag\\xaazigzagging\\x91\\xa6zigzag\\xa6zipped\\x91\\xa3zip\\xa7zipping\\x91\\xa3zip')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dele_bytestrings[:100], dele_bytestrings[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8566208034543834098"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lemma_lookup', 'lemma_rules', 'lemma_index', 'lemma_exc']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.lookups.tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StringStore 64-bit Hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Look up strings by 64-bit hashes.\\n\\n    DOCS: https://spacy.io/api/stringstore\\n    '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.strings import StringStore\n",
    "StringStore.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringstore = StringStore(['hellow' ,'world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6030250719154556199"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringstore['hellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringstore = StringStore([\"apple\", \"orange\"])\n",
    "banana_hash = stringstore.add(\"banana\")\n",
    "assert len(stringstore) == 3\n",
    "assert banana_hash == 2525716904149915114\n",
    "assert stringstore[banana_hash] == \"banana\"\n",
    "assert stringstore[\"banana\"] == banana_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.strings import hash_string\n",
    "assert hash_string(\"apple\") == 8566208034543834098"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.vectors import Vectors\n",
    "import numpy \n",
    "\n",
    "empty_vectors = Vectors(shape=(10000, 300))\n",
    "\n",
    "data = numpy.zeros((3, 300), dtype='f')\n",
    "keys = [\"cat\", \"dog\", \"rat\"]\n",
    "vectors = Vectors(data=data, keys=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_vectors.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Store, save and load word vectors.\\n\\n    Vectors data is kept in the vectors.data attribute, which should be an\\n    instance of numpy.ndarray (for CPU vectors) or cupy.ndarray\\n    (for GPU vectors). `vectors.key2row` is a dictionary mapping word hashes to\\n    rows in the vectors.data table.\\n\\n    Multiple keys can be mapped to the same vector, and not all of the rows in\\n    the table need to be assigned - so len(list(vectors.keys())) may be\\n    greater or smaller than vectors.shape[0].\\n\\n    DOCS: https://spacy.io/api/vectors\\n    '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 300)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-d9a5a9e0d6fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcat_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcat_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mcat_vector\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "cat_id = nlp.vocab.strings[\"cat\"]\n",
    "cat_vector = nlp.vocab.vectors[cat_id]\n",
    "assert cat_vector == nlp.vocab[\"cat\"].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3424551750583975941 croup\n"
     ]
    }
   ],
   "source": [
    "for key in nlp.vocab.vectors.keys():\n",
    "    print(key, nlp.vocab.strings[key])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookups and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A `KnowledgeBase` instance stores unique identifiers for entities and their textual aliases,\n",
      "    to support entity linking of named entities to real-world concepts.\n",
      "\n",
      "    DOCS: https://spacy.io/api/kb\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from spacy.kb import KnowledgeBase\n",
    "print(KnowledgeBase.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        Add an entity to the KB, optionally specifying its log probability based on corpus frequency\\n        Return the hash of the entity ID/name at the end.\\n        '"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.add_entity.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "kb.add_entity(entity=\"Q42\", freq=32, entity_vector=vector1)\n",
    "kb.add_entity(entity=\"Q463035\", freq=111, entity_vector=vector2)\n",
    "kb.set_entities(entity_list=[\"Q42\", \"Q463035\"], freq_list=[32, 111], vector_list=[vector1, vector2])\n",
    "kb.add_alias(alias=\"Douglas\", entities=[\"Q42\", \"Q463035\"], probabilities=[0.6, 0.3])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoldParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## nlp.entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beam_width': 1,\n",
       " 'beam_density': 0.0,\n",
       " 'beam_update_prob': 1.0,\n",
       " 'cnn_maxout_pieces': 3,\n",
       " 'deprecation_fixes': {'vectors_name': 'en_core_web_lg.vectors'},\n",
       " 'nr_class': 74,\n",
       " 'hidden_depth': 1,\n",
       " 'token_vector_width': 96,\n",
       " 'hidden_width': 64,\n",
       " 'maxout_pieces': 2,\n",
       " 'pretrained_vectors': 'en_core_web_lg.vectors',\n",
       " 'bilstm_depth': 0}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.entity.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<thinc.neural.optimizers.Optimizer at 0x7f04c00669d0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJb4NWspFWrZ",
    "toc-hr-collapsed": true
   },
   "source": [
    "## CLI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aoDZpBIJFWra"
   },
   "source": [
    "### Download\n",
    "\n",
    "- **Model** \n",
    "    - installed as Python Packages like any module\n",
    "    - Chinese (None model yet) (dependencies = Jieba)\n",
    "    - en_core_web_sm/md/lg\n",
    "    - en_vectors_web_lg (631MB, 300-Dim 1070971 unique vectors)\n",
    "\n",
    "> **It’s not recommended to use this command as part of an automated process. If you know which model your project needs, you should consider a direct download via pip, or uploading the model to a local PyPi installation and fetching it straight from there. This will also allow you to add it as a versioned package dependency to your project.**\n",
    "\n",
    "```bash\n",
    "python -m spacy download [model] [--direct]\n",
    "```\n",
    "> **As of v2.0, spaCy expects all shortcut links to be loadable model packages. If you want to load a data directory, call spacy.load() or Language.from_disk() with the path, or use the package command to create a model package.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ezu-qsjk9Uif"
   },
   "source": [
    "\n",
    "### Info & Validate\n",
    "\n",
    "```bash\n",
    "python -m spacy info [model] [--markdown]\n",
    "```\n",
    "\n",
    "- find all models installed (packages and symlinks) and check compatibility with spaCy\n",
    "- run after `pip install -U spacy` to ensure\n",
    "- useful to detect off-sync links\n",
    "- use in production build process (1 for error)\n",
    "\n",
    "```bash\n",
    "python -m spacy validate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyDQeDn59bp6"
   },
   "source": [
    "### Convert\n",
    "\n",
    "- convert files into spaCy's JSON for `train`\n",
    "\n",
    "```bash\n",
    "python -m spacy convert [input_file] [output_dir] [--file-type] [--converter]\n",
    "[--n-sents] [--morphology] [--lang]\n",
    "```\n",
    "\n",
    "- default `jsonl` format\n",
    "- options\n",
    "    - `auto`: auto pick converter based on file ext\n",
    "    - `conll, conllu, conllubio`: Unniversal Dependences\n",
    "    - `ner`: tab-based NER\n",
    "    - `iob`: IOB or IOB2 NER\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YfXsfdGW9eaF"
   },
   "source": [
    "### Train\n",
    "\n",
    "- Input as JSON\n",
    "- each epoch, a model will be saved to DIR\n",
    "- **accuracy and details added to `meta.json` to allow packaging model using `package` CLI**\n",
    "- `--pipeline tagger, parser` will only train tagger and parse\n",
    "\n",
    "```bash\n",
    "python -m spacy train [lang] [output_path] [train_path] [dev_path]\n",
    "[--base-model] [--pipeline] [--vectors] [--n-iter] [--n-examples] [--use-gpu]\n",
    "[--version] [--meta-path] [--init-tok2vec] [--parser-multitasks]\n",
    "[--entity-multitasks] [--gold-preproc] [--noise-level] [--learn-tokens]\n",
    "[--verbose]\n",
    "```\n",
    "\n",
    "**DETAIL OPTIONS**\n",
    "\n",
    "`output_path`\tpositional\n",
    "- Directory to store model in. Will be created if it doesn’t exist.\n",
    "\n",
    "`train_path`\tpositional\n",
    "- Location of JSON-formatted training data. Can be a file or a directory of files.\n",
    "\n",
    "`dev_path`\tpositional\n",
    "- Location of JSON-formatted development data for evaluation. Can be a file or a directory of files.\n",
    "\n",
    "`--base-model, -b`\toption\t\n",
    "- Optional name of base model to update. Can be any loadable spaCy model.\n",
    "\n",
    "`--pipeline, -p`\toption\t\n",
    "- Comma-separated names of pipeline components to train. Defaults to 'tagger,parser,ner'.\n",
    "\n",
    "`--vectors, -v`\toption\t\n",
    "- Model to load vectors from.\n",
    "\n",
    "`--n-iter, -n`\toption\t\n",
    "- Number of iterations (default: 30).\n",
    "\n",
    "`--n-examples, -ns`\toption\t\n",
    "- Number of examples to use (defaults to 0 for all examples).\n",
    "\n",
    "`--use-gpu, -g`\toption\t\n",
    "- Whether to use GPU. Can be either 0, 1 or -1.\n",
    "\n",
    "`--version, -V`\toption\t\n",
    "- Model version. Will be written out to the model’s meta.json after training.\n",
    "\n",
    "`--meta-path, -m`\toption\t\n",
    "- Optional path to model meta.json. All relevant properties like lang, pipeline and spacy_version will be overwritten.\n",
    "\n",
    "`--init-tok2vec, -t2v` option\t\n",
    "- Path to pretrained weights for the token-to-vector parts of the models. See spacy pretrain. Experimental.\n",
    "\n",
    "`--parser-multitasks, -pt`\toption\t\n",
    "- Side objectives for parser CNN, e.g. 'dep' or 'dep,tag\n",
    "`--entity-multitasks, -et`\toption\t\n",
    "- Side objectives for NER CNN, e.g. 'dep' or 'dep,tag'\n",
    "\n",
    "`--noise-level, -nl`\toption\t\n",
    "- Float indicating the amount of corruption for data augmentation.\n",
    "\n",
    "`--gold-preproc, -G`\tflag\t\n",
    "- Use gold preprocessing.\n",
    "\n",
    "`--learn-tokens, -T`\tflag\n",
    "- Make parser learn gold-standard tokenization by merging ] subtokens. Typically used for languages like Chinese.\n",
    "\n",
    "`--verbose, -VV` flag\t\n",
    "- Show more detailed messages during training.\n",
    "\n",
    "`--help, -h`\tflag\t\n",
    "- Show help message and available arguments.\n",
    "\n",
    "**CUSTOM ENV VARIABLE**\n",
    "\n",
    "```bash\n",
    "token_vector_width=256 learn_rate=0.0001 spacy train [...]\n",
    "\n",
    "alias train-parser=\"python -m spacy train en /output /data /train /dev -n 1000\"\n",
    "token_vector_width=256 train-parser\n",
    "```\n",
    "\n",
    "`dropout_from`\n",
    "- Initial dropout rate.\t0.2\n",
    "\n",
    "`dropout_to`\n",
    "- Final dropout rate.\t0.2\n",
    "\n",
    "`dropout_decay`\n",
    "- Rate of dropout change.\t0.0\n",
    "\n",
    "`batch_from`\n",
    "- Initial batch size.\t1\n",
    "\n",
    "`batch_to`\n",
    "- Final batch size.\t64\n",
    "\n",
    "`batch_compound`\n",
    "- Rate of batch size acceleration.\t1.001\n",
    "\n",
    "`token_vector_width`\n",
    "- Width of embedding tables and convolutional layers.\t128\n",
    "\n",
    "`embed_size`\n",
    "- Number of rows in embedding tables.\t7500\n",
    "\n",
    "`hidden_width`\n",
    "- Size of the parser’s and NER’s hidden layers.\t128\n",
    "\n",
    "`learn_rate`\n",
    "- Learning rate.\t0.001\n",
    "\n",
    "`optimizer_B1`\n",
    "- Momentum for the Adam solver.\t0.9\n",
    "\n",
    "`optimizer_B2`\n",
    "- Adagrad-momentum for the Adam solver.\t0.999\n",
    "\n",
    "`optimizer_eps`\n",
    "- Epsilon value for the Adam solver.\t1e-08\n",
    "\n",
    "`L2_penalty`\n",
    "- L2 regularization penalty.\t1e-06\n",
    "\n",
    "`grad_norm_clip`\t\n",
    "- Gradient L2 norm constraint.\t1.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SqLmxDuv9jqd"
   },
   "source": [
    "### Pretrain\n",
    "\n",
    "- pretrain on `tok2vec` layer of pipeline component\n",
    "- using LMAO\n",
    "    - load pre-trained vector\n",
    "    - train a component like CNN, BiLSTM, etc\n",
    "    - predict vectors matching pre-trained ones\n",
    "    - weights saved per epoch\n",
    "    - pass a path to one of these weights files to `spacy train`\n",
    "- esp helful in little labelled data\n",
    "- experimental now, result varies\n",
    "- piping to train must ensure all settings identical\n",
    "\n",
    "```bash\n",
    "python -m spacy pretrain [texts_loc] [vectors_model] [output_dir] [--width]\n",
    "[--depth] [--embed-rows] [--dropout] [--seed] [--n-iter] [--use-vectors]\n",
    "```\n",
    "\n",
    "`texts_loc`\tpositional\n",
    "- Path to JSONL file with raw texts to learn from, with text provided as the key \"text\". See here for details.\n",
    "\n",
    "`vectors_model`\tpositional\n",
    "- Name or path to spaCy model with vectors to learn from.\n",
    "\n",
    "`output_dir`\tpositional\n",
    "- Directory to write models to on each epoch.\n",
    "\n",
    "`--width, -cw`\toption\n",
    "- Width of CNN layers.\n",
    "\n",
    "`--depth, -cd`\toption\n",
    "- Depth of CNN layers.\n",
    "\n",
    "`--embed-rows, -er`\toption\n",
    "- Number of embedding rows.\n",
    "\n",
    "`--dropout, -d`\toption\n",
    "- Dropout rate.\n",
    "\n",
    "`--batch-size, -bs`\toption\n",
    "- Number of words per training batch.\n",
    "\n",
    "`--max-length, -xw`\toption\n",
    "- Maximum words per example. Longer examples are discarded.\n",
    "\n",
    "`--min-length, -nw`\toption\n",
    "- Minimum words per example. Shorter examples are discarded.\n",
    "\n",
    "`--seed, -s`\toption\n",
    "- Seed for random number generators.\n",
    "\n",
    "`--n-iter, -i`\toption\n",
    "- Number of iterations to pretrain.\n",
    "\n",
    "`--use-vectors, -uv`\tflag\n",
    "- Whether to use the static vectors as input features.\n",
    "\n",
    "**JSONL format raw text**\n",
    "\n",
    "   > raw text can be provided as a .jsonl (newline-delimited JSON) file containing one input text per line (roughly paragraph length is good). Optionally, custom tokenization can be provided.<br>\n",
    "    > Our utility library `srsly` provides a handy `write_jsonl` helper that takes a file path and list of dictionaries and writes out JSONL-formatted data.\n",
    "\n",
    "```python\n",
    "import srsly\n",
    "data = [{\"text\": \"Some text\"}, {\"text\": \"More...\"}]\n",
    "srsly.write_jsonl(\"/path/to/text.jsonl\", data)\n",
    "``` \n",
    "<br>\n",
    "\n",
    "- Example\n",
    "\n",
    "```python\n",
    "{\"text\": \"Can I ask where you work now and what you do, and if you enjoy it?\"}\n",
    "{\"text\": \"They may just pull out of the Seattle market completely, at least until they have autonomous vehicles.\"}\n",
    "{\"text\": \"My cynical view on this is that it will never be free to the public. Reason: what would be the draw of joining the military? Right now their selling point is free Healthcare and Education. Ironically both are run horribly and most, that I've talked to, come out wishing they never went in.\"}\n",
    "```\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igcv_Mca9mZC"
   },
   "source": [
    "### INIT-Model\n",
    "\n",
    "- **Converting word vectors for use in spaCy**\n",
    "- create new model DIR from raw data like word freq, Brown clusteres and word vectors \n",
    "- similar to `spacy model`\n",
    "- output = model containing vocab and vectors\n",
    "\n",
    "> As of v2.1.0, the --freqs-loc and --clusters-loc are deprecated and have been replaced with the --jsonl-loc argument, which lets you pass in a a newline-delimited JSON (JSONL) file containing one lexical entry per line. For more details on the format, see the annotation specs.\n",
    "\n",
    "**EXAMPLE Using other Vectors**\n",
    "\n",
    "```shell\n",
    "wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.la.300.vec.gz\n",
    "python -m spacy init-model en /tmp/la_vectors_wiki_lg --vectors-loc cc.la.300.vec.gz\n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m spacy init-model [lang] [output_dir] [--jsonl-loc] [--vectors-loc]\n",
    "[--prune-vectors]\n",
    "```\n",
    "\n",
    "```python\n",
    "nlp_latin = spacy.load(\"/tmp/la_vectors_wiki_lg\")\n",
    "doc1 = nlp_latin(u\"Caecilius est in horto\")\n",
    "doc2 = nlp_latin(u\"servus est in atrio\")\n",
    "doc1.similarity(doc2)\n",
    "```\n",
    "\n",
    "`lang`\n",
    "- positional\tModel language ISO code, e.g. en.\n",
    "\n",
    "`output_dir`\n",
    "- positional\tModel output directory. Will be created if it doesn’t exist.\n",
    "\n",
    "`--jsonl-loc, -j`\n",
    "- option\tOptional location of JSONL-formatted vocabulary file with lexical attributes.\n",
    "\n",
    "`--vectors-loc, -v`\n",
    "- option\tOptional location of vectors file. Should be a tab-separated file in Word2Vec format where the first column contains the word and the remaining columns the values. File can be provided in .txt format or as a zipped text file in .zip or .tar.gz format.\n",
    "\n",
    "`--prune-vectors, -V`\n",
    "- flag\tNumber of vectors to prune the vocabulary to. Defaults to -1 for no pruning.\n",
    "\n",
    "**Optimizing vector coverage **\n",
    "\n",
    "- To help you strike a good balance between coverage and memory usage, spaCy’s Vectors class lets you map multiple keys to the same row of the table. If you’re using the spacy init-model command to create a vocabulary, pruning the vectors will be taken care of automatically if you set the --prune-vectors flag. You can also do it manually in the following steps:\n",
    "  1. Start with a word vectors model that covers a huge vocabulary. For instance, the en_vectors_web_lg model provides 300-dimensional GloVe vectors for over 1 million terms of English.\n",
    "  2. If your vocabulary has values set for the Lexeme.prob attribute, the lexemes will be sorted by descending probability to determine which vectors to prune. Otherwise, lexemes will be sorted by their order in the Vocab.\n",
    "  3. Call Vocab.prune_vectors with the number of vectors you want to keep.\n",
    "  \n",
    "  ```python\n",
    "  nlp = spacy.load('en_vectors_web_lg')\n",
    "  n_vectors = 105000  # number of vectors to keep\n",
    "  removed_words = nlp.vocab.prune_vectors(n_vectors)\n",
    "\n",
    "  assert len(nlp.vocab.vectors) <= n_vectors  # unique vectors have been pruned\n",
    "  assert nlp.vocab.vectors.n_keys > n_vectors  # but not the total entries\n",
    "  ```\n",
    "  > **Vocab.prune_vectors reduces the current vector table to a given number of unique entries, and returns a dictionary containing the removed words, mapped to (string, score) tuples, where string is the entry the removed word was mapped to, and score the similarity score between the two words.**\n",
    "  \n",
    "  - In the example above, the vector for “Shore” was removed and remapped to the vector of “coast”, which is deemed about 73% similar. “Leaving” was remapped to the vector of “leaving”, which is identical.\n",
    "  - If you’re using the init-model command, you can set the --prune-vectors option to easily reduce the size of the vectors as you add them to a spaCy model:\n",
    "  \n",
    "  ```shell\n",
    "  python -m spacy init-model /tmp/la_vectors_web_md --vectors-loc la.300d.vec.tgz --prune-vectors 10000\n",
    "  ```\n",
    "  \n",
    "  - This will create a spaCy model with vectors for the first 10,000 words in the vectors model. All other words in the vectors model are mapped to the closest vector among those retained.\n",
    "  \n",
    "**Loading GloVe vectors**\n",
    "\n",
    "- spaCy comes with built-in support for loading GloVe vectors from a directory. The Vectors.from_glove method assumes a binary format, the vocab provided in a vocab.txt, and the naming scheme of vectors.{size}.[fd.bin]. For example:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.vocab.vectors.from_glove(\"/path/to/vectors\")\n",
    "```\n",
    "\n",
    "- If your instance of Language already contains vectors, they will be overwritten. To create your own GloVe vectors model package like spaCy’s en_vectors_web_lg, you can call nlp.to_disk, and then package the model using the package command.\n",
    "\n",
    "**Storing Vectors on GPU (Chain or PyTorch) https://spacy.io/usage/vectors-similarity#gpu**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzFiF3oP9o_5"
   },
   "source": [
    "### Evaluate\n",
    "\n",
    "- accuracy and speed on JSON-annotated data\n",
    "- print results with displaCy\n",
    "\n",
    "```bash\n",
    "python -m spacy evaluate [model] [data_path] [--displacy-path] [--displacy-limit]\n",
    "[--gpu-id] [--gold-preproc]\n",
    "```\n",
    "\n",
    "`model`\n",
    "- positional\tModel to evaluate. Can be a package or shortcut link name, or a path to a model data directory.\n",
    "\n",
    "`data_path`\n",
    "- positional\tLocation of JSON-formatted evaluation data.\n",
    "\n",
    "`--displacy-path, -dp`\n",
    "- option\tDirectory to output rendered parses as HTML. If not set, no visualizations will be generated.\n",
    "\n",
    "`--displacy-limit, -dl`\n",
    "- option\tNumber of parses to generate per file. Defaults to 25. Keep in mind that a significantly higher number might cause the .html files to render slowly.\n",
    "\n",
    "`--gpu-id, -g`\n",
    "- option\tGPU to use, if any. Defaults to -1 for CPU.\n",
    "\n",
    "`--gold-preproc, -G`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAL3Ark_9qiD"
   },
   "source": [
    "### Package\n",
    "\n",
    "- generate model package from existing data DIR\n",
    "- if path to `meta.json`, used\n",
    "- else data entered from CLI\n",
    "- `python setup.py sdist` from newly created DIR to turn model into installable archive file\n",
    "\n",
    "```bash\n",
    "python -m spacy package [input_dir] [output_dir] [--meta-path] [--create-meta] [--force]\n",
    "```\n",
    "\n",
    "**Example**\n",
    "\n",
    "```bash\n",
    "python -m spacy package /input /output\n",
    "cd /output/en_model-0.0.0\n",
    "python setup.py sdist\n",
    "pip install dist/en_model-0.0.0.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5_ROUqgFWra"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ra6mvnwbcxlp"
   },
   "source": [
    "# Custom Pipelines and Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YC7wg1ZNAO6J"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLwAmN64AX8E"
   },
   "outputs": [],
   "source": [
    "Doc.set_extension('is_greeting', default=False)\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'hello world')\n",
    "doc._.doc_extensions\n",
    "\n",
    "# ._ create extensibility and distinction to built-ins, code-break resilient upon update\n",
    "doc._.is_greeting = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0CP__iumA_8h"
   },
   "outputs": [],
   "source": [
    "# Customise Processing Pipeline (same nlp() as above)\n",
    "\n",
    "component = MyComponent() # See below for INIT\n",
    "\n",
    "nlp.add_pipe(component, after='tagger')\n",
    "\n",
    "doc = nlp(u'This is a sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UECqAGG5CEDJ"
   },
   "source": [
    "**The nlp object is an instance of Language, which contains the data and annotation scheme of the language you're using and a pre-defined pipeline of components, like the tagger, parser and entity recognizer. If you're loading a model, the Language instance also has access to the model's binary data. All of this is specific to each model, and defined in the model's meta.json – for example, a Spanish NER model requires different weights, language data and pipeline components than an English parsing and tagging model. This is also why the pipeline state is always held by the Language class. spacy.load() puts this all together and returns an instance of Language with a pipeline set and access to the binary data.**\n",
    "\n",
    "```python\n",
    "doc = nlp.make_doc(u'This is a sentence')   # create a Doc from raw text\n",
    "for name, proc in nlp.pipeline:             # iterate over components in order\n",
    "    doc = proc(doc)                         # call each component on the Doc\n",
    "```\n",
    "\n",
    "**spaCy 2.0 simply list of (name, function) tuple**\n",
    "\n",
    "```python\n",
    "nlp.pipeline\n",
    "[('tagger', <spacy.pipeline.Tagger>), ('parser', <spacy.pipeline.DependencyParser>),\n",
    " ('ner', <spacy.pipeline.EntityRecognizer>)]\n",
    "```\n",
    "\n",
    "To make it more convenient to modify the pipeline, there are several built-in methods to get, add, replace, rename or remove individual components. spaCy's default pipeline components, like the tagger, parser and entity recognizer now all follow the same, consistent API and are subclasses of `Pipe`. If you're developing your own component, using the Pipe API will make it fully trainable and serializable. At a minimum, a component needs to be a callable that takes a Doc and returns it:\n",
    "\n",
    "```python\n",
    "def my_component(doc):\n",
    "    print(\"The doc is {} characters long and has {} tokens.\"\n",
    "          .format(len(doc.text), len(doc))\n",
    "    return doc\n",
    "```\n",
    "\n",
    "The component can then be added at any position of the pipeline using the `nlp.add_pipe()` method. The arguments `before, after, first, and last` let you specify component names to insert the new component before or after, or tell spaCy to insert it first (i.e. directly after tokenization) or last in the pipeline.\n",
    "\n",
    "```python\n",
    "nlp = spacy.load('en')\n",
    "nlp.add_pipe(my_component, name='print_length', last=True)\n",
    "doc = nlp(u\"This is a sentence.\")\n",
    "```\n",
    "\n",
    "**Extension attributes on Doc, Token and Span**\n",
    "\n",
    "When you implement your own pipeline components that modify the `Doc`, you often want to extend the API, so that the information you're adding is conveniently accessible. spaCy v2.0 introduces a new mechanism that lets you register your own attributes, properties and methods that become available in the `._` namespace, for example, `doc._.my_attr`. There are mostly three types of extensions that can be registered via the `set_extension()`` method:\n",
    "**Why ._?**\n",
    "Writing to a ._ attribute instead of to the Doc directly keeps a clearer separation and makes it easier to ensure backwards compatibility. For example, if you've implemented your own .coref property and spaCy claims it one day, it'll break your code. Similarly, just by looking at the code, you'll immediately know what's built-in and what's custom – for example, doc.sentiment is spaCy, while doc._.sent_score isn't.\n",
    "\n",
    "1. Attribute extensions. Set a default value for an attribute, which can be overwritten.\n",
    "2. Property extensions. Define a `getter` and an optional `setter` function.\n",
    "3. Method extensions. Assign a function that becomes available as an object method.\n",
    "\n",
    "```python\n",
    "Doc.set_extension('hello_attr', default=True)\n",
    "Doc.set_extension('hello_property', getter=get_value, setter=set_value)\n",
    "Doc.set_extension('hello_method', method=lambda doc, name: 'Hi {}!'.format(name))\n",
    "\n",
    "doc._.hello_attr            # True\n",
    "doc._.hello_property        # return value of get_value\n",
    "doc._.hello_method('Ines')  # 'Hi Ines!'\n",
    "```\n",
    "\n",
    "**WHY Extensions?**\n",
    "\n",
    "Being able to easily write custom data to the `Doc, Token and Span` means that applications using spaCy can take full advantage of the built-in data structures and the benefits of Doc objects as the **single source of truth** containing all information:\n",
    "\n",
    "- No information is lost during tokenization and parsing, so you can always relate annotations to the original string.\n",
    "- The Token and Span are views of the Doc, so they're always up-to-date and consistent.\n",
    "- Efficient C-level access is available to the underlying TokenC* array via doc.c.\n",
    "- APIs can standardise on passing around Doc objects, reading and writing from them whenever necessary. Fewer signatures makes functions more reusable and composable.\n",
    "\n",
    "**TODO - learn these examples of custom componets**\n",
    "\n",
    "- https://explosion.ai/blog/spacy-v2-pipelines-extensions\n",
    "- https://github.com/explosion/spaCy/blob/develop/examples/pipeline/custom_component_countries_api.py\n",
    "- https://github.com/explosion/spaCy/blob/develop/examples/pipeline/custom_component_entities.py\n",
    "- https://github.com/explosion/spaCy/blob/develop/examples/pipeline/custom_attr_methods.py\n",
    "- https://github.com/explosion/spaCy/blob/develop/examples/pipeline/custom_sentence_segmentation.py\n",
    "- https://github.com/explosion/spaCy/blob/develop/examples/pipeline/fix_space_entities.py\n",
    "- https://github.com/explosion/spaCy/blob/develop/examples/pipeline/multi_processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5wzLaofFWrf",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Serialisation & Packaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Model\n",
    "\n",
    "- Custom `tokenizer` self-serialised as JSON\n",
    "- **custom componenet** not - best way to wrap as Python package\n",
    "- `spacy package` in model dir saved -> create all files needed to package (`__init__.py` consisting of `load()` for calling `spacy.load` equal:\n",
    "    ```python\n",
    "    import en_core_web_sm\n",
    "    nlp = en_core_web_sm.load()\n",
    "    ```\n",
    "- **QUICK-DIRTY** way to add all custom code to `__init__.py` and add `CustomEntityRecognizer` to global factories:\n",
    "    ```python\n",
    "    from spacy.language import Language\n",
    "    # add custom NER to global factories\n",
    "    Language.factories['CustomEntityRecognizer'] = CustomEntityRecognizer\n",
    "    ```\n",
    "- **Everything** needs to be available from within the package; also specify additional dependencies in `setup.py`, adding files for modules etc\n",
    "- Once done, `python setup.py sdist` to build package adding `.tar.gz` to `/dist` for `pip install` usage within dir later\n",
    "\n",
    "#### More Elegantly\n",
    "\n",
    "**ADD/SERIALISATION**\n",
    "- by add `to_disk, from_disk, to_bytes, from_bytes` methods\n",
    "- `nlp.from_disk` iterates over pipeline and checks for methods:\n",
    "    ```python\n",
    "    from pipe_name, proc in nlp.pipeline:\n",
    "        if hasattr(proc, 'from_disk'):\n",
    "            proc.from_disk(model_path/pipe_name)\n",
    "    ```\n",
    "- under `/path/to/model/custom_ner/`:\n",
    "\n",
    "    ```python\n",
    "    class CustomEntityRecognizer:\n",
    "        name = 'custom_ner'\n",
    "        def __init__(self, nlp):\n",
    "            self.vocab = nlp.vocab\n",
    "            self.some_data = None\n",
    "        def __call__(self, spacy_doc, *args, **kwargs):\n",
    "            return predcit_single(spacy_doc)\n",
    "        def from_disk(self, path, *kwargs):\n",
    "            # do sth here and load all data needed\n",
    "            data_path = path/'some_data.json'\n",
    "            with data_path.open() as f:\n",
    "                self.some_data = json.load(f)\n",
    "    ```\n",
    "\n",
    "**TL;DR**\n",
    "\n",
    "1. Save out model with custom tokenizer only\n",
    "2. `spacy package` in saved dir, edit meta etc\n",
    "3. Edit `__init__.py` include custom component, custom NER model etc and add entry to global factories\n",
    "4. `python setup.py sdist` within package dir to build package\n",
    "5. Install `.tar.gz` model created in `/dist` \n",
    "\n",
    "\n",
    "### Factories via ENTRY POINT\n",
    "\n",
    "```python\n",
    "# SERIALIZE\n",
    "\n",
    "bytes_data = nlp.to_bytes()\n",
    "lang = nlp.meta[\"lang\"]  # \"en\"\n",
    "pipeline = nlp.meta[\"pipeline\"]  # [\"tagger\", \"parser\", \"ner\"]\n",
    "\n",
    "# DESERIALIZE\n",
    "\n",
    "nlp = spacy.blank(lang)\n",
    "for pipe_name in pipeline:\n",
    "    pipe = nlp.create_pipe(pipe_name)\n",
    "    nlp.add_pipe(pipe)\n",
    "nlp.from_bytes(bytes_data)\n",
    "```\n",
    "\n",
    "```python\n",
    "# SPACY.LOAD UNDER THE HOOD\n",
    "\n",
    "lang = \"en\"\n",
    "pipeline = [\"tagger\", \"parser\", \"ner\"]\n",
    "data_path = \"path/to/en_core_web_sm/en_core_web_sm-2.0.0\"\n",
    "\n",
    "cls = spacy.util.get_lang_class(lang)   # 1. Get Language instance, e.g. English()\n",
    "nlp = cls()                             # 2. Initialize it\n",
    "for name in pipeline:\n",
    "    component = nlp.create_pipe(name)   # 3. Create the pipeline components\n",
    "    nlp.add_pipe(component)             # 4. Add the component to the pipeline\n",
    "nlp.from_disk(model_data_path)          # 5. Load in the binary data\n",
    "\n",
    "\n",
    "#THE PIPELINE UNDER THE HOOD\n",
    "\n",
    "doc = nlp.make_doc(u\"This is a sentence\")   # create a Doc from raw text\n",
    "for name, proc in nlp.pipeline:             # iterate over components in order\n",
    "    doc = proc(doc)                         # apply each component\n",
    "    \n",
    "```\n",
    "\n",
    "**Using Pickle**\n",
    "\n",
    "- pickling `Doc` or `EntityRecognizer` beware of all requiring common `vocab` (including string2has mappings, label schemes and optional vectors) - CANNOT be too large\n",
    "\n",
    "```python\n",
    "# PICKLING OBJECTS WITH SHARED DATA\n",
    "\n",
    "doc1 = nlp(u\"Hello world\")\n",
    "doc2 = nlp(u\"This is a test\")\n",
    "\n",
    "doc1_data = pickle.dumps(doc1)\n",
    "doc2_data = pickle.dumps(doc2)\n",
    "print(len(doc1_data) + len(doc2_data))  # 6636116 😞\n",
    "\n",
    "doc_data = pickle.dumps([doc1, doc2])print(len(doc_data))  # 3319761 😃\n",
    "```\n",
    "\n",
    "### Example\n",
    "- `EntityRuler` component, patterns saved as `.jsonl` if pipeline to_disk, and to a bytestring if pipeline to_bytes - allowing saving out model with rule-based ENR and incluidng all rules WITH the model data\n",
    "\n",
    "```python\n",
    "class CustomComponent(object):\n",
    "    name = \"my_component\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # Do something to the doc here\n",
    "        return doc\n",
    "\n",
    "    def add(self, data):\n",
    "        # Add something to the component's data\n",
    "        self.data.append(data)\n",
    "\n",
    "    def to_disk(self, path):\n",
    "        # This will receive the directory path + /my_component\n",
    "        data_path = path / \"data.json\"\n",
    "        with data_path.open(\"w\", encoding=\"utf8\") as f:\n",
    "            f.write(json.dumps(self.data))\n",
    "\n",
    "    def from_disk(self, path, **cfg):\n",
    "        # This will receive the directory path + /my_component\n",
    "        data_path = path / \"data.json\"\n",
    "        with data_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "            self.data = json.loads(f)\n",
    "        return self\n",
    "```\n",
    "\n",
    "- After adding Component to pipeline and adding some data to it, cna seriliase `nlp` object to dir\n",
    "- will call custom component's `to_disk` method\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "my_component = CustomComponent()\n",
    "my_component.add({\"hello\": \"world\"})\n",
    "nlp.add_pipe(my_component)\n",
    "nlp.to_disk(\"/path/to/model\")\n",
    "```\n",
    "\n",
    "```bash\n",
    "DIRECTORY STRUCTURE\n",
    "└── /path/to/model\n",
    "    ├── my_component     # data serialized by \"my_component\"\n",
    "    |   └── data.json\n",
    "    ├── ner              # data for \"ner\" component\n",
    "    ├── parser           # data for \"parser\" component\n",
    "    ├── tagger           # data for \"tagger\" component\n",
    "    ├── vocab            # model vocabulary\n",
    "    ├── meta.json        # model meta.json with name, language and pipeline\n",
    "    └── tokenizer        # tokenization rules\n",
    "```\n",
    "\n",
    "**NOTE on loading components**\n",
    "- `meta.json` check to look up compoentn name in internal factories\n",
    "- ensure spacy to INIT `my_component` :\n",
    "\n",
    "```python\n",
    "from spacy.language import Language\n",
    "Language.factories[\"my_component\"] = lambda nlp, **cfg: CustomComponent()\n",
    "```\n",
    "\n",
    "#### ENTRY POINT\n",
    "\n",
    "- specificall, `nlp.create_pipe` and look up in **factories**\n",
    "- Must write to `Language.factories` **BEFORE** loading model\n",
    "\n",
    "```python\n",
    "pipe = nlp.create_pipe(\"custom_component\")  # fails 👎\n",
    "\n",
    "Language.factories[\"custom_component\"] = CustomComponentFactory\n",
    "pipe = nlp.create_pipe(\"custom_component\")  # works 👍\n",
    "```\n",
    "\n",
    "- this is messy and often requires INIT code shipped with model\n",
    "- Using **ENTRY POINT** model pkg and ext pkg can define own `spacy_factories` to add and INIT\n",
    "- automated in package in same ENV exposes spacy entry points - **SNEK example**\n",
    "\n",
    "```bash\n",
    "PACKAGE DIRECTORY STRUCTURE\n",
    "├── snek.py   # the extension code\n",
    "└── setup.py  # setup file for pip installation\n",
    "```\n",
    "\n",
    "```python\n",
    "# SNEK.PY\n",
    "snek = \"\"\"\n",
    "    --..,_                     _,.--.\n",
    "       `'.'.                .'`__ o  `;__.\n",
    "          '.'.            .'.'`  '---'`  `\n",
    "            '.`'--....--'`.'\n",
    "              `'--....--'`\n",
    "\"\"\"\n",
    "\n",
    "class SnekFactory(object):\n",
    "    def __init__(self, nlp, **cfg):\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        print(snek)\n",
    "        return doc\n",
    "```\n",
    "\n",
    "- adding entry to factories need exposing it in `setup.py` via `entry_point`:\n",
    "\n",
    "```python\n",
    "# SETUP.PY\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name=\"snek\",\n",
    "    entry_points={\n",
    "        \"spacy_factories\": [\n",
    "            \"snek = snek:SnekFactory\"\n",
    "         ]\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "- Entry Point lets spacy name `snek` found in module `snek` (i.e. `snek.py`) as `SnekFactory`\n",
    "- same package can expose multiple EP \n",
    "- to make them available to spaCy, install via `python setup.py develop`\n",
    "- now from spacy::\n",
    "\n",
    "```python\n",
    ">>> from spacy.lang.en import English\n",
    ">>> nlp = English()\n",
    ">>> snek = nlp.create_pipe(\"snek\")  # this now works! 🐍🎉\n",
    ">>> nlp.add_pipe(snek)\n",
    ">>> doc = nlp(u\"I am snek\")\n",
    "    --..,_                     _,.--.\n",
    "       `'.'.                .'`__ o  `;__.\n",
    "          '.'.            .'.'`  '---'`  `\n",
    "            '.`'--....--'`.'\n",
    "              `'--....--'`\n",
    "```\n",
    "\n",
    "**ADVANCED COMPONENTS WITH SETTINGS `**cfg`**\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_snek_sm\", snek_style=\"cute\")\n",
    "\n",
    "# how\n",
    "SNEKS = {\"basic\": snek, \"cute\": cute_snek}  # collection of sneks\n",
    "\n",
    "class SnekFactory(object):\n",
    "    def __init__(self, nlp, **cfg):\n",
    "        self.nlp = nlp\n",
    "        self.snek_style = cfg.get(\"snek_style\", \"basic\")\n",
    "        self.snek = SNEKS[self.snek_style]\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        print(self.snek)\n",
    "        return doc\n",
    "\n",
    "    def to_disk(self, path):\n",
    "    snek_path = path / \"snek.txt\"\n",
    "    with snek_path.open(\"w\", encoding=\"utf8\") as snek_file:\n",
    "        snek_file.write(self.snek)\n",
    "\n",
    "    def from_disk(self, path, **cfg):\n",
    "        snek_path = path / \"snek.txt\"\n",
    "        with snek_path.open(\"r\", encoding=\"utf8\") as snek_file:\n",
    "            self.snek = snek_file.read()\n",
    "        return self\n",
    "```\n",
    "\n",
    "**CUSTOM LANGUAGE CLASSES VIA ENTRY POINT**\n",
    "\n",
    "- `SnekLanguage` class for custom model BUT not modifying code to add a language\n",
    "\n",
    "```python\n",
    "#SNEK.PY\n",
    "\n",
    "from spacy.language import Language\n",
    "from spacy.attrs import LANG\n",
    "\n",
    "class SnekDefaults(Language.Defaults):\n",
    "    lex_attr_getters = dict(Language.Defaults.lex_attr_getters)\n",
    "    lex_attr_getters[LANG] = lambda text: \"snk\"\n",
    "\n",
    "\n",
    "class SnekLanguage(Language):\n",
    "    lang = \"snk\"\n",
    "    Defaults = SnekDefaults\n",
    "    # Some custom snek language stuff here\n",
    "```\n",
    "\n",
    "- Alongside `spacy_factories` also EP opton for `spacy_language` mapping language codes to language-specific `Language` subclasses:\n",
    "\n",
    "```python\n",
    "#SETUP.PY\n",
    "\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name=\"snek\",\n",
    "    entry_points={\n",
    "        \"spacy_factories\": [\n",
    "            \"snek = snek:SnekFactory\"\n",
    "         ]\n",
    "+       \"spacy_languages\": [\n",
    "+           \"sk = snek:SnekLanguage\"\n",
    "+       ]\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "- Then load custom `sk` language and resolved to `SnekLanguage` via custom EP\n",
    "- e.g. `meta.json` specifying `\"lang\": \"snk\"\n",
    "\n",
    "```python\n",
    "from spacy.util import get_lang_class\n",
    "\n",
    "SnekLanguage = get_lang_class(\"snk\")\n",
    "nlp = SnekLanguage()\n",
    "```\n",
    "\n",
    "**Distribution of Model**\n",
    "\n",
    "- `Language.to_disk()`\n",
    "- Dir created writing out WHOLE pipeline\n",
    "- Deploy via wrapping as Python package\n",
    "- **CLI** \n",
    "\n",
    "```bash\n",
    "python -m spacy package /home/me/data/en_example_model /home/me/my_models\n",
    "\n",
    "# creating\n",
    "DIRECTORY STRUCTURE\n",
    "└── /\n",
    "    ├── MANIFEST.in                   # to include meta.json\n",
    "    ├── meta.json                     # model meta data\n",
    "    ├── setup.py                      # setup file for pip installation\n",
    "    └── en_example_model              # model directory\n",
    "        ├── __init__.py               # init for pip installation\n",
    "        └── en_example_model-1.0.0    # model data\n",
    "```\n",
    "\n",
    "- eware of directories need to be name per naming conventions of `lang_name` and `lang_name-version`\n",
    "\n",
    "**Custom Model Setup**\n",
    "\n",
    "- `load()` method coming with model package tempaltes will handle assembling and returning `Language` object with loaded pipeline and data\n",
    "- If requiring custom pipeline component / custom language class => **ship code with model**\n",
    "- For examples of this, check out the implementations of spaCy’s [`load_model_from_init_py`](https://spacy.io/api/top-level#util.load_model_from_init_py) and [`load_model_from_path`](https://spacy.io/api/top-level#util.load_model_from_path) utility functions.\n",
    "\n",
    "**Building Model PKG**\n",
    "\n",
    "- `python setup.py sdist`\n",
    "- `pip install /path/to/ex_xxx.tar.gz`\n",
    "- **Loading only binary data** => `nlp = spacy.blank('en').from_disk('/path/to/data')\n",
    "\n",
    "Publishing a new version of spaCy often means re-training all available models, which is [quite a lot](https://spacy.io/usage/models#languages). To make this run smoothly, we’re using an automated build process and a [`spacy train`](https://spacy.io/api/cli#train) template that looks like this:\n",
    "\n",
    "```bash\n",
    "python -m spacy train {lang} {models_dir}/{name} {train_data} {dev_data} -m meta/{name}.json -V {version} -g {gpu_id} -n {n_epoch} -ns {n_sents}\n",
    "```\n",
    "\n",
    "In a directory `meta`, we keep `meta.json` templates for the individual models, containing all relevant information that doesn’t change across versions, like the name, description, author info and training data sources. When we train the model, we pass in the file to the meta template as the `--meta` argument, and specify the current model version as the `--version` argument\n",
    "\n",
    "On each epoch, the model is saved out with a `meta.json` using our template and added properties, like the `pipeline`, `accuracy` scores and the `spacy_version` used to train the model. After training completion, the best model is selected automatically and packaged using the [`package`](https://spacy.io/api/cli#package) command. Since a full meta file is already present on the trained model, no further setup is required to build a valid model package.\n",
    "\n",
    "```bash\n",
    "python -m spacy package -f {best_model} dist/\n",
    "cd dist/{model_name}\n",
    "python setup.py sdist\n",
    "```\n",
    "\n",
    "This process allows us to quickly trigger the model training and build process for all available models and languages, and generate the correct meta data automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "customer_feedback = open(\"customer_feedback_627.txt\").read()\n",
    "doc = nlp(customer_feedback)\n",
    "doc.to_disk(\"/tmp/customer_feedback_627.bin\")\n",
    "\n",
    "new_doc = Doc(Vocab()).from_disk(\"/tmp/customer_feedback_627.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oF6d9LJSFWri"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for doc in textcat.pipe(docs, batch_size=50):\n",
    "    pass\n",
    "\n",
    "scores = textcat.predict([doc1, doc2])\n",
    "\n",
    "textcat.set_annotations([doc1, doc2], scores)\n",
    "\n",
    "losses = {}\n",
    "optimizer = nlp.begin_training()\n",
    "textcat.update([doc1, doc2], [gold1, gold2], losses=losses, sgd=optimizer)\n",
    "\n",
    "optimizer = textcat.begin_training(pipeline=nlp.pipeline)\n",
    "# An optional optimizer. Should take two arguments weights and gradient, and an optional ID. Will be created via TextCategorizer if not set.\n",
    "\n",
    "# demo\n",
    "optimizer = nlp.begin_training(get_data)\n",
    "for itn in range(100):\n",
    "    random.shuffle(train_data)\n",
    "    for raw_text, entity_offsets in train_data:\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        gold = GoldParse(doc, entities=entity_offsets)\n",
    "        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)\n",
    "nlp.to_disk(\"/model\")\n",
    "\n",
    "\n",
    "# recommended simple training format\n",
    "{\n",
    "   \"entities\": [(0, 4, \"ORG\")],\n",
    "   \"heads\": [1, 1, 1, 5, 5, 2, 7, 5],\n",
    "   \"deps\": [\"nsubj\", \"ROOT\", \"prt\", \"quantmod\", \"compound\", \"pobj\", \"det\", \"npadvmod\"],\n",
    "   \"tags\": [\"PROPN\", \"VERB\", \"ADP\", \"SYM\", \"NUM\", \"NUM\", \"DET\", \"NOUN\"],\n",
    "   \"cats\": {\"BUSINESS\": 1.0},\n",
    "}\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        nlp.update([text], [annotations], sgd=optimizer)\n",
    "nlp.to_disk(\"/model\")\n",
    "\n",
    "# BATCH HEURISTIC\n",
    "def get_batches(train_data, model_type):\n",
    "    max_batch_sizes = {\"tagger\": 32, \"parser\": 16, \"ner\": 16, \"textcat\": 64}\n",
    "    max_batch_size = max_batch_sizes[model_type]\n",
    "    if len(train_data) < 1000:\n",
    "        max_batch_size /= 2\n",
    "    if len(train_data) < 500:\n",
    "        max_batch_size /= 2\n",
    "    batch_size = compounding(1, max_batch_size, 1.001)\n",
    "    batches = minibatch(train_data, size=batch_size)\n",
    "    return batches\n",
    "```\n",
    "\n",
    "> This will set the batch size to start at 1, and increase each batch until it reaches a maximum size. The tagger, parser and entity recognizer all take whole sentences as input, so they’re learning a lot of labels in a single example. You therefore need smaller batches for them. The batch size for the text categorizer should be somewhat larger, especially if your documents are long.\n",
    "\n",
    "> By default spaCy uses the Adam solver, with default settings (learning rate 0.001, beta1=0.9, beta2=0.999). Some researchers have said they found these settings terrible on their problems – but they’ve always performed very well in training spaCy’s models, in combination with the rest of our recipe. You can change these settings directly, by modifying the corresponding attributes on the optimizer object. You can also set environment variables, to adjust the defaults.\n",
    "\n",
    "> There are two other key hyper-parameters of the solver: L2 regularization, and gradient clipping (max_grad_norm). Gradient clipping is a hack that’s not discussed often, but everybody seems to be using. It’s quite important in helping to ensure the network doesn’t diverge, which is a fancy way of saying “fall over during training”. The effect is sort of similar to setting the learning rate low. It can also compensate for a large batch size (this is a good example of how the choices of all these hyper-parameters intersect).\n",
    "\n",
    "> For small datasets, it’s useful to set a high dropout rate at first, and decay it down towards a more reasonable value. This helps avoid the network immediately overfitting, while still encouraging it to learn some of the more interesting things in your data. spaCy comes with a decaying utility function to facilitate this. You might try setting:\n",
    "\n",
    "```python\n",
    "from spacy.util import decaying\n",
    "dropout = decaying(0.6, 0.2, 1e-4)\n",
    "```\n",
    "\n",
    "> The trick is to store the moving average of the weights during training. We don’t optimize this average – we just track it. Then when we want to actually use the model, we use the averages, not the most recent value. In spaCy (and Thinc) this is done by using a context manager, use_params, to temporarily replace the weights:\n",
    "\n",
    "```python\n",
    "with nlp.use_params(optimizer.averages):\n",
    "    nlp.to_disk(\"/model\")\n",
    "```\n",
    "\n",
    "> The context manager is handy because you naturally want to evaluate and save the model at various points during training (e.g. after each epoch). After evaluating and saving, the context manager will exit and the weights will be restored, so you resume training from the most recent value, rather than the average. By evaluating the model after each epoch, you can remove one hyper-parameter from consideration (the number of epochs). Having one less magic number to guess is extremely nice – so having the averaging under a context manager is very convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THINC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CIZ3s3tjwreS"
   },
   "outputs": [],
   "source": [
    "\"\"\"This script is experimental.\n",
    "\n",
    "Try pre-training the CNN component of the text categorizer using a cheap\n",
    "language modelling-like objective. Specifically, we load pre-trained vectors\n",
    "(from something like word2vec, GloVe, FastText etc), and use the CNN to\n",
    "predict the tokens' pre-trained vectors. This isn't as easy as it sounds:\n",
    "we're not merely doing compression here, because heavy dropout is applied,\n",
    "including over the input words. This means the model must often (50% of the time)\n",
    "use the context in order to predict the word.\n",
    "\n",
    "To evaluate the technique, we're pre-training with the 50k texts from the IMDB\n",
    "corpus, and then training with only 100 labels. Note that it's a bit dirty to\n",
    "pre-train with the development data, but also not *so* terrible: we're not using\n",
    "the development labels, after all --- only the unlabelled text.\n",
    "\n",
    "@plac.annotations(\n",
    "    width=(\"Width of CNN layers\", \"positional\", None, int),\n",
    "    embed_size=(\"Embedding rows\", \"positional\", None, int),\n",
    "    pretrain_iters=(\"Number of iterations to pretrain\", \"option\", \"pn\", int),\n",
    "    train_iters=(\"Number of iterations to pretrain\", \"option\", \"tn\", int),\n",
    "    train_examples=(\"Number of labelled examples\", \"option\", \"eg\", int),\n",
    "    vectors_model=(\"Name or path to vectors model to learn from\"),\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "import plac\n",
    "import random\n",
    "import spacy\n",
    "import thinc.extra.datasets\n",
    "from spacy.util import minibatch, use_gpu, compounding\n",
    "import tqdm\n",
    "from spacy._ml import Tok2Vec\n",
    "from spacy.pipeline import TextCategorizer\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E5paL1tujZHb"
   },
   "outputs": [],
   "source": [
    "pretrain_iters=30\n",
    "train_iters=30\n",
    "train_examples=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Fik3Ndj3bys"
   },
   "outputs": [],
   "source": [
    "# Load pretrain data - un-labelled\n",
    "\n",
    "def load_texts(limit=0):\n",
    "  train, dev = thinc.extra.datasets.imdb()\n",
    "  train_texts, train_labels = zip(*train)\n",
    "  dev_texts, dev_labels = zip(*train)\n",
    "  train_texts = list(train_texts)\n",
    "  dev_texts = list(dev_texts)\n",
    "  random.shuffle(train_texts)\n",
    "  random.shuffle(dev_texts)\n",
    "  if limit >= 1:\n",
    "      return train_texts[:limit]\n",
    "  else:\n",
    "      return list(train_texts) + list(dev_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DKgGxyI_3jvo"
   },
   "outputs": [],
   "source": [
    "temp_text = load_texts(limit=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVTb_Swc3e5j"
   },
   "outputs": [],
   "source": [
    "# Load Textcat pipe train-dev data - LABELLED \n",
    "\n",
    "def load_textcat_data(limit=0):\n",
    "    \"\"\"Load data from the IMDB dataset.\"\"\"\n",
    "    # Partition off part of the train data for evaluation\n",
    "    train_data, eval_data = thinc.extra.datasets.imdb()\n",
    "    random.shuffle(train_data)\n",
    "    train_data = train_data[-limit:]\n",
    "    texts, labels = zip(*train_data)\n",
    "    eval_texts, eval_labels = zip(*eval_data)\n",
    "    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n",
    "    eval_cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in eval_labels]\n",
    "    return (texts, cats), (eval_texts, eval_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_tXSiEw99jH"
   },
   "outputs": [],
   "source": [
    "temp_train, temp_eval = load_textcat_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "YntOSg7w-CFf",
    "outputId": "882abab7-9ebe-4b23-acde-61a10f7e757b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'NEGATIVE': True, 'POSITIVE': False},\n",
       " {'NEGATIVE': True, 'POSITIVE': False},\n",
       " {'NEGATIVE': False, 'POSITIVE': True},\n",
       " {'NEGATIVE': False, 'POSITIVE': True},\n",
       " {'NEGATIVE': True, 'POSITIVE': False}]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels \n",
    "temp_train[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XStVTuWLAdYM"
   },
   "outputs": [],
   "source": [
    "def prefer_gpu():\n",
    "    used = spacy.util.use_gpu(0)\n",
    "    if used is None:\n",
    "        return False\n",
    "    else:\n",
    "        import cupy.random\n",
    "\n",
    "        cupy.random.seed(0)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "83rHr6K_AdxP",
    "outputId": "e1e28563-7b38-44e2-91a0-1f95243efd3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU? True\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "numpy.random.seed(0)\n",
    "use_gpu = prefer_gpu()\n",
    "print(\"Using GPU?\", use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcHO0tDBGf0p"
   },
   "outputs": [],
   "source": [
    "# Textcat model construct\n",
    "\n",
    "def build_textcat_model(tok2vec, nr_class, width):\n",
    "    from thinc.v2v import Model, Softmax, Maxout\n",
    "    from thinc.api import flatten_add_lengths, chain\n",
    "    from thinc.t2v import Pooling, sum_pool, mean_pool, max_pool\n",
    "    from thinc.misc import Residual, LayerNorm\n",
    "    from spacy._ml import logistic, zero_init\n",
    "\n",
    "    with Model.define_operators({\">>\": chain}):\n",
    "        model = (\n",
    "            tok2vec\n",
    "            >> flatten_add_lengths\n",
    "            >> Pooling(mean_pool)\n",
    "            >> Softmax(nr_class, width)\n",
    "        )\n",
    "    model.tok2vec = tok2vec\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PeCqyM0SAfIi"
   },
   "outputs": [],
   "source": [
    "# Create NLP or model object\n",
    "\n",
    "def create_pipeline(width, embed_size, vectors_model):\n",
    "    print(\"Load vectors\")\n",
    "    nlp = spacy.load(vectors_model)\n",
    "    print(\"Start training\")\n",
    "    textcat = TextCategorizer(\n",
    "        nlp.vocab,\n",
    "        labels=[\"POSITIVE\", \"NEGATIVE\"],\n",
    "        model=build_textcat_model(\n",
    "            Tok2Vec(width=width, embed_size=embed_size), 2, width\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    nlp.add_pipe(textcat)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "wAvbPAY6AkQF",
    "outputId": "40f76005-1da1-44b4-f9f2-bee2de0892de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load vectors\n",
      "Start training\n"
     ]
    }
   ],
   "source": [
    "nlp = create_pipeline(width=300, embed_size=7500, vectors_model='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXGt_C7qHUMZ"
   },
   "outputs": [],
   "source": [
    "# no idea what for this FN\n",
    "def block_gradients(model):\n",
    "    from thinc.api import wrap\n",
    "\n",
    "    def forward(X, drop=0.0):\n",
    "        Y, _ = model.begin_update(X, drop=drop)\n",
    "        return Y, None\n",
    "\n",
    "    return wrap(forward, model)\n",
    "\n",
    "# Main FN for pretraining \"tensorizer\" pipeline using texts\n",
    "def train_tensorizer(nlp, texts, dropout, n_iter):\n",
    "    tensorizer = nlp.create_pipe(\"tensorizer\")\n",
    "    nlp.add_pipe(tensorizer)\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        for i, batch in enumerate(minibatch(tqdm.tqdm(texts))):\n",
    "            docs = [nlp.make_doc(text) for text in batch]\n",
    "            tensorizer.update(docs, None, losses=losses, sgd=optimizer, drop=dropout)\n",
    "        print(losses)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MR5yxqDIIzcf"
   },
   "source": [
    "For GPU support, we're grateful to use the work of Chainer's cupy module, which provides a numpy-compatible interface for GPU arrays. However, installing Chainer when no GPU is available currently causes an error. We therefore do not list Chainer as an explicit dependency — so building Thinc for GPU requires some extra steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "xB-ypHy6I6uU",
    "outputId": "7b6c71db-256d-468c-9882-46e25ae81f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2018 NVIDIA Corporation\n",
      "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
      "Cuda compilation tools, release 10.0, V10.0.130\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "Mwww-2ctkatO",
    "outputId": "a7042a18-bb64-49ea-9062-1894173f259c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cupy-cuda100 in /usr/local/lib/python3.6/dist-packages (5.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100) (1.11.0)\n",
      "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100) (0.4)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100) (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "# Seems right version of CuPy needed\n",
    "pip install cupy-cuda100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJgho9pkJghO"
   },
   "outputs": [],
   "source": [
    "# Bunch of CLI for asserting the right CUDA for THINIC GPU implementation\n",
    "# Optional?? \n",
    "\n",
    "!ls /usr/local/cuda -a\n",
    "!export CUDA_HOME=/usr/local/cuda # Or wherever your CUDA is\n",
    "!export PATH=$PATH:$CUDA_HOME/bin\n",
    "!pip install chainer\n",
    "!python -c \"import cupy; assert cupy\" # Check it installed\n",
    "!pip install thinc_gpu_ops thinc # Or `thinc[cuda]`\n",
    "!python -c \"import thinc_gpu_ops\" # Check the GPU ops were built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chCdw7BPs8ww"
   },
   "source": [
    "**ERROR**\n",
    "\n",
    "- CuPy dtype error\n",
    "  - seems to be Colab env-dep issues\n",
    "  - But dimension error still occurs \n",
    "  - Perhaps due to incorrect dimension or width and embed_size hyperparams \n",
    "  - These unknown as not given in GitHub source\n",
    "  \n",
    "- Solution\n",
    "  - Not able to use this snippet\n",
    "  - Resort to only TextCat training as above without pretrain this way\n",
    "  - Could still pretrain using CLI model method (see later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1012
    },
    "colab_type": "code",
    "id": "MGfS-gdTAl7j",
    "outputId": "2b9fc6b6-94df-4d13-96e2-f163b1aaa821"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-55e2632b262b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tensorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrain_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-e7596c89b67e>\u001b[0m in \u001b[0;36mtrain_tensorizer\u001b[0;34m(nlp, texts, dropout, n_iter)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mtensorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tensorizer.update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/manipulation/join.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/manipulation/join.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/manipulation/dims.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \"\"\"\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_atleast_nd_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/manipulation/dims.py\u001b[0m in \u001b[0;36m_atleast_nd_helper\u001b[0;34m(n, arys)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mnew_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_atleast_nd_shape_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/creation/from_data.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported dtype object"
     ]
    }
   ],
   "source": [
    "optimizer = train_tensorizer(nlp, temp_text, dropout=0.2, n_iter=pretrain_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing TextCategorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "- to make default, override `Language.factories`\n",
    "- `Language.factories['textcat'] = lambda nlp, **cfg: CustomTextCat(nlp.vocab, **cfg)`\n",
    "- use:  `nlp.create_pipe('textcat')`\n",
    "\n",
    "class CustomTextCat(spacy.pipeline.TextCategorizer):\n",
    "    @classmethod\n",
    "    def Model(cls, nr_class=1, width=128, **cfg):\n",
    "        # this needs to return a Thinc model\n",
    "        return build_text_classifier(nr_class, width, **cfg)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tp = 0.0\n",
    "fp = 0.0\n",
    "fn = 0.0\n",
    "for eg in test_examples:\n",
    "    doc = nlp(eg[\"text\"])\n",
    "    guesses = set((ent.start_char, ent.end_char, ent.label_) for ent in doc.ents)\n",
    "    truths = set((span['start'], span['end'], span['label']) for span in eg['spans'])\n",
    "    tp += len(guesses.intersection(truths))\n",
    "    fn += len(truths - guesses)\n",
    "    fp += len(guesses - truths)\n",
    "precision = tp / (tp+fp+1e-10)\n",
    "recall = tp / (tp+fn+1e-10)\n",
    "fscore = (2 * precision * recall) / (precision + recall + 1e-10)\n",
    "\n",
    "# usage\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "model = EntityRecognizer(nlp, label=['PERSON', 'ORG'])\n",
    "stats = model.evaluate(examples, no_missing=True)\n",
    "\n",
    "def gold_to_spacy(dataset, spacy_model):\n",
    "    annos = []\n",
    "    for eg in dataset:\n",
    "        entities = [(span['start'], span['end'], span['label'])\n",
    "                    for span in eg.get('spans', [])]\n",
    "        if bilou:\n",
    "            doc = nlp(eg['text'])\n",
    "            entities = spacy.gold.bilou_tags_from_offsets(doc, entities)\n",
    "            anno_entry = [eg['text'], entities]\n",
    "        else:\n",
    "            anno_entry = [eg['text'], {'entities': entities}]\n",
    "        annos.append(anno_entry)\n",
    "\n",
    "def eval_prf(ner_model, examples):\n",
    "    scorer = spacy.scorer.Scorer()\n",
    "    for input_, anno in examples:\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        gold = spacy.gold.GoldParse(doc_gold_text, entities=anno['entities'])\n",
    "        pre_value = ner_model(input_)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores\n",
    "\n",
    "def model_stats(dataset, spacy_model, label=None, is_prf=False):\n",
    "    \"\"\"Evaluate model accuracy of model based on dataset without training\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(spacy_model)\n",
    "    \n",
    "    if is_prf:\n",
    "        examples = gold_to_spacy(dataset, spacy_model)\n",
    "        score = eval_prf(nlp, examples)\n",
    "        print(\"Precision {:0.4f}\\tRecall {:0.4f}\\tF-score {:0.4f}\".format(score['ents_p'],\n",
    "                                                                          score['ents_r'],\n",
    "                                                                          score['ents_f']))\n",
    "    else:\n",
    "        model = EntityRecognizer(nlp, label=label)\n",
    "        evaldoc = merge_spans(dataset)\n",
    "        evals = list(split_sentences(model.nlp, evaldoc))\n",
    "        scores = model.evaluate(evals)\n",
    "        print(\"Accuracy {:0.4f}\\tRight {:0.0f}\\tWrong {:0.0f}\\tUnknown {:0.0f}\\tEntities {:0.0f}\".format(scores['acc'],\n",
    "                                                                                                         scores['right'],\n",
    "                                                                                                         scores['wrong'],\n",
    "                                                                                                         scores['unk'],\n",
    "                                                                                                         scores['ents']))\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CI of NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# CI\n",
    "doc = nlp.make_doc(text)\n",
    "(beams, somethingelse) = nlp.entity.beam_parse([doc], beam_width=16, beam_density=0.0001)\n",
    "for score, ents in nlp.entity.moves.get_beam_parses(beams[0]):\n",
    "    print(score, ents)\n",
    "    entity_scores = defaultdict(float)\n",
    "    for start, end, label in ents:\n",
    "        # print(\"here\")\n",
    "        entity_scores[(start, end, label)] += score\n",
    "        print('entity_scores', entity_scores)\n",
    "for (start, end, label), value in entity_scores.items():\n",
    "    if label == 'LOCATION':\n",
    "        print(start, tokens[start], value)\n",
    "        \n",
    "        \n",
    "# another impl\n",
    "ner = nlp.get_pipe('ner')\n",
    "docs = [nlp.make_doc(text) for text in batch]\n",
    "beams = ner.beam_parse(docs, beam_width=16)\n",
    "for beam in beams:\n",
    "    entities = ner.moves.get_beam_annot(beam)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Rule-Based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Based\n",
    "\n",
    "**Adding 3 patterns**\n",
    "\n",
    "- 'hello' or 'HELLO'\n",
    "- is_punct flag == True\n",
    "- lowercase == \"world\"\n",
    "\n",
    "```python\n",
    "[{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "```\n",
    "\n",
    "**Important note**\n",
    "\n",
    "> When writing patterns, keep in mind that each dictionary represents one token. If spaCy’s tokenization doesn’t match the tokens defined in a pattern, the pattern is not going to produce any results. When developing complex patterns, make sure to check examples against spaCy’s tokenization:\n",
    "\n",
    "```python\n",
    "doc = nlp(u\"A complex-example,!\")\n",
    "print([token.text for token in doc])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# vocab must be shared with document the matcher operates on\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", None, pattern) # first arg is ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15578876784678163569 HelloWorld 0 3 Hello, World\n"
     ]
    }
   ],
   "source": [
    "# load text using same nlp object (hence same vocab space)\n",
    "doc = nlp(u\"Hello, World! Hello world!\")\n",
    "# operate on doc\n",
    "matches = matcher(doc) \n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15578876784678163569 HelloWorld 0 3 Hello, World\n",
      "15578876784678163569 HelloWorld 4 6 Hello world\n"
     ]
    }
   ],
   "source": [
    "# Optionally, we could also choose to add more than one pattern, \n",
    "# for example to also match sequences without punctuation between “hello” and “world”:\n",
    "\n",
    "matcher.add(\"HelloWorld\", None,\n",
    "            [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}],\n",
    "            [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default, the matcher will only return the matches and not do anything else, like merge entities or assign labels. This is all up to you and can be defined individually for each pattern, by passing in a callback function as the `on_match` argument on `add()`. This is useful, because it lets you write entirely custom and pattern-specific logic. For example, you might want to **merge some patterns into one token, while adding entity labels for other pattern types**. You shouldn’t have to create different matchers for each of those processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Available token attributes**\n",
    "\n",
    "`ORTH` \n",
    "- unicode\tThe exact verbatim text of a token.\n",
    "\n",
    "`TEXT` \n",
    "- V2.1\tunicode\tThe exact verbatim text of a token.\n",
    "\n",
    "`LOWER` \n",
    "- unicode\tThe lowercase form of the token text.\n",
    "\n",
    "`LENGTH` \n",
    "- int\tThe length of the token text.\n",
    "\n",
    "`IS_ALPHA` \n",
    "- , IS_ASCII, IS_DIGIT\tbool\tToken text consists of alphanumeric characters, ASCII characters, digits.\n",
    "\n",
    "`IS_LOWER` \n",
    "- , IS_UPPER, IS_TITLE\tbool\tToken text is in lowercase, uppercase, titlecase.\n",
    "\n",
    "`IS_PUNCT` \n",
    "- , IS_SPACE, IS_STOP\tbool\tToken is punctuation, whitespace, stop word.\n",
    "\n",
    "`LIKE_NUM` \n",
    "- , LIKE_URL, LIKE_EMAIL\tbool\tToken text resembles a number, URL, email.\n",
    "\n",
    "`POS` \n",
    "- , TAG, DEP, LEMMA, SHAPE\tunicode\tThe token’s simple and extended part-of-speech tag, dependency label, lemma, shape.\n",
    "\n",
    "`ENT_TYPE` \n",
    "- unicode\tThe token’s entity label.\n",
    "\n",
    "`_` \n",
    "- V2.1\tdict\tProperties in custom extension attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extended pattern syntax and attributes V2.1**\n",
    "\n",
    "> Instead of mapping to a single value, token patterns can also map to a `dictionary of properties`. For example, to specify that the value of a lemma should be part of a list of values, or to set a minimum character length. The following rich comparison attributes are available:\n",
    "\n",
    "```python\n",
    "# Matches \"love cats\" or \"likes flowers\"\n",
    "pattern1 = [{\"LEMMA\": {\"IN\": [\"like\", \"love\"]}},\n",
    "            {\"POS\": \"NOUN\"}] # NOT_IN, ==, >=, etc\n",
    "\n",
    "# Matches tokens of length >= 10\n",
    "pattern2 = [{\"LENGTH\": {\">=\": 10}}]\n",
    "```\n",
    "\n",
    "**REGEX**\n",
    "\n",
    "In some cases, only matching tokens and token attributes isn’t enough – for example, you might want to match different spellings of a word, without having to add a new pattern for each spelling.\n",
    "\n",
    "```python\n",
    "pattern = [{\"TEXT\": {\"REGEX\": \"^([Uu](\\.?|nited) ?[Ss](\\.?|tates)\"}},\n",
    "           {\"LOWER\": \"president\"}]\n",
    "```\n",
    "\n",
    "> 'REGEX' as an operator (instead of a top-level property that only matches on the token’s text) allows defining rules for any string value, including custom attributes\n",
    "\n",
    "```python\n",
    "# Match tokens with fine-grained POS tags starting with 'V'\n",
    "pattern = [{\"TAG\": {\"REGEX\": \"^V\"}}]\n",
    "\n",
    "# Match custom attribute values with regular expressions\n",
    "pattern = [{\"_\": {\"country\": {\"REGEX\": \"^([Uu](\\.?|nited) ?[Ss](\\.?|tates)\"}}}]\n",
    "```\n",
    "**Operators and quantifiers**\n",
    "\n",
    "The matcher also lets you use quantifiers, specified as the `OP` key. Quantifiers let you define sequences of tokens to be matched, e.g. one or more punctuation marks, or specify optional tokens. Note that there are no nested or scoped quantifiers – instead, you can build those behaviors with `on_match` callbacks.\n",
    "\n",
    "`!` \n",
    "- Negate the pattern, by requiring it to match exactly 0 times.\n",
    "\n",
    "`?` \n",
    "- Make the pattern optional, by allowing it to match 0 or 1 times.\n",
    "\n",
    "`+` \n",
    "- Require the pattern to match 1 or more times.\n",
    "\n",
    "`*` \n",
    "- Allow the pattern to match zero or more times.\n",
    "\n",
    "```python\n",
    "pattern = [{\"LOWER\": \"hello\"},\n",
    "           {\"IS_PUNCT\": True, \"OP\": \"?\"}]\n",
    "```\n",
    "\n",
    "**Wildcard**\n",
    "- empty dictionary, `{}` as a wildcard representing any token. \n",
    "- useful if you know the context of what you’re trying to match, but very little about the specific token and its characters.\n",
    "- For example, let’s say you’re trying to extract people’s user names from your data. All you know is that they are listed as `“User name: {username}“`. The name itself may contain any character, but no whitespace – so you’ll know it will be handled as one token.\n",
    "\n",
    "```python\n",
    "[{\"ORTH\": \"User\"}, {\"ORTH\": \"name\"}, {\"ORTH\": \":\"}, {}]\n",
    "```\n",
    "\n",
    "**Adding on_match rules**\n",
    "\n",
    "- See below demo\n",
    "> match all mentions of “Google I/O” (which spaCy tokenizes as `['Google', 'I', '/', 'O']`). To be safe, you only match on the uppercase versions, in case someone has written it as “Google i/o”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"EVENT\")\n",
    "    doc.ents += (entity,)\n",
    "    print(entity.text)\n",
    "\n",
    "pattern = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
    "matcher.add(\"GoogleIO\", add_event_ent, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"This is a text about Google I/O.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">This is a text about Google I/O.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "html = displacy.render(doc, style=\"ent\", page=True,\n",
    "                options={\"ents\": [\"EVENT\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Using linguistic annotations**\n",
    "\n",
    "- analysing user comments and you want to find out what people are saying about Facebook. \n",
    "- finding adjectives following “Facebook is” or “Facebook was”.\n",
    "\n",
    "```python\n",
    "[{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"ADJ\"}]\n",
    "```\n",
    "- quick overview of the results, collect all sentences containing a match and render them with the displaCy visualizer. \n",
    "- In the callback function, you’ll have access to the start and end of each match, as well as the parent Doc. \n",
    "- determine doc[start : end.sent], and calculate the start and end of the matched span within the sentence.\n",
    "- Using displaCy in “manual” mode lets you pass in a list of dictionaries containing the text and entities to render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"I'd say that Facebook is evil.\",\n",
       "  'ents': [{'start': 13, 'end': 29, 'label': 'MATCH'}]},\n",
       " {'text': 'Facebook is pretty cool, right?',\n",
       "  'ents': [{'start': 0, 'end': 23, 'label': 'MATCH'}]}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]  # Matched span\n",
    "    sent = span.sent  # Sentence containing matched span\n",
    "    # Append mock entity for match in displaCy style to matched_sents\n",
    "    # get the match span by ofsetting the start and end of the span with the\n",
    "    # start and end of the sentence in the doc\n",
    "    match_ents = [{\n",
    "        \"start\": span.start_char - sent.start_char,\n",
    "        \"end\": span.end_char - sent.start_char,\n",
    "        \"label\": \"MATCH\",\n",
    "    }]\n",
    "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
    "           {\"POS\": \"ADJ\"}]\n",
    "matcher.add(\"FacebookIs\", collect_sents, pattern)  # add pattern\n",
    "doc = nlp(u\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "matched_sents\n",
    "\n",
    "# error to be fixed\n",
    "# Serve visualization of sentences containing match with displaCy\n",
    "# set manual=True to make displaCy render straight from a dictionary\n",
    "# (if you're not running the code within a Jupyer environment, you can\n",
    "# use displacy.serve instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Phrase Matching\n",
    "\n",
    "- match large terminology lists to use `PhraseMatcher` and create `Doc` objects instead of token patterns\n",
    "- The Doc patterns can contain single or multiple tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terminology_list = [u\"Barack Obama\", u\"Angela Merkel\", u\"Washington, D.C.\"]\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terminology_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Barack Obama, Angela Merkel, Washington, D.C.]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angela Merkel\n",
      "Barack Obama\n",
      "Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "matcher.add(\"TerminologyList\", None, *patterns)\n",
    "\n",
    "doc = nlp(u\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "          u\"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed on creating pattern**\n",
    "\n",
    "```bash\n",
    "- patterns = [nlp(term) for term in LOTS_OF_TERMS]\n",
    "+ patterns = [nlp.make_doc(term) for term in LOTS_OF_TERMS]\n",
    "+ patterns = list(nlp.tokenizer.pipe(LOTS_OF_TERMS))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matching on other token attributes**\n",
    "\n",
    "- By default, the PhraseMatcher will match on the `verbatim token text, e.g. Token.text.`\n",
    "- By setting the attr argument on initialization, you can change which token attribute the matcher should use when comparing the phrase pattern to the matched Doc. \n",
    "- For example, using the attribute `LOWER` lets you match on Token.lower and create case-insensitive match patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(name) for name in [u\"Angela Merkel\", u\"Barack Obama\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Angela Merkel, Barack Obama]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched based on lowercase token text: angela merkel\n",
      "Matched based on lowercase token text: barack Obama\n"
     ]
    }
   ],
   "source": [
    "matcher.add(\"Names\", None, *patterns)\n",
    "\n",
    "doc = nlp(u\"angela merkel and us president barack Obama\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on lowercase token text:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matching on SHAPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched based on token shape: 192.168.1.1\n",
      "Matched based on token shape: 192.168.2.1\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
    "matcher.add(\"IP\", None, nlp(u\"127.0.0.1\"), nlp(u\"127.127.0.0\"))\n",
    "\n",
    "doc = nlp(u\"Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on token shape:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In theory, the same also works for attributes like POS. For example, a pattern nlp(\"I like cats\") matched based on its part-of-speech tag would return a match for “I love dogs”. You could also match on boolean flags like IS_PUNCT to match phrases with the same sequence of punctuation and non-punctuation tokens as the pattern. But this can easily get confusing and doesn’t have much of an advantage over writing one or two token patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based ER\n",
    "\n",
    "- The `EntityRuler` is an exciting new **component** that lets you **add named entities based on pattern dictionaries, and makes it easy to combine rule-based and statistical named entity recognition for even more powerful models.**\n",
    "\n",
    "**Entity Pattern**\n",
    "\n",
    "1. **Phrase patterns** for exact string matches (string).\n",
    "```python \n",
    "{\"label\": \"ORG\", \"pattern\": \"Apple\"}\n",
    "```\n",
    "2. **Token patterns** with one dictionary describing one token (list).\n",
    "```python\n",
    "{\"label\": \"GPE\", \"pattern\": [{\"lower\": \"san\"}, {\"lower\": \"francisco\"}]}\n",
    "```\n",
    "\n",
    "**Using the entity ruler**\n",
    "- The `EntityRuler` is a **pipeline component** that’s typically added via `nlp.add_pipe`. \n",
    "- When the nlp object is called on a text, it will **find matches in the doc and add them as entities to the `doc.ents`, using the specified pattern label as the entity label.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG'), ('San Francisco', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = English()\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"lower\": \"san\"}, {\"lower\": \"francisco\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(u\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The entity ruler is designed to **integrate with spaCy’s existing statistical models and enhance the named entity recognizer.**\n",
    "> If it’s **added before the `\"ner\"` component,** the entity recognizer will **respect the existing entity spans** and **adjust its predictions around it.**\n",
    "> This can significantly improve accuracy in some cases. **If it’s added after the \"ner\" component, the entity ruler will only add spans to the `doc.ents` if they don’t overlap with existing entities predicted by the model.**\n",
    "> To **overwrite overlapping entities**, you can set `overwrite_ents=True` on initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MyCorp Inc.', 'ORG'), ('U.S.', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(u\"MyCorp Inc. is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pattern FILE**\n",
    "\n",
    "The `to_disk` and `from_disk` let you save and load patterns to and from JSONL (newline-delimited JSON) files, containing one pattern object per line.\n",
    "\n",
    "```python\n",
    "# pattern.jsonl\n",
    "{\"label\": \"ORG\", \"pattern\": \"Apple\"}\n",
    "{\"label\": \"GPE\", \"pattern\": [{\"lower\": \"san\"}, {\"lower\": \"francisco\"}]}\n",
    "\n",
    "ruler.to_disk(\"./patterns.jsonl\")\n",
    "new_ruler = EntityRuler(nlp).from_disk(\"./patterns.jsonl\")\n",
    "```\n",
    "\n",
    "> When you save out an `nlp` object that has an `EntityRuler` added to its pipeline, its patterns are automatically exported to the model directory:\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = EntityRuler(nlp)\n",
    "ruler.add_patterns([{\"label\": \"ORG\", \"pattern\": \"Apple\"}])\n",
    "nlp.add_pipe(ruler)\n",
    "nlp.to_disk(\"/path/to/model\")\n",
    "```\n",
    "\n",
    "> The saved model now **includes the \"entity_ruler\" in its \"pipeline\" setting in the meta.json,** and the model directory contains a file `entityruler.jsonl` with the patterns. When you load the model back in, all pipeline components will be restored and deserialized – including the entity ruler. **This lets you ship powerful model packages with binary weights and rules included!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Model and Rules\n",
    "\n",
    "You can combine statistical and rule-based components in a variety of ways. Rule-based components can be used to improve the accuracy of statistical models, by presetting tags, entities or sentence boundaries for specific tokens. The statistical models will usually respect these preset annotations, which sometimes improves the accuracy of other decisions. You can also use rule-based components after a statistical model to correct common errors. Finally, rule-based components can reference the attributes set by statistical models, in order to implement more abstract logic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleIO Google I/O\n",
      "HAPPY 😀\n",
      "HAPPY 😀😀\n",
      "HAPPY 😀\n",
      "Sentiment 0.30000001192092896\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def set_sentiment(matcher, doc, i, matches):\n",
    "    doc.sentiment += 0.1\n",
    "\n",
    "pattern1 = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
    "pattern2 = [[{\"ORTH\": emoji, \"OP\": \"+\"}] for emoji in [\"😀\", \"😂\", \"🤣\", \"😍\"]]\n",
    "matcher.add(\"GoogleIO\", None, pattern1)  # Match \"Google I/O\" or \"Google i/o\"\n",
    "matcher.add(\"HAPPY\", set_sentiment, *pattern2)  # Match one or more happy emoji\n",
    "\n",
    "doc = nlp(u\"A text about Google I/O 😀😀\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(string_id, span.text)\n",
    "print(\"Sentiment\", doc.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Expanding Named Entity**\n",
    "\n",
    "- For example, the corpus spaCy’s English models were trained on defines a `PERSON` entity as just the **person name, without titles like “Mr” or “Dr”.**\n",
    "- This makes sense, because it makes it easier to resolve the entity type back to a knowledge base. \n",
    "- But what if your application needs the full names, including the titles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alex Smith', 'PERSON'), ('first', 'ORDINAL'), ('Acme Corp Inc.', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While you could try and teach the model a new definition of the PERSON entity by **updating it with more examples of spans that include the title, this might not be the most efficient approach.**\n",
    "- The existing model was trained on over **2 million words**, so in order to completely change the definition of an entity type, you might need a lot of training examples. \n",
    "- However, if you already have the predicted PERSON entities, you can use a **rule-based approach** that checks whether they come with a title and if so, expands the entity span by one token. \n",
    "- After all, what all titles in this example have in common is that if they occur, they occur in the previous token right before the person entity.\n",
    "\n",
    "> modify `Doc` and its `doc.ents` and returns it. This is **exactly what a pipeline component does**, so in order to let it run automatically when processing a text with the nlp object, we can use `nlp.add_pipe` to add it to the current pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dr Alex Smith', 'PERSON'), ('first', 'ORDINAL'), ('Acme Corp Inc.', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def expand_person_entities(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and ent.start != 0:\n",
    "            prev_token = doc[ent.start - 1]\n",
    "            if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
    "                new_ents.append(new_ent)\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "# Add the component after the named entity recognizer\n",
    "nlp.add_pipe(expand_person_entities, after='ner')\n",
    "\n",
    "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An alternative approach would be to an **extension attribute** like `._.person_title` and add it to `Span` objects (which includes entity spans in `doc.ents`). The **advantage here is that the entity text stays intact and can still be used to look up the name in a knowledge base.** The following function takes a Span object, checks the previous token if it’s a `PERSON` entity and returns the title if one is found. The Span.doc attribute gives us easy access to the span’s parent document.\n",
    "```python\n",
    "def get_person_title(span):\n",
    "    if span.label_ == \"PERSON\" and span.start != 0:\n",
    "        prev_token = span.doc[span.start - 1]\n",
    "        if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "            return prev_token.text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alex Smith', 'PERSON', 'Dr'), ('first', 'ORDINAL', None), ('Acme Corp Inc.', 'ORG', None)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_person_title(span):\n",
    "    if span.label_ == \"PERSON\" and span.start != 0:\n",
    "        prev_token = span.doc[span.start - 1]\n",
    "        if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "            return prev_token.text\n",
    "\n",
    "# Register the Span extension as 'person_title'\n",
    "Span.set_extension(\"person_title\", getter=get_person_title)\n",
    "\n",
    "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_, ent._.person_title) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Using entities, part-of-speech tags and the dependency parse**\n",
    "\n",
    "- Let’s say you want to parse **professional biographies and extract the person names and company names, and whether it’s a company they’re currently working at, or a previous company.**\n",
    "- One approach could be to try and train a named entity recognizer to predict CURRENT_ORG and PREVIOUS_ORG – but this distinction is very subtle and something the entity recognizer may struggle to learn. Nothing about “Acme Corp Inc.” is inherently “current” or “previous”.\n",
    "- However, the **syntax** of the sentence holds some very important clues: we can check for **trigger words like “work”, whether they’re past tense or present tense, whether company names are attached to it and whether the person is the subject.**\n",
    "- All of this information is available in the part-of-speech tags and the dependency parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alex Smith', 'PERSON'), ('Acme Corp Inc.', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a909aad5c4c7445ba5ab085012f6bcee-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Alex</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Smith</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">worked</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VBD</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">IN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Acme</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Corp</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Inc.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a909aad5c4c7445ba5ab085012f6bcee-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a909aad5c4c7445ba5ab085012f6bcee-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a909aad5c4c7445ba5ab085012f6bcee-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a909aad5c4c7445ba5ab085012f6bcee-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a909aad5c4c7445ba5ab085012f6bcee-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a909aad5c4c7445ba5ab085012f6bcee-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a909aad5c4c7445ba5ab085012f6bcee-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a909aad5c4c7445ba5ab085012f6bcee-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">subtok</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,179.0 L762,167.0 778,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a909aad5c4c7445ba5ab085012f6bcee-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a909aad5c4c7445ba5ab085012f6bcee-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">subtok</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a909aad5c4c7445ba5ab085012f6bcee-0-5\" stroke-width=\"2px\" d=\"M595,177.0 C595,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a909aad5c4c7445ba5ab085012f6bcee-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep', options={'fine_grained': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this example, “worked” is the root of the sentence and is a past tense verb. \n",
    "    - Its subject is “Alex Smith”, the person who worked. “at Acme Corp Inc.” is a prepositional phrase attached to the verb “worked”. \n",
    "    - To extract this relationship, we can start by looking at the predicted PERSON entities, find their heads and check whether they’re attached to a trigger word like “work”. Next, we can check for prepositional phrases attached to the head and whether they contain an ORG entity. \n",
    "    - Finally, to determine whether the company affiliation is current, we can check the head’s part-of-speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': Alex Smith, 'orgs': [Acme Corp Inc.], 'past': True}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"41bd36957b4a44e0a3f45d662df3dd5c-0\" class=\"displacy\" width=\"750\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Alex Smith</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">worked</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VBD</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">IN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Acme Corp Inc.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-41bd36957b4a44e0a3f45d662df3dd5c-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-41bd36957b4a44e0a3f45d662df3dd5c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-41bd36957b4a44e0a3f45d662df3dd5c-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-41bd36957b4a44e0a3f45d662df3dd5c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-41bd36957b4a44e0a3f45d662df3dd5c-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-41bd36957b4a44e0a3f45d662df3dd5c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import merge_entities\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_person_orgs(doc):\n",
    "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    for ent in person_entities:\n",
    "        head = ent.root.head\n",
    "        if head.lemma_ == \"work\":\n",
    "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "            for prep in preps:\n",
    "                orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n",
    "                print({'person': ent, 'orgs': orgs, 'past': head.tag_ == \"VBD\"})\n",
    "    return doc\n",
    "\n",
    "# To make the entities easier to work with, we'll merge them into single tokens\n",
    "nlp.add_pipe(merge_entities)\n",
    "nlp.add_pipe(extract_person_orgs)\n",
    "\n",
    "doc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n",
    "# If you're not in a Jupyter / IPython environment, use displacy.serve\n",
    "displacy.render(doc, options={'fine_grained': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you change the sentence structure above, for example to “was working”, you’ll notice that our current logic fails and doesn’t correctly detect the company as a past organization. That’s because the root is a participle and the tense information is in the attached auxiliary “was”:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To solve this, we can adjust the rules to also check for the above construction:\n",
    "```python\n",
    "def extract_person_orgs(doc):\n",
    "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    for ent in person_entities:\n",
    "        head = ent.root.head\n",
    "        if head.lemma_ == \"work\":\n",
    "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "            for prep in preps:\n",
    "                orgs = [t for t in prep.children if t.ent_type_ == \"ORG\"]\n",
    "                aux = [token for token in head.children if token.dep_ == \"aux\"]\n",
    "                past_aux = any(t.tag_ == \"VBD\" for t in aux)\n",
    "                past = head.tag_ == \"VBD\" or head.tag_ == \"VBG\" and past_aux\n",
    "                print({'person': ent, 'orgs': orgs, 'past': past})\n",
    "    return doc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Alex Smith, worked, at, Acme Corp Inc.]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok for tok in doc if tok.has_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CUSTOM SIMILARITY HOOKS\n",
    "\n",
    "class SimilarityModel(object):\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        doc.user_hooks[\"similarity\"] = self.similarity\n",
    "        doc.user_span_hooks[\"similarity\"] = self.similarity\n",
    "        doc.user_token_hooks[\"similarity\"] = self.similarity\n",
    "\n",
    "    def similarity(self, obj1, obj2):\n",
    "        y = self._model([obj1.vector, obj2.vector])\n",
    "        return float(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.user_hooks['vector'] = np.array(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-da973cf9aec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.vector.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "print(doc.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ines Guide\n",
    "\n",
    "```python\n",
    "### Ines Guide on NLP Task\n",
    "- start with generic cats and extract from which more specific info\n",
    "- start with lg stock NE to get similar info\n",
    "- incrementally updating particular NE with new data - fully batch train at end\n",
    "- add rules \"Q2 2018\" and match patterns based on regex and spaCy's linguistic pipelines\n",
    "- use \"parser\" to extract relationships around NE\n",
    "- train TextCat component to assign labels to whole sentences or paragraphs (good for less dense) \n",
    "- e.g. \"Sales totalled 864 million\"\n",
    "    - tokens 864 million == \"MONEY\"\n",
    "    - walk up the tree and check how it attaches to rest of sentence\n",
    "        - direct object attached to verb.lemma \"total / to total\" with subject \"sales\"\n",
    "    - NER \"NOUN\" and \"VERB\" for example\n",
    "- e.g. financial report: period/timeframe encoded in headline\n",
    "    - Detect headlines - rules or textcat \"HEADLINE\"\n",
    "    - say all text in between are \"body\" associated with headline\n",
    "    - detect if headline references a quarter, normalize into structured\n",
    "        - e.g. \"second quarter of 2018\" -> {\"q\": 2, \"year\": 2018} via Custom Attribute\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# register ext token._.headline\n",
    "Token.set_extension('headline', default=None)\n",
    "Token.set_extension('year', defualt=None)\n",
    "\n",
    "doc = nlp(\"This is a headline. This is some text.\")\n",
    "headline = doc[0:5] # Span containing token 0-4\n",
    "\n",
    "for token in doc[5:10]: # rest of text\n",
    "    token._.headline = headline\n",
    "    # set structured data on the token  could come from\n",
    "    # a function parsing headline text \n",
    "    token._.year = get_year_from_headline(headline)\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> for any token in doc now, \"MONEY\" ent now able to check its `._.year` to see if it's linked with a year based on its headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiTzXPWDFWrk"
   },
   "source": [
    "# Extra EcoSystem Lib\n",
    "\n",
    "### ADAM - Wikipedia Q&A\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/5hirish/adam_qas.git\n",
    "cd adam_qas\n",
    "pip install -r requirements.txt\n",
    "python -m qas.adam 'When was linux kernel version 4.0 released ?'\n",
    "```\n",
    "\n",
    "### AllenNLP\n",
    "\n",
    "- use to develop pipeline components ådding annotations to `Doc`\n",
    "\n",
    "### ExcelCy - Excel Integration with spaCy. Training NER using XLSX from PDF, DOCX, PPT, PNG or JPG.\n",
    "\n",
    "```bash\n",
    "from excelcy import ExcelCy\n",
    "# collect sentences, annotate Entities and train NER using spaCy\n",
    "excelcy = ExcelCy.execute(file_path='https://github.com/kororo/excelcy/raw/master/tests/data/test_data_01.xlsx')\n",
    "# use the nlp object as per spaCy API\n",
    "doc = excelcy.nlp('Google rebrands its business apps')\n",
    "# or save it for faster bootstrap for application\n",
    "excelcy.nlp.to_disk('/model')\n",
    "```\n",
    "\n",
    "### explacy - visualise spaCy parse\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "import explacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "explacy.print_parse_info(nlp, 'The salad was surprisingly tasty.')\n",
    "```\n",
    "\n",
    "### spacy_hunspell - spell checker\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "from spacy_hunspell import spaCyHunSpell\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "hunspell = spaCyHunSpell(nlp, 'mac')\n",
    "nlp.add_pipe(hunspell)\n",
    "doc = nlp('I can haz cheezeburger.')\n",
    "haz = doc[2]\n",
    "haz._.hunspell_spell  # False\n",
    "haz._.hunspell_suggest  # ['ha', 'haze', 'hazy', 'has', 'hat', 'had', 'hag', 'ham', 'hap', 'hay', 'haw', 'ha z']\n",
    "```\n",
    "\n",
    "### spacy-lookup \n",
    "- powerful NER matcher for large dictionaries using FlashText module\n",
    "-  The extension sets the custom `Doc, Token and Span` attributes `._.is_entity, ._.entity_type, ._.has_entities and ._.entities.` Named Entities are matched using the python module flashtext, and looked up in the data provided by different dictionaries.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "from spacy_lookup import Entity\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "entity = Entity(keywords_list=['python', 'java platform'])\n",
    "nlp.add_pipe(entity, last=True)\n",
    "\n",
    "doc = nlp(u\"I am a product manager for a java and python.\")\n",
    "assert doc._.has_entities == True\n",
    "assert doc[2:5]._.has_entities == True\n",
    "assert doc[0]._.is_entity == False\n",
    "assert doc[3]._.is_entity == True\n",
    "print(doc._.entities)\n",
    "```\n",
    "\n",
    "### spacy-vis using Hierplane\n",
    "\n",
    "- local installation https://github.com/DeNeutoy/spacy-vis\n",
    "\n",
    "```bash\n",
    "docker run -p 8080:8080 -it markn/spacy-vis bash bin/serve\n",
    "```\n",
    "\n",
    "### textpipe - clean and extracat metadata\n",
    "\n",
    "```python\n",
    "from textpipe import doc, pipeline\n",
    "sample_text = 'Sample text! <!DOCTYPE>'\n",
    "document = doc.Doc(sample_text)\n",
    "print(document.clean)\n",
    "'Sample text!'\n",
    "print(document.language)\n",
    "# 'en'\n",
    "print(document.nwords)\n",
    "# 2\n",
    "\n",
    "pipe = pipeline.Pipeline(['CleanText', 'NWords'])\n",
    "print(pipe(sample_text))\n",
    "# {'CleanText': 'Sample text!', 'NWords': 2}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PYey5mrSFWrm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbCuMK-Pdm6F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NrGa8A7FdnO8"
   },
   "source": [
    "# KEY SOURCE CODE - spacy_pipe.pyx\n",
    "\n",
    "```python\n",
    "# cython: infer_types=True\n",
    "# cython: profile=True\n",
    "# coding: utf8\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "cimport numpy as np\n",
    "\n",
    "import numpy\n",
    "import srsly\n",
    "from collections import OrderedDict\n",
    "from thinc.api import chain\n",
    "from thinc.v2v import Affine, Maxout, Softmax\n",
    "from thinc.misc import LayerNorm\n",
    "from thinc.neural.util import to_categorical, copy_array\n",
    "\n",
    "from ..tokens.doc cimport Doc\n",
    "from ..syntax.nn_parser cimport Parser\n",
    "from ..syntax.ner cimport BiluoPushDown\n",
    "from ..syntax.arc_eager cimport ArcEager\n",
    "from ..morphology cimport Morphology\n",
    "from ..vocab cimport Vocab\n",
    "\n",
    "from ..syntax import nonproj\n",
    "from ..attrs import POS, ID\n",
    "from ..parts_of_speech import X\n",
    "from .._ml import Tok2Vec, build_tagger_model\n",
    "from .._ml import build_text_classifier, build_simple_cnn_text_classifier\n",
    "from .._ml import build_bow_text_classifier\n",
    "from .._ml import link_vectors_to_models, zero_init, flatten\n",
    "from .._ml import masked_language_model, create_default_optimizer\n",
    "from ..errors import Errors, TempErrors\n",
    "from .. import util\n",
    "\n",
    "\n",
    "def _load_cfg(path):\n",
    "    if path.exists():\n",
    "        return srsly.read_json(path)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "class Pipe(object):\n",
    "    \"\"\"This class is not instantiated directly. Components inherit from it, and\n",
    "    it defines the interface that components should follow to function as\n",
    "    components in a spaCy analysis pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    name = None\n",
    "\n",
    "    @classmethod\n",
    "    def Model(cls, *shape, **kwargs):\n",
    "        \"\"\"Initialize a model for the pipe.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __init__(self, vocab, model=True, **cfg):\n",
    "        \"\"\"Create a new pipe instance.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipe to one document. The document is\n",
    "        modified in-place, and returned.\n",
    "\n",
    "        Both __call__ and pipe should delegate to the `predict()`\n",
    "        and `set_annotations()` methods.\n",
    "        \"\"\"\n",
    "        self.require_model()\n",
    "        scores, tensors = self.predict([doc])\n",
    "        self.set_annotations([doc], scores, tensors=tensors)\n",
    "        return doc\n",
    "\n",
    "    def require_model(self):\n",
    "        \"\"\"Raise an error if the component's model is not initialized.\"\"\"\n",
    "        if getattr(self, \"model\", None) in (None, True, False):\n",
    "            raise ValueError(Errors.E109.format(name=self.name))\n",
    "\n",
    "    def pipe(self, stream, batch_size=128, n_threads=-1):\n",
    "        \"\"\"Apply the pipe to a stream of documents.\n",
    "\n",
    "        Both __call__ and pipe should delegate to the `predict()`\n",
    "        and `set_annotations()` methods.\n",
    "        \"\"\"\n",
    "        for docs in util.minibatch(stream, size=batch_size):\n",
    "            docs = list(docs)\n",
    "            scores, tensors = self.predict(docs)\n",
    "            self.set_annotations(docs, scores, tensor=tensors)\n",
    "            yield from docs\n",
    "\n",
    "    def predict(self, docs):\n",
    "        \"\"\"Apply the pipeline's model to a batch of docs, without\n",
    "        modifying them.\n",
    "        \"\"\"\n",
    "        self.require_model()\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def set_annotations(self, docs, scores, tensors=None):\n",
    "        \"\"\"Modify a batch of documents, using pre-computed scores.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, docs, golds, drop=0.0, sgd=None, losses=None):\n",
    "        \"\"\"Learn from a batch of documents and gold-standard information,\n",
    "        updating the pipe's model.\n",
    "\n",
    "        Delegates to predict() and get_loss().\n",
    "        \"\"\"\n",
    "        self.require_model()\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def rehearse(self, docs, sgd=None, losses=None, **config):\n",
    "        pass\n",
    "\n",
    "    def get_loss(self, docs, golds, scores):\n",
    "        \"\"\"Find the loss and gradient of loss for the batch of\n",
    "        documents and their predicted scores.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def add_label(self, label):\n",
    "        \"\"\"Add an output label, to be predicted by the model.\n",
    "\n",
    "        It's possible to extend pre-trained models with new labels,\n",
    "        but care should be taken to avoid the \"catastrophic forgetting\"\n",
    "        problem.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        return create_default_optimizer(self.model.ops, **self.cfg.get(\"optimizer\", {}))\n",
    "\n",
    "    def begin_training(\n",
    "        self, get_gold_tuples=lambda: [], pipeline=None, sgd=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize the pipe for training, using data exampes if available.\n",
    "        If no model has been initialized yet, the model is added.\"\"\"\n",
    "        if self.model is True:\n",
    "            self.model = self.Model(**self.cfg)\n",
    "        link_vectors_to_models(self.vocab)\n",
    "        if sgd is None:\n",
    "            sgd = self.create_optimizer()\n",
    "        return sgd\n",
    "\n",
    "    def use_params(self, params):\n",
    "        \"\"\"Modify the pipe's model, to use the given parameter values.\"\"\"\n",
    "        with self.model.use_params(params):\n",
    "            yield\n",
    "\n",
    "    def to_bytes(self, exclude=tuple(), **kwargs):\n",
    "        \"\"\"Serialize the pipe to a bytestring.\n",
    "\n",
    "        exclude (list): String names of serialization fields to exclude.\n",
    "        RETURNS (bytes): The serialized object.\n",
    "        \"\"\"\n",
    "        serialize = OrderedDict()\n",
    "        serialize[\"cfg\"] = lambda: srsly.json_dumps(self.cfg)\n",
    "        if self.model not in (True, False, None):\n",
    "            serialize[\"model\"] = self.model.to_bytes\n",
    "        serialize[\"vocab\"] = self.vocab.to_bytes\n",
    "        exclude = util.get_serialization_exclude(serialize, exclude, kwargs)\n",
    "        return util.to_bytes(serialize, exclude)\n",
    "\n",
    "    def from_bytes(self, bytes_data, exclude=tuple(), **kwargs):\n",
    "        \"\"\"Load the pipe from a bytestring.\"\"\"\n",
    "\n",
    "        def load_model(b):\n",
    "            # TODO: Remove this once we don't have to handle previous models\n",
    "            if self.cfg.get(\"pretrained_dims\") and \"pretrained_vectors\" not in self.cfg:\n",
    "                self.cfg[\"pretrained_vectors\"] = self.vocab.vectors.name\n",
    "            if self.model is True:\n",
    "                self.model = self.Model(**self.cfg)\n",
    "            self.model.from_bytes(b)\n",
    "\n",
    "        deserialize = OrderedDict()\n",
    "        deserialize[\"cfg\"] = lambda b: self.cfg.update(srsly.json_loads(b))\n",
    "        deserialize[\"vocab\"] = lambda b: self.vocab.from_bytes(b)\n",
    "        deserialize[\"model\"] = load_model\n",
    "        exclude = util.get_serialization_exclude(deserialize, exclude, kwargs)\n",
    "        util.from_bytes(bytes_data, deserialize, exclude)\n",
    "        return self\n",
    "\n",
    "    def to_disk(self, path, exclude=tuple(), **kwargs):\n",
    "        \"\"\"Serialize the pipe to disk.\"\"\"\n",
    "        serialize = OrderedDict()\n",
    "        serialize[\"cfg\"] = lambda p: srsly.write_json(p, self.cfg)\n",
    "        serialize[\"vocab\"] = lambda p: self.vocab.to_disk(p)\n",
    "        if self.model not in (None, True, False):\n",
    "            serialize[\"model\"] = lambda p: p.open(\"wb\").write(self.model.to_bytes())\n",
    "        exclude = util.get_serialization_exclude(serialize, exclude, kwargs)\n",
    "        util.to_disk(path, serialize, exclude)\n",
    "\n",
    "    def from_disk(self, path, exclude=tuple(), **kwargs):\n",
    "        \"\"\"Load the pipe from disk.\"\"\"\n",
    "\n",
    "        def load_model(p):\n",
    "            # TODO: Remove this once we don't have to handle previous models\n",
    "            if self.cfg.get(\"pretrained_dims\") and \"pretrained_vectors\" not in self.cfg:\n",
    "                self.cfg[\"pretrained_vectors\"] = self.vocab.vectors.name\n",
    "            if self.model is True:\n",
    "                self.model = self.Model(**self.cfg)\n",
    "            self.model.from_bytes(p.open(\"rb\").read())\n",
    "\n",
    "        deserialize = OrderedDict()\n",
    "        deserialize[\"cfg\"] = lambda p: self.cfg.update(_load_cfg(p))\n",
    "        deserialize[\"vocab\"] = lambda p: self.vocab.from_disk(p)\n",
    "        deserialize[\"model\"] = load_model\n",
    "        exclude = util.get_serialization_exclude(deserialize, exclude, kwargs)\n",
    "        util.from_disk(path, deserialize, exclude)\n",
    "        return self\n",
    "\n",
    "\n",
    "class Tensorizer(Pipe):\n",
    "    \"\"\"Pre-train position-sensitive vectors for tokens.\"\"\"\n",
    "\n",
    "    name = \"tensorizer\"\n",
    "\n",
    "    @classmethod\n",
    "    def Model(cls, output_size=300, **cfg):\n",
    "        \"\"\"Create a new statistical model for the class.\n",
    "\n",
    "        width (int): Output size of the model.\n",
    "        embed_size (int): Number of vectors in the embedding table.\n",
    "        **cfg: Config parameters.\n",
    "        RETURNS (Model): A `thinc.neural.Model` or similar instance.\n",
    "        \"\"\"\n",
    "        input_size = util.env_opt(\"token_vector_width\", cfg.get(\"input_size\", 96))\n",
    "        return zero_init(Affine(output_size, input_size, drop_factor=0.0))\n",
    "\n",
    "    def __init__(self, vocab, model=True, **cfg):\n",
    "        \"\"\"Construct a new statistical model. Weights are not allocated on\n",
    "        initialisation.\n",
    "\n",
    "        vocab (Vocab): A `Vocab` instance. The model must share the same\n",
    "            `Vocab` instance with the `Doc` objects it will process.\n",
    "        model (Model): A `Model` instance or `True` allocate one later.\n",
    "        **cfg: Config parameters.\n",
    "\n",
    "        EXAMPLE:\n",
    "            >>> from spacy.pipeline import TokenVectorEncoder\n",
    "            >>> tok2vec = TokenVectorEncoder(nlp.vocab)\n",
    "            >>> tok2vec.model = tok2vec.Model(128, 5000)\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        self.input_models = []\n",
    "        self.cfg = dict(cfg)\n",
    "        self.cfg.setdefault(\"cnn_maxout_pieces\", 3)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Add context-sensitive vectors to a `Doc`, e.g. from a CNN or LSTM\n",
    "        model. Vectors are set to the `Doc.tensor` attribute.\n",
    "\n",
    "        docs (Doc or iterable): One or more documents to add vectors to.\n",
    "        RETURNS (dict or None): Intermediate computations.\n",
    "        \"\"\"\n",
    "        tokvecses = self.predict([doc])\n",
    "        self.set_annotations([doc], tokvecses)\n",
    "        return doc\n",
    "\n",
    "    def pipe(self, stream, batch_size=128, n_threads=-1):\n",
    "        \"\"\"Process `Doc` objects as a stream.\n",
    "\n",
    "        stream (iterator): A sequence of `Doc` objects to process.\n",
    "        batch_size (int): Number of `Doc` objects to group.\n",
    "        YIELDS (iterator): A sequence of `Doc` objects, in order of input.\n",
    "        \"\"\"\n",
    "        for docs in util.minibatch(stream, size=batch_size):\n",
    "            docs = list(docs)\n",
    "            tensors = self.predict(docs)\n",
    "            self.set_annotations(docs, tensors)\n",
    "            yield from docs\n",
    "\n",
    "    def predict(self, docs):\n",
    "        \"\"\"Return a single tensor for a batch of documents.\n",
    "\n",
    "        docs (iterable): A sequence of `Doc` objects.\n",
    "        RETURNS (object): Vector representations for each token in the docs.\n",
    "        \"\"\"\n",
    "        self.require_model()\n",
    "        inputs = self.model.ops.flatten([doc.tensor for doc in docs])\n",
    "        outputs = self.model(inputs)\n",
    "        return self.model.ops.unflatten(outputs, [len(d) for d in docs])\n",
    "\n",
    "    def set_annotations(self, docs, tensors):\n",
    "        \"\"\"Set the tensor attribute for a batch of documents.\n",
    "\n",
    "        docs (iterable): A sequence of `Doc` objects.\n",
    "        tensors (object): Vector representation for each token in the docs.\n",
    "        \"\"\"\n",
    "        for doc, tensor in zip(docs, tensors):\n",
    "            if tensor.shape[0] != len(doc):\n",
    "                raise ValueError(Errors.E076.format(rows=tensor.shape[0], words=len(doc)))\n",
    "            doc.tensor = tensor\n",
    "\n",
    "    def update(self, docs, golds, state=None, drop=0.0, sgd=None, losses=None):\n",
    "        \"\"\"Update the model.\n",
    "\n",
    "        docs (iterable): A batch of `Doc` objects.\n",
    "        golds (iterable): A batch of `GoldParse` objects.\n",
    "        drop (float): The droput rate.\n",
    "        sgd (callable): An optimizer.\n",
    "        RETURNS (dict): Results from the update.\n",
    "        \"\"\"\n",
    "        self.require_model()\n",
    "        if isinstance(docs, Doc):\n",
    "            docs = [docs]\n",
    "        inputs = []\n",
    "        bp_inputs = []\n",
    "        for tok2vec in self.input_models:\n",
    "            tensor, bp_tensor = tok2vec.begin_update(docs, drop=drop)\n",
    "            inputs.append(tensor)\n",
    "            bp_inputs.append(bp_tensor)\n",
    "        inputs = self.model.ops.xp.hstack(inputs)\n",
    "        scores, bp_scores = self.model.begin_update(inputs, drop=drop)\n",
    "        loss, d_scores = self.get_loss(docs, golds, scores)\n",
    "        d_inputs = bp_scores(d_scores, sgd=sgd)\n",
    "        d_inputs = self.model.ops.xp.split(d_inputs, len(self.input_models), axis=1)\n",
    "        for d_input, bp_input in zip(d_inputs, bp_inputs):\n",
    "            bp_input(d_input, sgd=sgd)\n",
    "        if losses is not None:\n",
    "            losses.setdefault(self.name, 0.0)\n",
    "            losses[self.name] += loss\n",
    "        return loss\n",
    "\n",
    "    def get_loss(self, docs, golds, prediction):\n",
    "        ids = self.model.ops.flatten([doc.to_array(ID).ravel() for doc in docs])\n",
    "        target = self.vocab.vectors.data[ids]\n",
    "        d_scores = (prediction - target) / prediction.shape[0]\n",
    "        loss = (d_scores ** 2).sum()\n",
    "        return loss, d_scores\n",
    "\n",
    "    def begin_training(self, gold_tuples=lambda: [], pipeline=None, sgd=None, **kwargs):\n",
    "        \"\"\"Allocate models, pre-process training data and acquire an\n",
    "        optimizer.\n",
    "\n",
    "        gold_tuples (iterable): Gold-standard training data.\n",
    "        pipeline (list): The pipeline the model is part of.\n",
    "        \"\"\"\n",
    "        if pipeline is not None:\n",
    "            for name, model in pipeline:\n",
    "                if getattr(model, \"tok2vec\", None):\n",
    "                    self.input_models.append(model.tok2vec)\n",
    "        if self.model is True:\n",
    "            self.model = self.Model(**self.cfg)\n",
    "        link_vectors_to_models(self.vocab)\n",
    "        if sgd is None:\n",
    "            sgd = self.create_optimizer()\n",
    "        return sgd\n",
    "\n",
    "\n",
    "class Tagger(Pipe):\n",
    "    \"\"\"Pipeline component for part-of-speech tagging.\n",
    "\n",
    "    DOCS: https://spacy.io/api/tagger\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"tagger\"\n",
    "\n",
    "    def __init__(self, vocab, model=True, **cfg):\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        self._rehearsal_model = None\n",
    "        self.cfg = OrderedDict(sorted(cfg.items()))\n",
    "        self.cfg.setdefault(\"cnn_maxout_pieces\", 2)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return tuple(self.vocab.morphology.tag_names)\n",
    "\n",
    "    @property\n",
    "    def tok2vec(self):\n",
    "        if self.model in (None, True, False):\n",
    "            return None\n",
    "        else:\n",
    "            return chain(self.model.tok2vec, flatten)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        tags, tokvecs = self.predict([doc])\n",
    "        self.set_annotations([doc], tags, tensors=tokvecs)\n",
    "        return doc\n",
    "\n",
    "    def pipe(self, stream, batch_size=128, n_threads=-1):\n",
    "        for docs in util.minibatch(stream, size=batch_size):\n",
    "            docs = list(docs)\n",
    "            tag_ids, tokvecs = self.predict(docs)\n",
    "            self.set_annotations(docs, tag_ids, tensors=tokvecs)\n",
    "            yield from docs\n",
    "\n",
    "    def predict(self, docs):\n",
    "        self.require_model()\n",
    "        if not any(len(doc) for doc in docs):\n",
    "            # Handle case where there are no tokens in any docs.\n",
    "            n_labels = len(self.labels)\n",
    "            guesses = [self.model.ops.allocate((0, n_labels)) for doc in docs]\n",
    "            tokvecs = self.model.ops.allocate((0, self.model.tok2vec.nO))\n",
    "            return guesses, tokvecs\n",
    "        tokvecs = self.model.tok2vec(docs)\n",
    "        scores = self.model.softmax(tokvecs)\n",
    "        guesses = []\n",
    "        for doc_scores in scores:\n",
    "            doc_guesses = doc_scores.argmax(axis=1)\n",
    "            if not isinstance(doc_guesses, numpy.ndarray):\n",
    "                doc_guesses = doc_guesses.get()\n",
    "            guesses.append(doc_guesses)\n",
    "        return guesses, tokvecs\n",
    "\n",
    "    def set_annotations(self, docs, batch_tag_ids, tensors=None):\n",
    "        if isinstance(docs, Doc):\n",
    "            docs = [docs]\n",
    "        cdef Doc doc\n",
    "        cdef int idx = 0\n",
    "        cdef Vocab vocab = self.vocab\n",
    "        for i, doc in enumerate(docs):\n",
    "            doc_tag_ids = batch_tag_ids[i]\n",
    "            if hasattr(doc_tag_ids, \"get\"):\n",
    "                doc_tag_ids = doc_tag_ids.get()\n",
    "            for j, tag_id in enumerate(doc_tag_ids):\n",
    "                # Don't clobber preset POS tags\n",
    "                if doc.c[j].tag == 0 and doc.c[j].pos == 0:\n",
    "                    # Don't clobber preset lemmas\n",
    "                    lemma = doc.c[j].lemma\n",
    "                    vocab.morphology.assign_tag_id(&doc.c[j], tag_id)\n",
    "                    if lemma != 0 and lemma != doc.c[j].lex.orth:\n",
    "                        doc.c[j].lemma = lemma\n",
    "                idx += 1\n",
    "            if tensors is not None and len(tensors):\n",
    "                if isinstance(doc.tensor, numpy.ndarray) \\\n",
    "                and not isinstance(tensors[i], numpy.ndarray):\n",
    "                    doc.extend_tensor(tensors[i].get())\n",
    "                else:\n",
    "                    doc.extend_tensor(tensors[i])\n",
    "            doc.is_tagged = True\n",
    "\n",
    "    def update(self, docs, golds, drop=0., sgd=None, losses=None):\n",
    "        self.require_model()\n",
    "        if losses is not None and self.name not in losses:\n",
    "            losses[self.name] = 0.\n",
    "\n",
    "        tag_scores, bp_tag_scores = self.model.begin_update(docs, drop=drop)\n",
    "        loss, d_tag_scores = self.get_loss(docs, golds, tag_scores)\n",
    "        bp_tag_scores(d_tag_scores, sgd=sgd)\n",
    "\n",
    "        if losses is not None:\n",
    "            losses[self.name] += loss\n",
    "\n",
    "    def rehearse(self, docs, drop=0., sgd=None, losses=None):\n",
    "        \"\"\"Perform a 'rehearsal' update, where we try to match the output of\n",
    "        an initial model.\n",
    "        \"\"\"\n",
    "        if self._rehearsal_model is None:\n",
    "            return\n",
    "        guesses, backprop = self.model.begin_update(docs, drop=drop)\n",
    "        target = self._rehearsal_model(docs)\n",
    "        gradient = guesses - target\n",
    "        backprop(gradient, sgd=sgd)\n",
    "        if losses is not None:\n",
    "            losses.setdefault(self.name, 0.0)\n",
    "            losses[self.name] += (gradient**2).sum()\n",
    "\n",
    "    def get_loss(self, docs, golds, scores):\n",
    "        scores = self.model.ops.flatten(scores)\n",
    "        tag_index = {tag: i for i, tag in enumerate(self.labels)}\n",
    "        cdef int idx = 0\n",
    "        correct = numpy.zeros((scores.shape[0],), dtype=\"i\")\n",
    "        guesses = scores.argmax(axis=1)\n",
    "        known_labels = numpy.ones((scores.shape[0], 1), dtype=\"f\")\n",
    "        for gold in golds:\n",
    "            for tag in gold.tags:\n",
    "                if tag is None:\n",
    "                    correct[idx] = guesses[idx]\n",
    "                elif tag in tag_index:\n",
    "                    correct[idx] = tag_index[tag]\n",
    "                else:\n",
    "                    correct[idx] = 0\n",
    "                    known_labels[idx] = 0.\n",
    "                idx += 1\n",
    "        correct = self.model.ops.xp.array(correct, dtype=\"i\")\n",
    "        d_scores = scores - to_categorical(correct, nb_classes=scores.shape[1])\n",
    "        d_scores *= self.model.ops.asarray(known_labels)\n",
    "        loss = (d_scores**2).sum()\n",
    "        d_scores = self.model.ops.unflatten(d_scores, [len(d) for d in docs])\n",
    "        return float(loss), d_scores\n",
    "\n",
    "    def begin_training(self, get_gold_tuples=lambda: [], pipeline=None, sgd=None,\n",
    "                       **kwargs):\n",
    "        orig_tag_map = dict(self.vocab.morphology.tag_map)\n",
    "        new_tag_map = OrderedDict()\n",
    "        for raw_text, annots_brackets in get_gold_tuples():\n",
    "            for annots, brackets in annots_brackets:\n",
    "                ids, words, tags, heads, deps, ents = annots\n",
    "                for tag in tags:\n",
    "                    if tag in orig_tag_map:\n",
    "                        new_tag_map[tag] = orig_tag_map[tag]\n",
    "                    else:\n",
    "                        new_tag_map[tag] = {POS: X}\n",
    "        cdef Vocab vocab = self.vocab\n",
    "        if new_tag_map:\n",
    "            vocab.morphology = Morphology(vocab.strings, new_tag_map,\n",
    "                                          vocab.morphology.lemmatizer,\n",
    "                                          exc=vocab.morphology.exc)\n",
    "        self.cfg[\"pretrained_vectors\"] = kwargs.get(\"pretrained_vectors\")\n",
    "        if self.model is True:\n",
    "            for hp in [\"token_vector_width\", \"conv_depth\"]:\n",
    "                if hp in kwargs:\n",
    "                    self.cfg[hp] = kwargs[hp]\n",
    "            self.model = self.Model(self.vocab.morphology.n_tags, **self.cfg)\n",
    "        link_vectors_to_models(self.vocab)\n",
    "        if sgd is None:\n",
    "            sgd = self.create_optimizer()\n",
    "        return sgd\n",
    "\n",
    "    @classmethod\n",
    "    def Model(cls, n_tags, **cfg):\n",
    "        if cfg.get(\"pretrained_dims\") and not cfg.get(\"pretrained_vectors\"):\n",
    "            raise ValueError(TempErrors.T008)\n",
    "        return build_tagger_model(n_tags, **cfg)\n",
    "\n",
    "    def add_label(self, label, values=None):\n",
    "        if label in self.labels:\n",
    "            return 0\n",
    "        if self.model not in (True, False, None):\n",
    "            # Here's how the model resizing will work, once the\n",
    "            # neuron-to-tag mapping is no longer controlled by\n",
    "            # the Morphology class, which sorts the tag names.\n",
    "            # The sorting makes adding labels difficult.\n",
    "            # smaller = self.model._layers[-1]\n",
    "            # larger = Softmax(len(self.labels)+1, smaller.nI)\n",
    "            # copy_array(larger.W[:smaller.nO], smaller.W)\n",
    "            # copy_array(larger.b[:smaller.nO], smaller.b)\n",
    "            # self.model._layers[-1] = larger\n",
    "            raise ValueError(TempErrors.T003)\n",
    "        tag_map = dict(self.vocab.morphology.tag_map)\n",
    "        if values is None:\n",
    "            values = {POS: \"X\"}\n",
    "        tag_map[label] = values\n",
    "        self.vocab.morphology = Morphology(\n",
    "            self.vocab.strings, tag_map=tag_map,\n",
    "            lemmatizer=self.vocab.morphology.lemmatizer,\n",
    "            exc=self.vocab.morphology.exc)\n",
    "        return 1\n",
    "\n",
    "    def use_params(self, params):\n",
    "        with self.model.use_params(params):\n",
    "            yield\n",
    "\n",
    "    def to_bytes(self, exclude=tuple(), **kwargs):\n",
    "        serialize = OrderedDict()\n",
    "        if self.model not in (None, True, False):\n",
    "            serialize[\"model\"] = self.model.to_bytes\n",
    "        serialize[\"vocab\"] = self.vocab.to_bytes\n",
    "        serialize[\"cfg\"] = lambda: srsly.json_dumps(self.cfg)\n",
    "        tag_map = OrderedDict(sorted(self.vocab.morphology.tag_map.items()))\n",
    "        serialize[\"tag_map\"] = lambda: srsly.msgpack_dumps(tag_map)\n",
    "        exclude = util.get_serialization_exclude(serialize, exclude, kwargs)\n",
    "        return util.to_bytes(serialize, exclude)\n",
    "\n",
    "    def from_bytes(self, bytes_data, exclude=tuple(), **kwargs):\n",
    "        def load_model(b):\n",
    "            # TODO: Remove this once we don't have to handle previous models\n",
    "            if self.cfg.get(\"pretrained_dims\") and \"pretrained_vectors\" not in self.cfg:\n",
    "                self.cfg[\"pretrained_vectors\"] = self.vocab.vectors.name\n",
    "            if self.model is True:\n",
    "                token_vector_width = util.env_opt(\n",
    "                    \"token_vector_width\",\n",
    "                    self.cfg.get(\"token_vector_width\", 96))\n",
    "                self.model = self.Model(self.vocab.morphology.n_tags, **self.cfg)\n",
    "            self.model.from_bytes(b)\n",
    "\n",
    "        def load_tag_map(b):\n",
    "            tag_map = srsly.msgpack_loads(b)\n",
    "            self.vocab.morphology = Morphology(\n",
    "                self.vocab.strings, tag_map=tag_map,\n",
    "                lemmatizer=self.vocab.morphology.lemmatizer,\n",
    "                exc=self.vocab.morphology.exc)\n",
    "\n",
    "        deserialize = OrderedDict((\n",
    "            (\"vocab\", lambda b: self.vocab.from_bytes(b)),\n",
    "            (\"tag_map\", load_tag_map),\n",
    "            (\"cfg\", lambda b: self.cfg.update(srsly.json_loads(b))),\n",
    "            (\"model\", lambda b: load_model(b)),\n",
    "        ))\n",
    "        exclude = util.get_serialization_exclude(deserialize, exclude, kwargs)\n",
    "        util.from_bytes(bytes_data, deserialize, exclude)\n",
    "        return self\n",
    "\n",
    "    def to_disk(self, path, exclude=tuple(), **kwargs):\n",
    "        tag_map = OrderedDict(sorted(self.vocab.morphology.tag_map.items()))\n",
    "        serialize = OrderedDict((\n",
    "            (\"vocab\", lambda p: self.vocab.to_disk(p)),\n",
    "            (\"tag_map\", lambda p: srsly.write_msgpack(p, tag_map)),\n",
    "            (\"model\", lambda p: p.open(\"wb\").write(self.model.to_bytes())),\n",
    "            (\"cfg\", lambda p: srsly.write_json(p, self.cfg))\n",
    "        ))\n",
    "        exclude = util.get_serialization_exclude(serialize, exclude, kwargs)\n",
    "        util.to_disk(path, serialize, exclude)\n",
    "\n",
    "    def from_disk(self, path, exclude=tuple(), **kwargs):\n",
    "        def load_model(p):\n",
    "            # TODO: Remove this once we don't have to handle previous models\n",
    "            if self.cfg.get(\"pretrained_dims\") and \"pretrained_vectors\" not in self.cfg:\n",
    "                self.cfg[\"pretrained_vectors\"] = self.vocab.vectors.name\n",
    "            if self.model is True:\n",
    "                self.model = self.Model(self.vocab.morphology.n_tags, **self.cfg)\n",
    "            with p.open(\"rb\") as file_:\n",
    "                self.model.from_bytes(file_.read())\n",
    "\n",
    "        def load_tag_map(p):\n",
    "            tag_map = srsly.read_msgpack(p)\n",
    "            self.vocab.morphology = Morphology(\n",
    "                self.vocab.strings, tag_map=tag_map,\n",
    "                lemmatizer=self.vocab.morphology.lemmatizer,\n",
    "                exc=self.vocab.morphology.exc)\n",
    "\n",
    "        deserialize = OrderedDict((\n",
    "            (\"cfg\", lambda p: self.cfg.update(_load_cfg(p))),\n",
    "            (\"vocab\", lambda p: self.vocab.from_disk(p)),\n",
    "            (\"tag_map\", load_tag_map),\n",
    "            (\"model\", load_model),\n",
    "        ))\n",
    "        exclude = util.get_serialization_exclude(deserialize, exclude, kwargs)\n",
    "        util.from_disk(path, deserialize, exclude)\n",
    "        return self\n",
    "\n",
    "\n",
    "class MultitaskObjective(Tagger):\n",
    "    \"\"\"Experimental: Assist training of a parser or tagger, by training a\n",
    "    side-objective.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"nn_labeller\"\n",
    "\n",
    "    def __init__(self, vocab, model=True, target='dep_tag_offset', **cfg):\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        if target == \"dep\":\n",
    "            self.make_label = self.make_dep\n",
    "        elif target == \"tag\":\n",
    "            self.make_label = self.make_tag\n",
    "        elif target == \"ent\":\n",
    "            self.make_label = self.make_ent\n",
    "        elif target == \"dep_tag_offset\":\n",
    "            self.make_label = self.make_dep_tag_offset\n",
    "        elif target == \"ent_tag\":\n",
    "            self.make_label = self.make_ent_tag\n",
    "        elif target == \"sent_start\":\n",
    "            self.make_label = self.make_sent_start\n",
    "        elif hasattr(target, \"__call__\"):\n",
    "            self.make_label = target\n",
    "        else:\n",
    "            raise ValueError(Errors.E016)\n",
    "        self.cfg = dict(cfg)\n",
    "        self.cfg.setdefault(\"cnn_maxout_pieces\", 2)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self.cfg.setdefault(\"labels\", {})\n",
    "\n",
    "    @labels.setter\n",
    "    def labels(self, value):\n",
    "        self.cfg[\"labels\"] = value\n",
    "\n",
    "    def set_annotations(self, docs, dep_ids, tensors=None):\n",
    "        pass\n",
    "\n",
    "    def begin_training(self, get_gold_tuples=lambda: [], pipeline=None, tok2vec=None,\n",
    "                       sgd=None, **kwargs):\n",
    "        gold_tuples = nonproj.preprocess_training_data(get_gold_tuples())\n",
    "        for raw_text, annots_brackets in gold_tuples:\n",
    "            for annots, brackets in annots_brackets:\n",
    "                ids, words, tags, heads, deps, ents = annots\n",
    "                for i in range(len(ids)):\n",
    "                    label = self.make_label(i, words, tags, heads, deps, ents)\n",
    "                    if label is not None and label not in self.labels:\n",
    "                        self.labels[label] = len(self.labels)\n",
    "        if self.model is True:\n",
    "            token_vector_width = util.env_opt(\"token_vector_width\")\n",
    "            self.model = self.Model(len(self.labels), tok2vec=tok2vec)\n",
    "        link_vectors_to_models(self.vocab)\n",
    "        if sgd is None:\n",
    "            sgd = self.create_optimizer()\n",
    "        return sgd\n",
    "\n",
    "    @classmethod\n",
    "    def Model(cls, n_tags, tok2vec=None, **cfg):\n",
    "        token_vector_width = util.env_opt(\"token_vector_width\", 96)\n",
    "        softmax = Softmax(n_tags, token_vector_width*2)\n",
    "        model = chain(\n",
    "            tok2vec,\n",
    "            LayerNorm(Maxout(token_vector_width*2, token_vector_width, pieces=3)),\n",
    "            softmax\n",
    "        )\n",
    "        model.tok2vec = tok2vec\n",
    "        model.softmax = softmax\n",
    "        return model\n",
    "\n",
    "    def predict(self, docs):\n",
    "        self.require_model()\n",
    "        tokvecs = self.model.tok2vec(docs)\n",
    "        scores = self.model.softmax(tokvecs)\n",
    "        return tokvecs, scores\n",
    "\n",
    "    def get_loss(self, docs, golds, scores):\n",
    "        if len(docs) != len(golds):\n",
    "            raise ValueError(Errors.E077.format(value=\"loss\", n_docs=len(docs),\n",
    "                                                n_golds=len(golds)))\n",
    "        cdef int idx = 0\n",
    "        correct = numpy.zeros((scores.shape[0],), dtype=\"i\")\n",
    "        guesses = scores.argmax(axis=1)\n",
    "        for i, gold in enumerate(golds):\n",
    "            for j in range(len(docs[i])):\n",
    "                # Handes alignment for tokenization differences\n",
    "                label = self.make_label(j, gold.words, gold.tags,\n",
    "                                        gold.heads, gold.labels, gold.ents)\n",
    "                if label is None or label not in self.labels:\n",
    "                    correct[idx] = guesses[idx]\n",
    "                else:\n",
    "                    correct[idx] = self.labels[label]\n",
    "                idx += 1\n",
    "        correct = self.model.ops.xp.array(correct, dtype=\"i\")\n",
    "        d_scores = scores - to_categorical(correct, nb_classes=scores.shape[1])\n",
    "        loss = (d_scores**2).sum()\n",
    "        return float(loss), d_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dep(i, words, tags, heads, deps, ents):\n",
    "        if deps[i] is None or heads[i] is None:\n",
    "            return None\n",
    "        return deps[i]\n",
    "\n",
    "    @staticmethod\n",
    "    def make_tag(i, words, tags, heads, deps, ents):\n",
    "        return tags[i]\n",
    "\n",
    "    @staticmethod\n",
    "    def make_ent(i, words, tags, heads, deps, ents):\n",
    "        if ents is None:\n",
    "            return None\n",
    "        return ents[i]\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dep_tag_offset(i, words, tags, heads, deps, ents):\n",
    "        if deps[i] is None or heads[i] is None:\n",
    "            return None\n",
    "        offset = heads[i] - i\n",
    "        offset = min(offset, 2)\n",
    "        offset = max(offset, -2)\n",
    "        return \"%s-%s:%d\" % (deps[i], tags[i], offset)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_ent_tag(i, words, tags, heads, deps, ents):\n",
    "        if ents is None or ents[i] is None:\n",
    "            return None\n",
    "        else:\n",
    "            return \"%s-%s\" % (tags[i], ents[i])\n",
    "\n",
    "    @staticmethod\n",
    "    def make_sent_start(target, words, tags, heads, deps, ents, cache=True, _cache={}):\n",
    "        \"\"\"A multi-task objective for representing sentence boundaries,\n",
    "        using BILU scheme. (O is impossible)\n",
    "\n",
    "        The implementation of this method uses an internal cache that relies\n",
    "        on the identity of the heads array, to avoid requiring a new piece\n",
    "        of gold data. You can pass cache=False if you know the cache will\n",
    "        do the wrong thing.\n",
    "        \"\"\"\n",
    "        assert len(words) == len(heads)\n",
    "        assert target < len(words), (target, len(words))\n",
    "        if cache:\n",
    "            if id(heads) in _cache:\n",
    "                return _cache[id(heads)][target]\n",
    "            else:\n",
    "                for key in list(_cache.keys()):\n",
    "                    _cache.pop(key)\n",
    "            sent_tags = [\"I-SENT\"] * len(words)\n",
    "            _cache[id(heads)] = sent_tags\n",
    "        else:\n",
    "            sent_tags = [\"I-SENT\"] * len(words)\n",
    "\n",
    "        def _find_root(child):\n",
    "            seen = set([child])\n",
    "            while child is not None and heads[child] != child:\n",
    "                seen.add(child)\n",
    "                child = heads[child]\n",
    "            return child\n",
    "\n",
    "        sentences = {}\n",
    "        for i in range(len(words)):\n",
    "            root = _find_root(i)\n",
    "            if root is None:\n",
    "                sent_tags[i] = None\n",
    "            else:\n",
    "                sentences.setdefault(root, []).append(i)\n",
    "        for root, span in sorted(sentences.items()):\n",
    "            if len(span) == 1:\n",
    "                sent_tags[span[0]] = \"U-SENT\"\n",
    "            else:\n",
    "                sent_tags[span[0]] = \"B-SENT\"\n",
    "                sent_tags[span[-1]] = \"L-SENT\"\n",
    "        return sent_tags[target]\n",
    "\n",
    "\n",
    "class ClozeMultitask(Pipe):\n",
    "    @classmethod\n",
    "    def Model(cls, vocab, tok2vec, **cfg):\n",
    "        output_size = vocab.vectors.data.shape[1]\n",
    "        output_layer = chain(\n",
    "            LayerNorm(Maxout(output_size, tok2vec.nO, pieces=3)),\n",
    "            zero_init(Affine(output_size, output_size, drop_factor=0.0))\n",
    "        )\n",
    "        model = chain(tok2vec, output_layer)\n",
    "        model = masked_language_model(vocab, model)\n",
    "        model.tok2vec = tok2vec\n",
    "        model.output_layer = output_layer\n",
    "        return model\n",
    "\n",
    "    def __init__(self, vocab, model=True, **cfg):\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def set_annotations(self, docs, dep_ids, tensors=None):\n",
    "        pass\n",
    "\n",
    "    def begin_training(self, get_gold_tuples=lambda: [], pipeline=None,\n",
    "                        tok2vec=None, sgd=None, **kwargs):\n",
    "        link_vectors_to_models(self.vocab)\n",
    "        if self.model is True:\n",
    "            self.model = self.Model(self.vocab, tok2vec)\n",
    "        X = self.model.ops.allocate((5, self.model.tok2vec.nO))\n",
    "        self.model.output_layer.begin_training(X)\n",
    "        if sgd is None:\n",
    "            sgd = self.create_optimizer()\n",
    "        return sgd\n",
    "\n",
    "    def predict(self, docs):\n",
    "        self.require_model()\n",
    "        tokvecs = self.model.tok2vec(docs)\n",
    "        vectors = self.model.output_layer(tokvecs)\n",
    "        return tokvecs, vectors\n",
    "\n",
    "    def get_loss(self, docs, vectors, prediction):\n",
    "        # The simplest way to implement this would be to vstack the\n",
    "        # token.vector values, but that's a bit inefficient, especially on GPU.\n",
    "        # Instead we fetch the index into the vectors table for each of our tokens,\n",
    "        # and look them up all at once. This prevents data copying.\n",
    "        ids = self.model.ops.flatten([doc.to_array(ID).ravel() for doc in docs])\n",
    "        target = vectors[ids]\n",
    "        gradient = (prediction - target) / prediction.shape[0]\n",
    "        loss = (gradient**2).sum()\n",
    "        return float(loss), gradient\n",
    "\n",
    "    def update(self, docs, golds, drop=0., sgd=None, losses=None):\n",
    "        pass\n",
    "\n",
    "    def rehearse(self, docs, drop=0., sgd=None, losses=None):\n",
    "        self.require_model()\n",
    "        if losses is not None and self.name not in losses:\n",
    "            losses[self.name] = 0.\n",
    "        predictions, bp_predictions = self.model.begin_update(docs, drop=drop)\n",
    "        loss, d_predictions = self.get_loss(docs, self.vocab.vectors.data, predictions)\n",
    "        bp_predictions(d_predictions, sgd=sgd)\n",
    "\n",
    "        if losses is not None:\n",
    "            losses[self.name] += loss\n",
    "\n",
    "\n",
    "class TextCategorizer(Pipe):\n",
    "    \"\"\"Pipeline component for text classification.\n",
    "\n",
    "    DOCS: https://spacy.io/api/textcategorizer\n",
    "    \"\"\"\n",
    "    name = 'textcat'\n",
    "\n",
    "    @classmethod\n",
    "    def Model(cls, nr_class=1, **cfg):\n",
    "        embed_size = util.env_opt(\"embed_size\", 2000)\n",
    "        if \"token_vector_width\" in cfg:\n",
    "            token_vector_width = cfg[\"token_vector_width\"]\n",
    "        else:\n",
    "            token_vector_width = util.env_opt(\"token_vector_width\", 96)\n",
    "        if cfg.get(\"architecture\") == \"simple_cnn\":\n",
    "            tok2vec = Tok2Vec(token_vector_width, embed_size, **cfg)\n",
    "            return build_simple_cnn_text_classifier(tok2vec, nr_class, **cfg)\n",
    "        elif cfg.get(\"architecture\") == \"bow\":\n",
    "            return build_bow_text_classifier(nr_class, **cfg)\n",
    "        else:\n",
    "            return build_text_classifier(nr_class, **cfg)\n",
    "\n",
    "    @property\n",
    "    def tok2vec(self):\n",
    "        if self.model in (None, True, False):\n",
    "            return None\n",
    "        else:\n",
    "            return self.model.tok2vec\n",
    "\n",
    "    def __init__(self, vocab, model=True, **cfg):\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        self._rehearsal_model = None\n",
    "        self.cfg = dict(cfg)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return tuple(self.cfg.setdefault(\"labels\", []))\n",
    "\n",
    "    @labels.setter\n",
    "    def labels(self, value):\n",
    "        self.cfg[\"labels\"] = tuple(value)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        scores, tensors = self.predict([doc])\n",
    "        self.set_annotations([doc], scores, tensors=tensors)\n",
    "        return doc\n",
    "\n",
    "    def pipe(self, stream, batch_size=128, n_threads=-1):\n",
    "        for docs in util.minibatch(stream, size=batch_size):\n",
    "            docs = list(docs)\n",
    "            scores, tensors = self.predict(docs)\n",
    "            self.set_annotations(docs, scores, tensors=tensors)\n",
    "            yield from docs\n",
    "\n",
    "    def predict(self, docs):\n",
    "        self.require_model()\n",
    "        scores = self.model(docs)\n",
    "        scores = self.model.ops.asarray(scores)\n",
    "        tensors = [doc.tensor for doc in docs]\n",
    "        return scores, tensors\n",
    "\n",
    "    def set_annotations(self, docs, scores, tensors=None):\n",
    "        for i, doc in enumerate(docs):\n",
    "            for j, label in enumerate(self.labels):\n",
    "                doc.cats[label] = float(scores[i, j])\n",
    "\n",
    "    def update(self, docs, golds, state=None, drop=0., sgd=None, losses=None):\n",
    "        scores, bp_scores = self.model.begin_update(docs, drop=drop)\n",
    "        loss, d_scores = self.get_loss(docs, golds, scores)\n",
    "        bp_scores(d_scores, sgd=sgd)\n",
    "        if losses is not None:\n",
    "            losses.setdefault(self.name, 0.0)\n",
    "            losses[self.name] += loss\n",
    "\n",
    "    def rehearse(self, docs, drop=0., sgd=None, losses=None):\n",
    "        if self._rehearsal_model is None:\n",
    "            return\n",
    "        scores, bp_scores = self.model.begin_update(docs, drop=drop)\n",
    "        target = self._rehearsal_model(docs)\n",
    "        gradient = scores - target\n",
    "        bp_scores(gradient, sgd=sgd)\n",
    "        if losses is not None:\n",
    "            losses.setdefault(self.name, 0.0)\n",
    "            losses[self.name] += (gradient**2).sum()\n",
    "\n",
    "    def get_loss(self, docs, golds, scores):\n",
    "        truths = numpy.zeros((len(golds), len(self.labels)), dtype=\"f\")\n",
    "        not_missing = numpy.ones((len(golds), len(self.labels)), dtype=\"f\")\n",
    "        for i, gold in enumerate(golds):\n",
    "            for j, label in enumerate(self.labels):\n",
    "                if label in gold.cats:\n",
    "                    truths[i, j] = gold.cats[label]\n",
    "                else:\n",
    "                    not_missing[i, j] = 0.\n",
    "        truths = self.model.ops.asarray(truths)\n",
    "        not_missing = self.model.ops.asarray(not_missing)\n",
    "        d_scores = (scores-truths) / scores.shape[0]\n",
    "        d_scores *= not_missing\n",
    "        mean_square_error = (d_scores**2).sum(axis=1).mean()\n",
    "        return float(mean_square_error), d_scores\n",
    "\n",
    "    def add_label(self, label):\n",
    "        if label in self.labels:\n",
    "            return 0\n",
    "        if self.model not in (None, True, False):\n",
    "            # This functionality was available previously, but was broken.\n",
    "            # The problem is that we resize the last layer, but the last layer\n",
    "            # is actually just an ensemble. We're not resizing the child layers\n",
    "            # - a huge problem.\n",
    "            raise ValueError(Errors.E116)\n",
    "            # smaller = self.model._layers[-1]\n",
    "            # larger = Affine(len(self.labels)+1, smaller.nI)\n",
    "            # copy_array(larger.W[:smaller.nO], smaller.W)\n",
    "            # copy_array(larger.b[:smaller.nO], smaller.b)\n",
    "            # self.model._layers[-1] = larger\n",
    "        self.labels = tuple(list(self.labels) + [label])\n",
    "        return 1\n",
    "\n",
    "    def begin_training(self, get_gold_tuples=lambda: [], pipeline=None, sgd=None, **kwargs):\n",
    "        if self.model is True:\n",
    "            self.cfg[\"pretrained_vectors\"] = kwargs.get(\"pretrained_vectors\")\n",
    "            self.model = self.Model(len(self.labels), **self.cfg)\n",
    "            link_vectors_to_models(self.vocab)\n",
    "        if sgd is None:\n",
    "            sgd = self.create_optimizer()\n",
    "        return sgd\n",
    "\n",
    "\n",
    "cdef class DependencyParser(Parser):\n",
    "    \"\"\"Pipeline component for dependency parsing.\n",
    "\n",
    "    DOCS: https://spacy.io/api/dependencyparser\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"parser\"\n",
    "    TransitionSystem = ArcEager\n",
    "\n",
    "    @property\n",
    "    def postprocesses(self):\n",
    "        return [nonproj.deprojectivize]\n",
    "\n",
    "    def add_multitask_objective(self, target):\n",
    "        if target == \"cloze\":\n",
    "            cloze = ClozeMultitask(self.vocab)\n",
    "            self._multitasks.append(cloze)\n",
    "        else:\n",
    "            labeller = MultitaskObjective(self.vocab, target=target)\n",
    "            self._multitasks.append(labeller)\n",
    "\n",
    "    def init_multitask_objectives(self, get_gold_tuples, pipeline, sgd=None, **cfg):\n",
    "        for labeller in self._multitasks:\n",
    "            tok2vec = self.model.tok2vec\n",
    "            labeller.begin_training(get_gold_tuples, pipeline=pipeline,\n",
    "                                    tok2vec=tok2vec, sgd=sgd)\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return (DependencyParser, (self.vocab, self.moves, self.model), None, None)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        # Get the labels from the model by looking at the available moves\n",
    "        return tuple(set(move.split(\"-\")[1] for move in self.move_names))\n",
    "\n",
    "\n",
    "cdef class EntityRecognizer(Parser):\n",
    "    \"\"\"Pipeline component for named entity recognition.\n",
    "\n",
    "    DOCS: https://spacy.io/api/entityrecognizer\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"ner\"\n",
    "    TransitionSystem = BiluoPushDown\n",
    "    nr_feature = 6\n",
    "\n",
    "    def add_multitask_objective(self, target):\n",
    "        if target == \"cloze\":\n",
    "            cloze = ClozeMultitask(self.vocab)\n",
    "            self._multitasks.append(cloze)\n",
    "        else:\n",
    "            labeller = MultitaskObjective(self.vocab, target=target)\n",
    "            self._multitasks.append(labeller)\n",
    "\n",
    "    def init_multitask_objectives(self, get_gold_tuples, pipeline, sgd=None, **cfg):\n",
    "        for labeller in self._multitasks:\n",
    "            tok2vec = self.model.tok2vec\n",
    "            labeller.begin_training(get_gold_tuples, pipeline=pipeline,\n",
    "                                    tok2vec=tok2vec)\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return (EntityRecognizer, (self.vocab, self.moves, self.model),\n",
    "                None, None)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        # Get the labels from the model by looking at the available moves, e.g.\n",
    "        # B-PERSON, I-PERSON, L-PERSON, U-PERSON\n",
    "        return tuple(set(move.split(\"-\")[1] for move in self.move_names\n",
    "                if move[0] in (\"B\", \"I\", \"L\", \"U\")))\n",
    "\n",
    "\n",
    "class EntityLinker(Pipe):\n",
    "    name = 'entity_linker'\n",
    "\n",
    "    @classmethod\n",
    "    def Model(cls, nr_class=1, **cfg):\n",
    "        # TODO: non-dummy EL implementation\n",
    "        return None\n",
    "\n",
    "    def __init__(self, model=True, **cfg):\n",
    "        self.model = False\n",
    "        self.cfg = dict(cfg)\n",
    "        self.kb = self.cfg[\"kb\"]\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        self.set_annotations([doc], scores=None, tensors=None)\n",
    "        return doc\n",
    "\n",
    "    def pipe(self, stream, batch_size=128, n_threads=-1):\n",
    "        \"\"\"Apply the pipe to a stream of documents.\n",
    "        Both __call__ and pipe should delegate to the `predict()`\n",
    "        and `set_annotations()` methods.\n",
    "        \"\"\"\n",
    "        for docs in util.minibatch(stream, size=batch_size):\n",
    "            docs = list(docs)\n",
    "            self.set_annotations(docs, scores=None, tensors=None)\n",
    "            yield from docs\n",
    "\n",
    "    def set_annotations(self, docs, scores, tensors=None):\n",
    "        \"\"\"\n",
    "        Currently implemented as taking the KB entry with highest prior probability for each named entity\n",
    "        TODO: actually use context etc\n",
    "        \"\"\"\n",
    "        for i, doc in enumerate(docs):\n",
    "            for ent in doc.ents:\n",
    "                candidates = self.kb.get_candidates(ent.text)\n",
    "                if candidates:\n",
    "                    best_candidate = max(candidates, key=lambda c: c.prior_prob)\n",
    "                    for token in ent:\n",
    "                        token.ent_kb_id_ = best_candidate.entity_\n",
    "\n",
    "    def get_loss(self, docs, golds, scores):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def add_label(self, label):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "class Sentencizer(object):\n",
    "    \"\"\"Segment the Doc into sentences using a rule-based strategy.\n",
    "\n",
    "    DOCS: https://spacy.io/api/sentencizer\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"sentencizer\"\n",
    "    default_punct_chars = [\".\", \"!\", \"?\"]\n",
    "\n",
    "    def __init__(self, punct_chars=None, **kwargs):\n",
    "        \"\"\"Initialize the sentencizer.\n",
    "\n",
    "        punct_chars (list): Punctuation characters to split on. Will be\n",
    "            serialized with the nlp object.\n",
    "        RETURNS (Sentencizer): The sentencizer component.\n",
    "\n",
    "        DOCS: https://spacy.io/api/sentencizer#init\n",
    "        \"\"\"\n",
    "        self.punct_chars = punct_chars or self.default_punct_chars\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the sentencizer to a Doc and set Token.is_sent_start.\n",
    "\n",
    "        doc (Doc): The document to process.\n",
    "        RETURNS (Doc): The processed Doc.\n",
    "\n",
    "        DOCS: https://spacy.io/api/sentencizer#call\n",
    "        \"\"\"\n",
    "        start = 0\n",
    "        seen_period = False\n",
    "        for i, token in enumerate(doc):\n",
    "            is_in_punct_chars = token.text in self.punct_chars\n",
    "            token.is_sent_start = i == 0\n",
    "            if seen_period and not token.is_punct and not is_in_punct_chars:\n",
    "                doc[start].is_sent_start = True\n",
    "                start = token.i\n",
    "                seen_period = False\n",
    "            elif is_in_punct_chars:\n",
    "                seen_period = True\n",
    "        if start < len(doc):\n",
    "            doc[start].is_sent_start = True\n",
    "        return doc\n",
    "\n",
    "    def to_bytes(self, **kwargs):\n",
    "        \"\"\"Serialize the sentencizer to a bytestring.\n",
    "\n",
    "        RETURNS (bytes): The serialized object.\n",
    "\n",
    "        DOCS: https://spacy.io/api/sentencizer#to_bytes\n",
    "        \"\"\"\n",
    "        return srsly.msgpack_dumps({\"punct_chars\": self.punct_chars})\n",
    "\n",
    "    def from_bytes(self, bytes_data, **kwargs):\n",
    "        \"\"\"Load the sentencizer from a bytestring.\n",
    "\n",
    "        bytes_data (bytes): The data to load.\n",
    "        returns (Sentencizer): The loaded object.\n",
    "\n",
    "        DOCS: https://spacy.io/api/sentencizer#from_bytes\n",
    "        \"\"\"\n",
    "        cfg = srsly.msgpack_loads(bytes_data)\n",
    "        self.punct_chars = cfg.get(\"punct_chars\", self.default_punct_chars)\n",
    "        return self\n",
    "\n",
    "    def to_disk(self, path, exclude=tuple(), **kwargs):\n",
    "        \"\"\"Serialize the sentencizer to disk.\n",
    "\n",
    "        DOCS: https://spacy.io/api/sentencizer#to_disk\n",
    "        \"\"\"\n",
    "        path = util.ensure_path(path)\n",
    "        path = path.with_suffix(\".json\")\n",
    "        srsly.write_json(path, {\"punct_chars\": self.punct_chars})\n",
    "\n",
    "\n",
    "    def from_disk(self, path, exclude=tuple(), **kwargs):\n",
    "        \"\"\"Load the sentencizer from disk.\n",
    "\n",
    "        DOCS: https://spacy.io/api/sentencizer#from_disk\n",
    "        \"\"\"\n",
    "        path = util.ensure_path(path)\n",
    "        path = path.with_suffix(\".json\")\n",
    "        cfg = srsly.read_json(path)\n",
    "        self.punct_chars = cfg.get(\"punct_chars\", self.default_punct_chars)\n",
    "        return self\n",
    "\n",
    "      \n",
    "__all__ = [\"Tagger\", \"DependencyParser\", \"EntityRecognizer\", \"Tensorizer\", \"TextCategorizer\", \"EntityLinker\", \"Sentencizer\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Build LANG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVbqv-yawrTx"
   },
   "source": [
    "## TRAIN Language Model\n",
    "https://spacy.io/usage/training\n",
    "\n",
    "**Key demo (Norwagian language model creation) https://github.com/explosion/spaCy/issues/3082**\n",
    "\n",
    "Flow of Training\n",
    "- Creating a vocabulary file\n",
    "  - spaCy expects that common words will be cached in a Vocab instance. The vocabulary caches lexical features. spaCy loads the vocabulary from binary data, in order to keep loading efficient. The easiest way to save out a new binary vocabulary file is to use the spacy init-model command, which expects a JSONL file with words and their lexical attributes. See the docs on the vocab JSONL format for details.\n",
    "- Training the word vectors\n",
    "  - Word2vec and related algorithms let you train useful word similarity models from unlabeled text. This is a key part of using deep learning for NLP with limited labeled data. The vectors are also useful by themselves – they power the .similarity methods in spaCy. For best results, you should pre-process the text with spaCy before training the Word2vec model. This ensures your tokenization will match. You can use our word vectors training script, which pre-processes the text with your language-specific tokenizer and trains the model using Gensim. The vectors.bin file should consist of one word and vector per line.\n",
    "  - https://github.com/explosion/spacy/tree/master/bin/train_word_vectors.py\n",
    "  - If you don’t have a large sample of text available, you can also convert word vectors produced by a variety of other tools into spaCy’s format. See the docs on converting word vectors for details.\n",
    "- Creating or converting a training corpus\n",
    "  - The easiest way to train spaCy’s tagger, parser, entity recognizer or text categorizer is to use the spacy train command-line utility. In order to use this, you’ll need training and evaluation data in the JSON format spaCy expects for training.\n",
    "  - You can now train the model using a corpus for your language annotated with If your data is in one of the supported formats, the easiest solution might be to use the spacy convert command-line utility. This supports several popular formats, including the IOB format for named entity recognition, the JSONL format produced by our annotation tool Prodigy, and the CoNLL-U format used by the Universal Dependencies corpus.\n",
    "  - One thing to keep in mind is that spaCy expects to train its models from whole documents, not just single sentences. If your corpus only contains single sentences, spaCy’s models will never learn to expect multi-sentence documents, leading to low performance on real text. To mitigate this problem, you can use the -N argument to the spacy convert command, to merge some of the sentences into longer pseudo-documents.\n",
    "- Training the tagger and parser\n",
    "  - Once you have your training and evaluation data in the format spaCy expects, you can train your model use the using spaCy’s train command. Note that training statistical models still involves a degree of trial-and-error. You may need to tune one or more settings, also called “hyper-parameters”, to achieve optimal performance. See the usage guide on training for more details.\n",
    "  \n",
    "\n",
    "\n",
    "1. From scratch \n",
    "2. Update on existing model\n",
    "\n",
    "\n",
    "> Both can be preceded by **Pretrain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hAS8cb7pXOw"
   },
   "source": [
    "### (1) From Scratch (CLI or Code)\n",
    "\n",
    "**CLI method**\n",
    "- Input\n",
    "  - **Annotated format - supports several popular formats, including the IOB format for named entity recognition, the JSONL format produced by our annotation tool Prodigy, and the CoNLL-U format used by the Universal Dependencies corpus.**\n",
    "  - `spacy convert` into spaCy JSON format\n",
    "- Example:\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora\n",
    "mkdir ancora-json\n",
    "python -m spacy convert UD_Spanish-AnCora/es_ancora-ud-train.conllu ancora-json\n",
    "python -m spacy convert UD_Spanish-AnCora/es_ancora-ud-dev.conllu ancora-json\n",
    "mkdir models\n",
    "python -m spacy train es models ancora-json/es_ancora-ud-train.json ancora-json/es_ancora-ud-dev.json\n",
    "```\n",
    "\n",
    "**Simple code method (Preferred)**\n",
    "\n",
    "> Instead of sequences of `Doc and GoldParse` objects, you can also use the “simple training style” and **pass raw texts and dictionaries of annotations to nlp.update.** The dictionaries can have the **keys entities, heads, deps, tags and cats.** This is generally recommended, as it removes one layer of abstraction, and avoids unnecessary imports. It also makes it easier to structure and load your training data.\n",
    "\n",
    "- Example Annotations\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"entities\": [(0, 4, \"ORG\")],\n",
    "   \"heads\": [1, 1, 1, 5, 5, 2, 7, 5],\n",
    "   \"deps\": [\"nsubj\", \"ROOT\", \"prt\", \"quantmod\", \"compound\", \"pobj\", \"det\", \"npadvmod\"],\n",
    "   \"tags\": [\"PROPN\", \"VERB\", \"ADP\", \"SYM\", \"NUM\", \"NUM\", \"DET\", \"NOUN\"],\n",
    "   \"cats\": {\"BUSINESS\": 1.0},\n",
    "}\n",
    "```\n",
    "\n",
    "- Simple Training Loop\n",
    "\n",
    "```python\n",
    "TRAIN_DATA = [\n",
    "        (u\"Uber blew through $1 million a week\", {\"entities\": [(0, 4, \"ORG\")]}),\n",
    "        (u\"Google rebrands its business apps\", {\"entities\": [(0, 6, \"ORG\")]})]\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        nlp.update([text], [annotations], sgd=optimizer)\n",
    "nlp.to_disk(\"/model\")\n",
    "```\n",
    "\n",
    "> The above training loop leaves out a few details that can really improve accuracy – but the principle really is that simple. Once you’ve got your pipeline together and you want to tune the accuracy, you usually want to process your training examples in batches, and experiment with minibatch sizes and dropout rates, set via the drop keyword argument. See the Language and Pipe API docs for available options.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mKozDiPt0YlY"
   },
   "source": [
    "#### NER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIFzfYh22D1h"
   },
   "source": [
    "\n",
    "**(1) BUILT-IN ENTITY**\n",
    "\n",
    "**Blank Model or Load Built-in**\n",
    "\n",
    "**(2) CUSTOM ENTITY**\n",
    "\n",
    "**Training an additional entity type** \\\n",
    "\n",
    "> **In practice, you’ll need many more — a few hundred would be a good start. You will also likely need to mix in examples of other entity types, which might be obtained by running the entity recognizer over unlabelled sentences, and adding their annotations to the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = \"In practice, you’ll need many more — a few hundred would be a good start. You will also likely need to mix in examples of other entity types, which might be obtained by running the entity recognizer over unlabelled sentences, and adding their annotations to the training set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sent_nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_doc = sent_nlp(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-348025155926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sent' is not defined"
     ]
    }
   ],
   "source": [
    "sent.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_nlp.add_pipe(sent_nlp.create_pipe('sentencizer'), before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'sentencizer', 'parser', 'ner']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_doc = sent_nlp(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text = [sent for sent in sent_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[In practice, you’ll need many more — a few hundred would be a good start.,\n",
       " You will also likely need to mix in examples of other entity types, which might be obtained by running the entity recognizer over unlabelled sentences, and adding their annotations to the training set.]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_text[0].text.find('good') + len('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3EyfHvU65UE"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    # instead u'raw text' using sent1 made above\n",
    "    (sent1, {'entities': [\n",
    "        (17, 21, 'MONEY'),\n",
    "        (52, 57, 'LOC'),\n",
    "        (77, 91, 'PERSON'),\n",
    "        (129, 134, 'MONEY')\n",
    "    ]}),\n",
    "    (sent2, {'entities': [\n",
    "        (250, 256, 'PERSON')\n",
    "    ]}),\n",
    "    (sent3, {'entities': [\n",
    "        (64, 71, 'ORG'),\n",
    "        (154, 158, 'MONEY')\n",
    "    ]}),\n",
    "    (sent4, {'entities': [\n",
    "        (55, 61, 'PERSON'),\n",
    "        (88, 93, 'MONEY'),\n",
    "        (111, 115, 'MONEY')\n",
    "    ]}),\n",
    "    (sent5, {'entities': [\n",
    "        (6, 11, 'MONEY'),\n",
    "        (25, 32, 'ORG')\n",
    "    ]}),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JegQEiSNdoH0",
    "toc-hr-collapsed": true
   },
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JegQEiSNdoH0"
   },
   "source": [
    "## GENSIM loading Pretrained Word2Vec \n",
    "\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "# Load vectors directly from the file 1G\n",
    "model = KeyedVectors.load_word2vec_format('data/GoogleGoogleNews-vectors-negative300.bin', binary=True)\n",
    "# Access vectors for specific words with a keyed lookup:\n",
    "vector = model['easy']\n",
    "# see the shape of the vector (300,)\n",
    "vector.shape\n",
    "# Processing sentences is not as simple as with Spacy:\n",
    "vectors = [model[x] for x in \"This is some text I am processing with Spacy\".split(' ')]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SpaCy_Guide.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
