{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RA_SpaCy_DEMO.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "qMtQcg8z8-F7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TddRiwSPYouV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Snippet: Self-made 3 Versions of Preprocessing Text\n",
        "\n",
        "1. Bare Token: Sans grammer, sans semantics\n",
        "\n",
        "```python\n",
        "def token_cleaner(text):\n",
        "    text = strip_multiple_whitespaces(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = strip_numeric(text) \n",
        "    text = strip_non_alphanum(text)\n",
        "    text = strip_punctuation(text)\n",
        "    text = strip_short(text, minsize=3)\n",
        "    text = [ tok.lemma_.lower().strip() for tok in nlp(text, disable=['tagger', 'parser', 'ner']) ]\n",
        "    text = [ tok for tok in text if tok not in SYMBOLS and tok not in STOPLIST ]\n",
        "    return ' '.join(text)\n",
        "```\n",
        "\n",
        "2. Lemmas: Retain grammar and semantcis\n",
        "\n",
        "```python\n",
        "def token_cleaner(text):\n",
        "    text = strip_multiple_whitespaces(text)\n",
        "    text = strip_non_alphanum(text)\n",
        "    text = strip_punctuation(text)\n",
        "    text = strip_short(text, minsize=3) # optional\n",
        "    text = [ tok.lemma_.lower().strip() for tok in nlp(text, disable=['tagger', 'parser', 'ner']) ]\n",
        "    text = [ tok for tok in text if tok not in SYMBOLS ]\n",
        "    return ' '.join(text)\n",
        "```\n",
        "\n",
        "3. Clean Text: Remove non-text only\n",
        "\n",
        "```python\n",
        "def token_cleaner(text):\n",
        "    text = strip_multiple_whitespaces(text)\n",
        "    text = strip_non_alphanum(text)\n",
        "    text = strip_punctuation(text)\n",
        "    text = strip_short(text, minsize=3) # optional\n",
        "    text = [ tok.text.lower().strip() for tok in nlp(text, disable=['tagger', 'parser', 'ner']) ]\n",
        "    text = [ tok for tok in text if tok not in SYMBOLS ]\n",
        "    return ' '.join(text)\n",
        "```\n",
        "\n",
        "### SpaCy Trick to speed up above by applying on whole text\n",
        "\n",
        "```python\n",
        "def doc_to_spans(list_of_texts, join_string=' ||| '):\n",
        "    all_docs = nlp(' ||| '.join(list_of_texts))\n",
        "    split_inds = [i for i, token in enumerate(all_docs) if token.text == '|||'] + [len(all_docs)]\n",
        "    new_docs = [all_docs[(i + 1 if i > 0 else i):j] for i, j in zip([0] + split_inds[:-1], split_inds)]\n",
        "    return new_docs \n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "if9kDJNBYs27",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Snippet: Train Text Classification (TextCat pipe)\n",
        "\n",
        "### Step by step guide\n",
        "\n",
        "1. Load the model you want to start with, or create an empty model using spacy.blank with the ID of your language. If you’re using an existing model, make sure to disable all other pipeline components during training using nlp.disable_pipes. This way, you’ll only be training the text classifier.\n",
        "2. Add the text classifier to the pipeline, and add the labels you want to train – for example, POSITIVE.\n",
        "3. Load and pre-process the dataset, shuffle the data and split off a part of it to hold back for evaluation. This way, you’ll be able to see results on each training iteration.\n",
        "4. Loop over the training examples and partition them into batches using spaCy’s minibatch and compounding helpers.\n",
        "5. Update the model by calling nlp.update, which steps through the examples and makes a prediction. It then consults the annotations to see whether it was right. If it was wrong, it adjusts its weights so that the correct prediction will score higher next time.\n",
        "6. Optionally, you can also evaluate the text classifier on each iteration, by checking how it performs on the development data held back from the dataset. This lets you print the precision, recall and F-score.\n",
        "7. Save the trained model using nlp.to_disk.\n",
        "8. Test the model to make sure the text classifier works as expected."
      ]
    },
    {
      "metadata": {
        "id": "dUJyEj3aXr4Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import thinc.extra.datasets\n",
        "\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ONplKKCCW8o_",
        "colab_type": "code",
        "outputId": "9e6547b1-bb22-4064-fc8e-8b1a36b41fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0a7 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0a7/en_core_web_md-2.1.0a7.tar.gz#egg=en_core_web_md==2.1.0a7\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0a7/en_core_web_md-2.1.0a7.tar.gz (95.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 95.4MB 2.5MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "  Found existing installation: en-core-web-md 2.0.0\n",
            "    Uninstalling en-core-web-md-2.0.0:\n",
            "      Successfully uninstalled en-core-web-md-2.0.0\n",
            "  Running setup.py install for en-core-web-md ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed en-core-web-md-2.1.0a7\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C17WCbKkaM0Z",
        "colab_type": "code",
        "outputId": "fd5fab80-c6cb-4c1b-bbe8-1f67b58ffa37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy validate"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r⠙ Loading compatibility table...\r\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
            "\u001b[1m\n",
            "===================== Installed models (spaCy v2.1.0a13) =====================\u001b[0m\n",
            "\u001b[38;5;4mℹ spaCy installation: /usr/local/lib/python3.6/dist-packages/spacy\u001b[0m\n",
            "\n",
            "TYPE      NAME                MODEL               VERSION                              \n",
            "package   en-vectors-web-lg   en_vectors_web_lg   \u001b[38;5;2m2.1.0a0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "package   en-core-web-sm      en_core_web_sm      \u001b[38;5;1m2.0.0\u001b[0m     --> 2.1.0a7   \n",
            "package   en-core-web-md      en_core_web_md      \u001b[38;5;2m2.1.0a7\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "package   en-core-web-lg      en_core_web_lg      \u001b[38;5;2m2.1.0a7\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "link      en                  en_core_web_sm      \u001b[38;5;1m2.0.0\u001b[0m     --> 2.1.0a7   \n",
            "link      en_core_web_md      en_core_web_md      \u001b[38;5;2m2.1.0a7\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "link      en_vectors_web_lg   en_vectors_web_lg   \u001b[38;5;2m2.1.0a0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "\n",
            "\u001b[1m\n",
            "============================== Install updates ==============================\u001b[0m\n",
            "Use the following commands to update the model packages:\n",
            "python -m spacy download en_core_web_sm\n",
            "\n",
            "You may also want to overwrite the incompatible links using the `python -m spacy\n",
            "link` command with `--force`, or remove them from the data directory. Data path:\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JII3cHQ6hIed",
        "colab_type": "code",
        "outputId": "1e8e3299-2c08-41d0-dd3b-e7b38cb99eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_vectors_web_lg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_vectors_web_lg==2.1.0a0 from https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.1.0a0/en_vectors_web_lg-2.1.0a0.tar.gz#egg=en_vectors_web_lg==2.1.0a0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.1.0a0/en_vectors_web_lg-2.1.0a0.tar.gz (661.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 661.8MB 123.4MB/s \n",
            "\u001b[?25hInstalling collected packages: en-vectors-web-lg\n",
            "  Found existing installation: en-vectors-web-lg 2.0.0\n",
            "    Uninstalling en-vectors-web-lg-2.0.0:\n",
            "      Successfully uninstalled en-vectors-web-lg-2.0.0\n",
            "  Running setup.py install for en-vectors-web-lg ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed en-vectors-web-lg-2.1.0a0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_vectors_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zLlE7u3faXmx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GITHUB EXAMPLE"
      ]
    },
    {
      "metadata": {
        "id": "HYLN3raFpz-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SAMPLE DATA LOADING\n",
        "\n",
        "def load_data(limit=0, split=0.8):\n",
        "    \"\"\"Load data from the IMDB dataset.\"\"\"\n",
        "    # Partition off part of the train data for evaluation\n",
        "    train_data, _ = thinc.extra.datasets.imdb()\n",
        "    random.shuffle(train_datam)\n",
        "    train_data = train_data[-limit:]\n",
        "    texts, labels = zip(*train_data)\n",
        "    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n",
        "    split = int(len(train_data) * split)\n",
        "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
        "\n",
        "  \n",
        "# EVAL FUNC\n",
        "\n",
        "def evaluate(tokenizer, textcat, texts, cats):\n",
        "    docs = (tokenizer(text) for text in texts)\n",
        "    tp = 0.0  # True positives\n",
        "    fp = 1e-8  # False positives\n",
        "    fn = 1e-8  # False negatives\n",
        "    tn = 0.0  # True negatives\n",
        "    for i, doc in enumerate(textcat.pipe(docs)):\n",
        "        gold = cats[i]\n",
        "        for label, score in doc.cats.items():\n",
        "            if label not in gold:\n",
        "                continue\n",
        "            if label == \"NEGATIVE\":\n",
        "                continue\n",
        "            if score >= 0.5 and gold[label] >= 0.5:\n",
        "                tp += 1.0\n",
        "            elif score >= 0.5 and gold[label] < 0.5:\n",
        "                fp += 1.0\n",
        "            elif score < 0.5 and gold[label] < 0.5:\n",
        "                tn += 1\n",
        "            elif score < 0.5 and gold[label] >= 0.5:\n",
        "                fn += 1\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    if (precision + recall) == 0:\n",
        "        f_score = 0.0\n",
        "    else:\n",
        "        f_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}\n",
        "\n",
        "  # WRAPPING WHOLE PROCESS AS MAIN\n",
        "# Key steps are commented\n",
        "\n",
        "def main(model=None, output_dir=None, n_iter=20, n_texts=2000, init_tok2vec=None):\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # KEY: Use or Make \"textcat\" pipe\n",
        "    \n",
        "    # add the text classifier to the pipeline if it doesn't exist\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"textcat\" not in nlp.pipe_names:\n",
        "        textcat = nlp.create_pipe(\n",
        "            \"textcat\",\n",
        "            config={\n",
        "                \"exclusive_classes\": True,\n",
        "                \"architecture\": \"simple_cnn\",\n",
        "            }\n",
        "        )\n",
        "        nlp.add_pipe(textcat, last=True)\n",
        "    # otherwise, get it, so we can add labels to it\n",
        "    else:\n",
        "        textcat = nlp.get_pipe(\"textcat\")\n",
        "\n",
        "    # KEY: Add Label\n",
        "    \n",
        "    # add label to text classifier\n",
        "    textcat.add_label(\"POSITIVE\")\n",
        "    textcat.add_label(\"NEGATIVE\")\n",
        "\n",
        "    # load the IMDB dataset\n",
        "    print(\"Loading IMDB data...\")\n",
        "    (train_texts, train_cats), (dev_texts, dev_cats) = load_data()\n",
        "    train_texts = train_texts[:n_texts]\n",
        "    train_cats = train_cats[:n_texts]\n",
        "    print(\n",
        "        \"Using {} examples ({} training, {} evaluation)\".format(\n",
        "            n_texts, len(train_texts), len(dev_texts)\n",
        "        )\n",
        "    )\n",
        "    train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
        "\n",
        "    \n",
        "    # KEY: Disabling other pipes\n",
        "    # Note logic of nlp.begin_training()\n",
        "    # Load model.tok2vec\n",
        "    # Batching using minibatch\n",
        "    # nlp.update() is main training func\n",
        "    # extract params using use_param()\n",
        "    \n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
        "        optimizer = nlp.begin_training()\n",
        "        if init_tok2vec is not None:\n",
        "            with init_tok2vec.open(\"rb\") as file_:\n",
        "                textcat.model.tok2vec.from_bytes(file_.read())\n",
        "        print(\"Training the model...\")\n",
        "        print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
        "        batch_sizes = compounding(4.0, 32.0, 1.001)\n",
        "        for i in range(n_iter):\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            random.shuffle(train_data)\n",
        "            batches = minibatch(train_data, size=batch_sizes)\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
        "            with textcat.model.use_params(optimizer.averages):\n",
        "                # evaluate on the dev data split off in load_data()\n",
        "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
        "            print(\n",
        "                \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(  # print a simple table\n",
        "                    losses[\"textcat\"],\n",
        "                    scores[\"textcat_p\"],\n",
        "                    scores[\"textcat_r\"],\n",
        "                    scores[\"textcat_f\"],\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # Test Snippet\n",
        "    test_text = \"This movie sucked\"\n",
        "    doc = nlp(test_text)\n",
        "    print(test_text, doc.cats)\n",
        "\n",
        "    if output_dir is not None:\n",
        "        with nlp.use_params(optimizer.averages):\n",
        "            nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "        # test the saved model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        doc2 = nlp2(test_text)\n",
        "        print(test_text, doc2.cats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zFzcW0Eyp9Xy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "main() # not ran"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N4i26nzPaeSh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### OWN EXAMPLE"
      ]
    },
    {
      "metadata": {
        "id": "S4RjY-U6USAJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Kaggle Project Example\n",
        "\n",
        "# Params\n",
        "n_iter=20\n",
        "n_texts=2000\n",
        "init_tok2vec=None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rbpb0lt-Wwjn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_vectors_web_lg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1iM0CqZWaqcY",
        "colab_type": "code",
        "outputId": "cd611092-4a8d-450c-ad62-ff516706048b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "nlp.pipe_names # assert no built-in textcat pipeline (actually nothing as blank model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "metadata": {
        "id": "T1Y0xj-zavDw",
        "colab_type": "code",
        "outputId": "8e0d2440-6179-4cc2-e9e0-a0776f3e302f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# create pipeline\n",
        "textcat = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": True, \"archtecture\": \"simple_cnn\"})\n",
        "\n",
        "nlp.add_pipe(textcat, last=True) # place at last in pipeline\n",
        "\n",
        "# add custom label for clf\n",
        "textcat.add_label(\"POSITIVE\")\n",
        "textcat.add_label(\"NEGATIVE\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "metadata": {
        "id": "mrBpAhueWVqQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare kaggle text - shuffle and split\n",
        "\n",
        "kaggle_df = pd.read_csv('kaggle_train.csv')\n",
        "kaggle_df = kaggle_df.sample(frac=1).reset_index(drop=True)\n",
        "kaggle_df.head(), kaggle_df.shape[0] * 0.8, kaggle_df.shape[0] * 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bZNKn9ESYKfr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_text, train_cat = kaggle_df['sentences'][:6824].tolist(), kaggle_df['sentiment'][:6824].tolist()\n",
        "train_cat = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in train_cat]\n",
        "dev_text, dev_cat = kaggle_df['sentences'][6824:7677].tolist(), kaggle_df['sentiment'][6824:7677].tolist()\n",
        "dev_cat = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in dev_cat]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OsnAghnEp8MS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# leave test as binary for prediction metrics\n",
        "test_text, test_cat = kaggle_df['sentences'][7677:].tolist(), kaggle_df['sentiment'][7677:].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYNXGKJGZvRC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# bundle train as tuple for later\n",
        "train_data = list(zip(train_text, [{'cats': cat} for cat in train_cat]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x4vY82t9iO9K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# EVAL func: Precision, Recall, F1-score\n",
        "\n",
        "def evaluate(tokenizer, textcat, texts, cats):\n",
        "    docs = (tokenizer(text) for text in texts)\n",
        "    tp = 0.0  # True positives\n",
        "    fp = 1e-8  # False positives\n",
        "    fn = 1e-8  # False negatives\n",
        "    tn = 0.0  # True negatives\n",
        "    for i, doc in enumerate(textcat.pipe(docs)):\n",
        "        gold = cats[i]\n",
        "        for label, score in doc.cats.items():\n",
        "            if label not in gold:\n",
        "                continue\n",
        "            if label == \"NEGATIVE\":\n",
        "                continue\n",
        "            if score >= 0.5 and gold[label] >= 0.5:\n",
        "                tp += 1.0\n",
        "            elif score >= 0.5 and gold[label] < 0.5:\n",
        "                fp += 1.0\n",
        "            elif score < 0.5 and gold[label] < 0.5:\n",
        "                tn += 1\n",
        "            elif score < 0.5 and gold[label] >= 0.5:\n",
        "                fn += 1\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    if (precision + recall) == 0:\n",
        "        f_score = 0.0\n",
        "    else:\n",
        "        f_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JBrwmj_jbYRv",
        "colab_type": "code",
        "outputId": "918f9c99-9b0f-4ca5-d85b-bba9ac38beb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "cell_type": "code",
      "source": [
        "# Main training code\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
        "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
        "    optimizer = nlp.begin_training()\n",
        "    \n",
        "    # DID NOT WORK (due to mismatch parameters between \n",
        "    # how .bin was trained and this custom model\n",
        "    # already pretrained binary vector weight to use \n",
        "    # tok2vec_bin = 'spacy-pretrain-polyaxon/lmao-imdb-1k/weights/model832.bin'\n",
        "    # with open(tok2vec_bin, 'rb') as file_:\n",
        "    #    textcat.model.tok2vec.from_bytes(file_.read())\n",
        "    \n",
        "    print(\"Training the model...\")\n",
        "    print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
        "    batch_sizes = compounding(4.0, 32.0, 1.001)\n",
        "    for i in range(n_iter):\n",
        "        losses = {}\n",
        "        # batch up the examples using spaCy's minibatch\n",
        "        random.shuffle(train_data)\n",
        "        batches = minibatch(train_data, size=batch_sizes)\n",
        "        for batch in batches:\n",
        "            texts, annotations = zip(*batch)\n",
        "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
        "        with textcat.model.use_params(optimizer.averages):\n",
        "            # evaluate on the dev data split off in load_data()\n",
        "            scores = evaluate(nlp.tokenizer, textcat, dev_text, dev_cat)\n",
        "        print(\n",
        "            \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(  # print a simple table\n",
        "                losses[\"textcat\"],\n",
        "                scores[\"textcat_p\"],\n",
        "                scores[\"textcat_r\"],\n",
        "                scores[\"textcat_f\"],\n",
        "            )\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "LOSS \t  P  \t  R  \t  F  \n",
            "15.183\t0.681\t0.670\t0.675\n",
            "0.937\t0.730\t0.693\t0.711\n",
            "0.209\t0.749\t0.719\t0.734\n",
            "0.069\t0.746\t0.743\t0.745\n",
            "0.040\t0.729\t0.736\t0.732\n",
            "0.031\t0.726\t0.726\t0.726\n",
            "0.028\t0.721\t0.708\t0.714\n",
            "0.021\t0.723\t0.722\t0.723\n",
            "0.019\t0.719\t0.724\t0.722\n",
            "0.015\t0.725\t0.745\t0.735\n",
            "0.013\t0.719\t0.750\t0.734\n",
            "0.011\t0.720\t0.752\t0.736\n",
            "0.011\t0.704\t0.745\t0.724\n",
            "0.009\t0.708\t0.738\t0.723\n",
            "0.007\t0.717\t0.745\t0.731\n",
            "0.007\t0.716\t0.748\t0.731\n",
            "0.006\t0.716\t0.750\t0.733\n",
            "0.005\t0.711\t0.762\t0.736\n",
            "0.004\t0.709\t0.759\t0.733\n",
            "0.004\t0.702\t0.745\t0.723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IhSm3dxqU8lP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save and load model\n",
        "\n",
        "with nlp.use_params(optimizer.averages):\n",
        "    nlp.to_disk('./')\n",
        "\n",
        "nlp2 = spacy.load('./')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X6ev9Mchi35E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# re-merge test text and cat into dataframe\n",
        "\n",
        "test_df = pd.DataFrame(list(zip(test_text, test_cat)), columns=['sentences', 'sentiment'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nPTAnK2Bpxlk",
        "colab_type": "code",
        "outputId": "be8e15ec-061f-43ae-fb5b-d209fc7017f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i weep for the future when a good portion of t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it's a trifle of a movie , with a few laughs s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a disoriented but occasionally disarming saga ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the film's 45-minute running time stops shy of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>haneke challenges us to confront the reality o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           sentences  sentiment\n",
              "0  i weep for the future when a good portion of t...          0\n",
              "1  it's a trifle of a movie , with a few laughs s...          0\n",
              "2  a disoriented but occasionally disarming saga ...          1\n",
              "3  the film's 45-minute running time stops shy of...          1\n",
              "4  haneke challenges us to confront the reality o...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "metadata": {
        "id": "vH58fx2SwcnK",
        "colab_type": "code",
        "outputId": "b3f55a54-0000-4cd5-8cbc-0ac1b40e553c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# sample prediction result\n",
        "\n",
        "nlp2(test_df.iloc[0, 0]).cats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NEGATIVE': 0.0034787263721227646, 'POSITIVE': 0.9965212345123291}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "metadata": {
        "id": "qEMq9nkboXkh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# predict and convert text\n",
        "\n",
        "test_df['pred'] = test_df['sentences'].apply(lambda text: 1 if nlp2(text).cats.get('POSITIVE') > 0.5 else 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y7d0Enl9qJz4",
        "colab_type": "code",
        "outputId": "72090ece-4f8d-4492-9277-15a236172606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "print('Accuracy =======>> {}'.format(accuracy_score(test_df['sentiment'], test_df['pred'])))\n",
        "print('F1 Score =======>> {}'.format(f1_score(test_df['sentiment'], test_df['pred'])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy =======>> 0.7268464243845252\n",
            "F1 Score =======>> 0.7268464243845251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZRPPxqE0n_3c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Official: Pretrain Vectors for Textcat"
      ]
    },
    {
      "metadata": {
        "id": "HrZxMkNcwrYu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In case needing python3.7\n",
        "\n",
        "```shell\n",
        "sudo apt-get update\n",
        "sudo apt-get install build-essential libpq-dev libssl-dev openssl libffi-dev zlib1g-dev\n",
        "sudo apt-get install python3-pip python3-dev\n",
        "sudo apt-get install python3.7\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "jsKF4grZwrcA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# debugger - breakpoint\n",
        "from pdb import set_trace as bp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1OXZdxt4MQVM",
        "colab_type": "code",
        "outputId": "539f4c85-7ffb-47d9-93d3-c865e156b114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "cell_type": "code",
      "source": [
        "# Pretrain experimental in spacy-nightly\n",
        "!pip install spacy-nightly"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy-nightly in /usr/local/lib/python3.6/dist-packages (2.1.0a13)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy-nightly) (2.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy-nightly) (1.16.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-nightly) (2.0.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy-nightly) (2.0.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-nightly) (7.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy-nightly) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.6/dist-packages (from spacy-nightly) (0.2.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy-nightly) (0.0.5)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy-nightly) (0.9.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy-nightly) (2.18.4)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy-nightly) (0.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy-nightly) (4.28.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CIZ3s3tjwreS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"This script is experimental.\n",
        "\n",
        "Try pre-training the CNN component of the text categorizer using a cheap\n",
        "language modelling-like objective. Specifically, we load pre-trained vectors\n",
        "(from something like word2vec, GloVe, FastText etc), and use the CNN to\n",
        "predict the tokens' pre-trained vectors. This isn't as easy as it sounds:\n",
        "we're not merely doing compression here, because heavy dropout is applied,\n",
        "including over the input words. This means the model must often (50% of the time)\n",
        "use the context in order to predict the word.\n",
        "\n",
        "To evaluate the technique, we're pre-training with the 50k texts from the IMDB\n",
        "corpus, and then training with only 100 labels. Note that it's a bit dirty to\n",
        "pre-train with the development data, but also not *so* terrible: we're not using\n",
        "the development labels, after all --- only the unlabelled text.\n",
        "\n",
        "@plac.annotations(\n",
        "    width=(\"Width of CNN layers\", \"positional\", None, int),\n",
        "    embed_size=(\"Embedding rows\", \"positional\", None, int),\n",
        "    pretrain_iters=(\"Number of iterations to pretrain\", \"option\", \"pn\", int),\n",
        "    train_iters=(\"Number of iterations to pretrain\", \"option\", \"tn\", int),\n",
        "    train_examples=(\"Number of labelled examples\", \"option\", \"eg\", int),\n",
        "    vectors_model=(\"Name or path to vectors model to learn from\"),\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "import plac\n",
        "import random\n",
        "import spacy\n",
        "import thinc.extra.datasets\n",
        "from spacy.util import minibatch, use_gpu, compounding\n",
        "import tqdm\n",
        "from spacy._ml import Tok2Vec\n",
        "from spacy.pipeline import TextCategorizer\n",
        "import numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E5paL1tujZHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pretrain_iters=30\n",
        "train_iters=30\n",
        "train_examples=1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mGYZx7RNOHoR",
        "colab_type": "code",
        "outputId": "0f8ea910-c667-47f2-b971-2e4c63106037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# Using md model as base\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_md==2.1.0a7 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0a7/en_core_web_md-2.1.0a7.tar.gz#egg=en_core_web_md==2.1.0a7 in /usr/local/lib/python3.6/dist-packages (2.1.0a7)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1a51CuPDPox5",
        "colab_type": "code",
        "outputId": "eb8bcbbe-8310-45bf-d729-f4d5bce4abfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy link en_core_web_md en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SOHotrJce8s4",
        "colab_type": "code",
        "outputId": "d982b840-bdce-4285-8832-578d63bb1c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy info"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    2.1.0a13                      \n",
            "Location         /usr/local/lib/python3.6/dist-packages/spacy\n",
            "Platform         Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic\n",
            "Python version   3.6.7                         \n",
            "Models           en                            \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nJtjZN5me-M8",
        "colab_type": "code",
        "outputId": "dfef0d52-d6f9-4466-c491-3f9f7e8dff0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy validate"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r⠙ Loading compatibility table...\r\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
            "\u001b[1m\n",
            "===================== Installed models (spaCy v2.1.0a13) =====================\u001b[0m\n",
            "\u001b[38;5;4mℹ spaCy installation: /usr/local/lib/python3.6/dist-packages/spacy\u001b[0m\n",
            "\n",
            "TYPE      NAME             MODEL            VERSION                              \n",
            "package   en-core-web-sm   en_core_web_sm   \u001b[38;5;1m2.0.0\u001b[0m     --> 2.1.0a7   \n",
            "package   en-core-web-md   en_core_web_md   \u001b[38;5;2m2.1.0a7\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "link      en               en_core_web_md   \u001b[38;5;2m2.1.0a7\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "\n",
            "\u001b[1m\n",
            "============================== Install updates ==============================\u001b[0m\n",
            "Use the following commands to update the model packages:\n",
            "python -m spacy download en_core_web_sm\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5Fik3Ndj3bys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load pretrain data - un-labelled\n",
        "\n",
        "def load_texts(limit=0):\n",
        "  train, dev = thinc.extra.datasets.imdb()\n",
        "  train_texts, train_labels = zip(*train)\n",
        "  dev_texts, dev_labels = zip(*train)\n",
        "  train_texts = list(train_texts)\n",
        "  dev_texts = list(dev_texts)\n",
        "  random.shuffle(train_texts)\n",
        "  random.shuffle(dev_texts)\n",
        "  if limit >= 1:\n",
        "      return train_texts[:limit]\n",
        "  else:\n",
        "      return list(train_texts) + list(dev_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DKgGxyI_3jvo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "temp_text = load_texts(limit=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7oQ0wD467FDn",
        "colab_type": "code",
        "outputId": "0b6233a1-7c44-4a7f-ad5a-bae0f4d4436e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "temp_text[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Just finished watching this movie for maybe the 7th or 8th time, picked it up one night previously viewed at Blockbuster and absolutely loved it, I\\'ve shown it to 4 people so far and they have enjoyed it as well. Avoid of all the Hollywood glamour, special effects and stress on the \"shock factor\", this independent film by Paul F. Ryan hits the nail on the head in dealing with the after affects of traumatic situations. Taking place after a high school shooting, two characters Alicia (Busy Philipps) and Deanna (Erika Christensen) form an unlikely bond. Alicia, the girl with the stone heart, the Goth who has a pessimistic attitude to life assists Deanna to overcome the issues of life and death and living in the aftermath. Meanwhile Deanna attempts to help Alicia to see some of the softness and light in the world again. Not stressing on the shocking event of the shooting, but on the interpersonal relationships amongst those who survived it sets this movie apart. Despite its low-budget and short filming time, this movie is far from cheesy. Ryan pays respect to a situation he has never endured and attempts to delve into the human psyche. With an amazing up and coming actress, Philipps, adds the necessary dramatics to the dialogue and overall feel for the film and Christensen helps to balance out the \"doom and gloom\" feeling this movie may have. Overall, I recommend this movie and if you enjoy the topic of school aggression and violence and learning more about it, I also suggest the documentary \"It\\'s a Girls World\" put out by CBC in 2004, which deals with the topic of social bullying, comparing and contrasting two groups of girls one in Montreal, Quebec and the other in Victoria, British Columbia, the group of friends and acquaintances of Dawn Marie Wellesley a 14 year old girl who killed herself after being brutally bullied.',\n",
              " 'This is an installment in the notorious Guinea Pig series. A short lived japanese TV-show, that got cancelled after a psychopath admitted to being inspired in the killing of a young schoolgirl by the show. This short in the series is, like all the other films in the series, practically without any story. A group of guys have captured a young woman. They tie her down and proceeds to torturing her to death while videofilming her. They beat her, pour boiling oil over her, use pliers on her and finally, in \"loving\" closeup, push a needle through her eye. This is the most straightforward of all the Guinea Pig movies, and one of the first. It was probably this film, more than any of the others, that gave Guinea Pig the rumour of being snuff. They certainly gave inspiration to Nicolas Cage\\'s movie \"8 mm.\". These movies have gotten quite popular in horror circles. They have progressed to more polished, but equally graphic movies like \"Naked Blood\". They probably fill the void left by the Mondo movies, that got slightly cleaned up and became reality TV. Not recommended, but will probably allure those who will see anything once, and wonder why afterwards, I know I did.',\n",
              " 'Some of those guys that watch films and complain about them for a living are forgetting something: DVD menu system. I tell you the people, I watched the main screen repeat in this one about 35 times. It was awesome. A cinematic tapestry of cascading brilliance that had me from where it was, which was the very beginning. Many times the sum and Bam! I was hooked. Over and over and over. And over.\\n\\n\\n\\n\"Doot de doot, de doo de dodedo.\" And that\\'s just the soundtrack! \\n\\n\\n\\nI is laid aside in the bed, curled up with my Vaio. The rain is in the flat roof and tonight soft is again soft. The cat is comfortable and my ankle which crosses in me, is already rested. I popped in the DVD. I was mesmerized. Through the night. \"doot de doot, de doo de dodedo.\"\\n\\n\\n\\nThe Blob. See it. Steve Queen, two cops, and one girl in a dress. Two thumbs way up!',\n",
              " \"OK well i found this movie in my dads old pile of movies and it looked pretty good from the cover but the movie actually sucked!! OK the first story with the swimmer was pretty good but it took a while to get into, then the one with the boy was completely retarded! It wasn't even scary! His dream sounds like a little kid's bedtime story. Then the news girls one was completely retarded too. I'm sure someones going to call up the news guy and ask him to go out with you. But that one ended cool where she stabbed him and she was in the hospital and she saw him on t.v and he said all that junk to her. Next was that pretty gay story about the guy who brought back the dead people..OMG its so stupid I'm not even going to say any more about it.The last one was the best. It wasn't that scary but the idea of the story was pretty cool..uh yeah the girl gets possessed and she kills all her classmates or something. Then when they're all done telling their dreams to each other the losers get on the bus (TO HELL AHAHAHAH) and they see all the people from their dreams on the bus(Ha). The End.\",\n",
              " \"Seriously, the fact that this show is so popular just boggles the mind. This show isn't funny, it isn't clever, it isn't original, it's just a steaming pile of bull crap. Let me start with the characters. The characters are all one-dimensional morons with loud, exaggerated voices that just sound like fingernails on a blackboard. The voice acting could've been better. Then there's the animation. MY GOD, it hurts my eyes just looking at it. Everything is too flat, too pointy, too bright, and too candy coated. Then there's the humor, or lack thereof. It's completely idiotic! They just take these B-grade jokes that aren't even that funny in the first place and then repeat them to death. They also throw in some pointless potty humor which sickens me. And finally, last and least, the music. It's just plain annoying. It sounds like it was composed on a child's computer and generates no emotion whatsoever. I wish there was a score lower than 1, I really do. This show seriously needs to be canceled. It's a show I try to avoid like the plague. Whenever I hear the theme song I immediately turn the TV off. If you've never watched this show then don't. Watch quality programming like The Simpsons or Futurama.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "dVTb_Swc3e5j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load Textcat pipe train-dev data - LABELLED \n",
        "\n",
        "def load_textcat_data(limit=0):\n",
        "    \"\"\"Load data from the IMDB dataset.\"\"\"\n",
        "    # Partition off part of the train data for evaluation\n",
        "    train_data, eval_data = thinc.extra.datasets.imdb()\n",
        "    random.shuffle(train_data)\n",
        "    train_data = train_data[-limit:]\n",
        "    texts, labels = zip(*train_data)\n",
        "    eval_texts, eval_labels = zip(*eval_data)\n",
        "    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n",
        "    eval_cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in eval_labels]\n",
        "    return (texts, cats), (eval_texts, eval_cats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f_tXSiEw99jH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "temp_train, temp_eval = load_textcat_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YntOSg7w-CFf",
        "colab_type": "code",
        "outputId": "882abab7-9ebe-4b23-acde-61a10f7e757b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "# labels \n",
        "temp_train[1][:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'NEGATIVE': True, 'POSITIVE': False},\n",
              " {'NEGATIVE': True, 'POSITIVE': False},\n",
              " {'NEGATIVE': False, 'POSITIVE': True},\n",
              " {'NEGATIVE': False, 'POSITIVE': True},\n",
              " {'NEGATIVE': True, 'POSITIVE': False}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "XStVTuWLAdYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prefer_gpu():\n",
        "    used = spacy.util.use_gpu(0)\n",
        "    if used is None:\n",
        "        return False\n",
        "    else:\n",
        "        import cupy.random\n",
        "\n",
        "        cupy.random.seed(0)\n",
        "        return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "83rHr6K_AdxP",
        "colab_type": "code",
        "outputId": "e1e28563-7b38-44e2-91a0-1f95243efd3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "random.seed(0)\n",
        "numpy.random.seed(0)\n",
        "use_gpu = prefer_gpu()\n",
        "print(\"Using GPU?\", use_gpu)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qcHO0tDBGf0p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Textcat model construct\n",
        "\n",
        "def build_textcat_model(tok2vec, nr_class, width):\n",
        "    from thinc.v2v import Model, Softmax, Maxout\n",
        "    from thinc.api import flatten_add_lengths, chain\n",
        "    from thinc.t2v import Pooling, sum_pool, mean_pool, max_pool\n",
        "    from thinc.misc import Residual, LayerNorm\n",
        "    from spacy._ml import logistic, zero_init\n",
        "\n",
        "    with Model.define_operators({\">>\": chain}):\n",
        "        model = (\n",
        "            tok2vec\n",
        "            >> flatten_add_lengths\n",
        "            >> Pooling(mean_pool)\n",
        "            >> Softmax(nr_class, width)\n",
        "        )\n",
        "    model.tok2vec = tok2vec\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PeCqyM0SAfIi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create NLP or model object\n",
        "\n",
        "def create_pipeline(width, embed_size, vectors_model):\n",
        "    print(\"Load vectors\")\n",
        "    nlp = spacy.load(vectors_model)\n",
        "    print(\"Start training\")\n",
        "    textcat = TextCategorizer(\n",
        "        nlp.vocab,\n",
        "        labels=[\"POSITIVE\", \"NEGATIVE\"],\n",
        "        model=build_textcat_model(\n",
        "            Tok2Vec(width=width, embed_size=embed_size), 2, width\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    nlp.add_pipe(textcat)\n",
        "    return nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wAvbPAY6AkQF",
        "colab_type": "code",
        "outputId": "40f76005-1da1-44b4-f9f2-bee2de0892de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "nlp = create_pipeline(width=300, embed_size=7500, vectors_model='en')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load vectors\n",
            "Start training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oFu34FjFGKoN",
        "colab_type": "code",
        "outputId": "8ed58c7f-bcb1-46d7-f7da-9d001d9439c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "nlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7fa0e5ee5eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "ZXGt_C7qHUMZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# no idea what for this FN\n",
        "def block_gradients(model):\n",
        "    from thinc.api import wrap\n",
        "\n",
        "    def forward(X, drop=0.0):\n",
        "        Y, _ = model.begin_update(X, drop=drop)\n",
        "        return Y, None\n",
        "\n",
        "    return wrap(forward, model)\n",
        "\n",
        "# Main FN for pretraining \"tensorizer\" pipeline using texts\n",
        "def train_tensorizer(nlp, texts, dropout, n_iter):\n",
        "    tensorizer = nlp.create_pipe(\"tensorizer\")\n",
        "    nlp.add_pipe(tensorizer)\n",
        "    optimizer = nlp.begin_training()\n",
        "    for i in range(n_iter):\n",
        "        losses = {}\n",
        "        for i, batch in enumerate(minibatch(tqdm.tqdm(texts))):\n",
        "            docs = [nlp.make_doc(text) for text in batch]\n",
        "            tensorizer.update(docs, None, losses=losses, sgd=optimizer, drop=dropout)\n",
        "        print(losses)\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MR5yxqDIIzcf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For GPU support, we're grateful to use the work of Chainer's cupy module, which provides a numpy-compatible interface for GPU arrays. However, installing Chainer when no GPU is available currently causes an error. We therefore do not list Chainer as an explicit dependency — so building Thinc for GPU requires some extra steps:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xB-ypHy6I6uU",
        "colab_type": "code",
        "outputId": "7b6c71db-256d-468c-9882-46e25ae81f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mwww-2ctkatO",
        "colab_type": "code",
        "outputId": "a7042a18-bb64-49ea-9062-1894173f259c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "# Seems right version of CuPy needed\n",
        "pip install cupy-cuda100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cupy-cuda100 in /usr/local/lib/python3.6/dist-packages (5.2.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100) (1.11.0)\n",
            "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100) (0.4)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100) (1.16.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TJgho9pkJghO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Bunch of CLI for asserting the right CUDA for THINIC GPU implementation\n",
        "# Optional?? \n",
        "\n",
        "!ls /usr/local/cuda -a\n",
        "!export CUDA_HOME=/usr/local/cuda # Or wherever your CUDA is\n",
        "!export PATH=$PATH:$CUDA_HOME/bin\n",
        "!pip install chainer\n",
        "!python -c \"import cupy; assert cupy\" # Check it installed\n",
        "!pip install thinc_gpu_ops thinc # Or `thinc[cuda]`\n",
        "!python -c \"import thinc_gpu_ops\" # Check the GPU ops were built"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "chCdw7BPs8ww",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**ERROR**\n",
        "\n",
        "- CuPy dtype error\n",
        "  - seems to be Colab env-dep issues\n",
        "  - But dimension error still occurs \n",
        "  - Perhaps due to incorrect dimension or width and embed_size hyperparams \n",
        "  - These unknown as not given in GitHub source\n",
        "  \n",
        "- Solution\n",
        "  - Not able to use this snippet\n",
        "  - Resort to only TextCat training as above without pretrain this way\n",
        "  - Could still pretrain using CLI model method (see later)"
      ]
    },
    {
      "metadata": {
        "id": "MGfS-gdTAl7j",
        "colab_type": "code",
        "outputId": "2b9fc6b6-94df-4d13-96e2-f163b1aaa821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1012
        }
      },
      "cell_type": "code",
      "source": [
        "optimizer = train_tensorizer(nlp, temp_text, dropout=0.2, n_iter=pretrain_iters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50000 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-55e2632b262b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tensorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrain_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-e7596c89b67e>\u001b[0m in \u001b[0;36mtrain_tensorizer\u001b[0;34m(nlp, texts, dropout, n_iter)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mtensorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tensorizer.update\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/manipulation/join.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/manipulation/join.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/manipulation/dims.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \"\"\"\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_atleast_nd_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/manipulation/dims.py\u001b[0m in \u001b[0;36m_atleast_nd_helper\u001b[0;34m(n, arys)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mnew_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_atleast_nd_shape_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/creation/from_data.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unsupported dtype object"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "SGQlzohtk_Ry",
        "colab_type": "code",
        "outputId": "c5491a5f-c59e-461b-cc99-e28c2e967ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "# in case above failed on \"Unsupported dtype object\"\n",
        "# remove \"tensorizer\" from pipeline\n",
        "# redo above\n",
        "\n",
        "nlp.pipeline\n",
        "nlp.remove_pipe('tensorizer')\n",
        "nlp.pipeline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f12f504f860>),\n",
              " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f12f4f5fd68>),\n",
              " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f12f4f5fdc8>),\n",
              " ('textcat', <spacy.pipeline.pipes.TextCategorizer at 0x7f12ca155940>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "4BAjSr2aIyPM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_textcat(nlp, n_texts, n_iter=10):\n",
        "    textcat = nlp.get_pipe(\"textcat\")\n",
        "    tok2vec_weights = textcat.model.tok2vec.to_bytes()\n",
        "    (train_texts, train_cats), (dev_texts, dev_cats) = load_textcat_data(limit=n_texts)\n",
        "    print(\n",
        "        \"Using {} examples ({} training, {} evaluation)\".format(\n",
        "            n_texts, len(train_texts), len(dev_texts)\n",
        "        )\n",
        "    )\n",
        "    train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
        "        optimizer = nlp.begin_training()\n",
        "        textcat.model.tok2vec.from_bytes(tok2vec_weights)\n",
        "        print(\"Training the model...\")\n",
        "        print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
        "        for i in range(n_iter):\n",
        "            losses = {\"textcat\": 0.0}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            batches = minibatch(tqdm.tqdm(train_data), size=2)\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
        "            with textcat.model.use_params(optimizer.averages):\n",
        "                # evaluate on the dev data split off in load_data()\n",
        "                scores = evaluate_textcat(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
        "            print(\n",
        "                \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(  # print a simple table\n",
        "                    losses[\"textcat\"],\n",
        "                    scores[\"textcat_p\"],\n",
        "                    scores[\"textcat_r\"],\n",
        "                    scores[\"textcat_f\"],\n",
        "                )\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "795pzRNCKRuT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_textcat(tokenizer, textcat, texts, cats):\n",
        "    docs = (tokenizer(text) for text in texts)\n",
        "    tp = 1e-8\n",
        "    fp = 1e-8\n",
        "    tn = 1e-8\n",
        "    fn = 1e-8\n",
        "    for i, doc in enumerate(textcat.pipe(docs)):\n",
        "        gold = cats[i]\n",
        "        for label, score in doc.cats.items():\n",
        "            if label not in gold:\n",
        "                continue\n",
        "            if score >= 0.5 and gold[label] >= 0.5:\n",
        "                tp += 1.0\n",
        "            elif score >= 0.5 and gold[label] < 0.5:\n",
        "                fp += 1.0\n",
        "            elif score < 0.5 and gold[label] < 0.5:\n",
        "                tn += 1\n",
        "            elif score < 0.5 and gold[label] >= 0.5:\n",
        "                fn += 1\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AuM5OxqzKY-Q",
        "colab_type": "code",
        "outputId": "fd2c92f5-4a9a-4131-e51d-67fe131b7e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1105
        }
      },
      "cell_type": "code",
      "source": [
        "train_textcat(nlp, train_examples, n_iter=train_iters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 1000 examples (1000 training, 25000 evaluation)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "LOSS \t  P  \t  R  \t  F  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:33<00:00, 34.55it/s]\n",
            "  0%|          | 4/1000 [00:00<00:30, 32.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "64.268\t0.728\t0.728\t0.728\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 34.93it/s]\n",
            "  0%|          | 4/1000 [00:00<00:28, 34.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "35.027\t0.770\t0.770\t0.770\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 34.88it/s]\n",
            "  0%|          | 4/1000 [00:00<00:28, 35.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13.984\t0.770\t0.770\t0.770\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 34.20it/s]\n",
            "  0%|          | 4/1000 [00:00<00:29, 33.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4.842\t0.766\t0.766\t0.766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 34.99it/s]\n",
            "  0%|          | 4/1000 [00:00<00:32, 30.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5.686\t0.767\t0.767\t0.767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:30<00:00, 33.11it/s]\n",
            "  0%|          | 4/1000 [00:00<00:28, 35.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3.089\t0.767\t0.767\t0.767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 34.44it/s]\n",
            "  0%|          | 4/1000 [00:00<00:27, 35.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.700\t0.766\t0.766\t0.766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:28<00:00, 34.72it/s]\n",
            "  0%|          | 4/1000 [00:00<00:27, 35.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.260\t0.767\t0.767\t0.767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:28<00:00, 35.56it/s]\n",
            "  0%|          | 4/1000 [00:00<00:29, 34.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.039\t0.769\t0.769\t0.769\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 33.53it/s]\n",
            "  0%|          | 4/1000 [00:00<00:31, 32.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.055\t0.767\t0.767\t0.767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:28<00:00, 34.68it/s]\n",
            "  0%|          | 4/1000 [00:00<00:30, 32.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.283\t0.766\t0.766\t0.766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 33.89it/s]\n",
            "  0%|          | 4/1000 [00:00<00:27, 35.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.133\t0.763\t0.763\t0.763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 33.53it/s]\n",
            "  0%|          | 4/1000 [00:00<00:28, 35.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.000\t0.762\t0.762\t0.762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:30<00:00, 32.50it/s]\n",
            "  0%|          | 4/1000 [00:00<00:28, 35.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.532\t0.763\t0.763\t0.763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 33.54it/s]\n",
            "  0%|          | 4/1000 [00:00<00:27, 35.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.530\t0.764\t0.764\t0.764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 33.52it/s]\n",
            "  0%|          | 4/1000 [00:00<00:29, 34.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.864\t0.763\t0.763\t0.763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 33.56it/s]\n",
            "  0%|          | 4/1000 [00:00<00:28, 35.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.946\t0.763\t0.763\t0.763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:30<00:00, 33.02it/s]\n",
            "  0%|          | 4/1000 [00:00<00:28, 35.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.888\t0.763\t0.763\t0.763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 34.06it/s]\n",
            "  0%|          | 4/1000 [00:00<00:29, 34.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.233\t0.763\t0.763\t0.763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:28<00:00, 34.53it/s]\n",
            "  0%|          | 4/1000 [00:00<00:28, 34.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.355\t0.763\t0.763\t0.763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 35.41it/s]\n",
            "  0%|          | 4/1000 [00:00<00:27, 35.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.162\t0.764\t0.764\t0.764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 33.76it/s]\n",
            "  0%|          | 4/1000 [00:00<00:29, 34.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.624\t0.764\t0.764\t0.764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:28<00:00, 35.65it/s]\n",
            "  0%|          | 4/1000 [00:00<00:28, 35.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.251\t0.766\t0.766\t0.766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:28<00:00, 35.99it/s]\n",
            "  0%|          | 4/1000 [00:00<00:27, 35.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.028\t0.765\t0.765\t0.765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:28<00:00, 34.81it/s]\n",
            "  0%|          | 4/1000 [00:00<00:30, 32.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.879\t0.764\t0.764\t0.764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:28<00:00, 35.65it/s]\n",
            "  0%|          | 4/1000 [00:00<00:27, 35.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.419\t0.763\t0.763\t0.763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:28<00:00, 34.83it/s]\n",
            "  0%|          | 4/1000 [00:00<00:27, 35.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.702\t0.762\t0.762\t0.762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:28<00:00, 34.78it/s]\n",
            "  0%|          | 4/1000 [00:00<00:31, 31.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.412\t0.761\t0.761\t0.761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:28<00:00, 34.53it/s]\n",
            "  0%|          | 4/1000 [00:00<00:30, 32.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.339\t0.762\t0.762\t0.762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 35.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.509\t0.763\t0.763\t0.763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "owXwYooZ98Xu",
        "colab_type": "code",
        "outputId": "595c1851-a1c2-4ec0-edf5-db00e368dc42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1185
        }
      },
      "cell_type": "code",
      "source": [
        "    # test the trained model\n",
        "    test_text = \"This movie sucked\"\n",
        "    doc = nlp(test_text)\n",
        "    print(test_text, doc.cats)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-2dcfa44b8d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This movie sucked\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tensorizer.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tensorizer.predict\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \"\"\"\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/check.py\u001b[0m in \u001b[0;36mchecked_function\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mExpectedTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Callable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0marg_check_adder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/affine.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input__BI)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput__BI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mops.pyx\u001b[0m in \u001b[0;36mthinc.neural.ops.CupyOps.gemm\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/linalg/product.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# TODO(okuta): check type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.__array__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.astype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy.core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/util.pyx\u001b[0m in \u001b[0;36mcupy.util.memoize.decorator.ret\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy.core._kernel._get_ufunc_kernel\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy.core._kernel._get_kernel_params\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/core/_scalar.pyx\u001b[0m in \u001b[0;36mcupy.core._scalar.get_typename\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/core/_scalar.pyx\u001b[0m in \u001b[0;36mcupy.core._scalar.get_typename\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: <class 'numpy.object_'>"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ra6mvnwbcxlp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Snippet:  2.0 custom pipelines and extensions\n",
        "\n",
        "> Available: download, link, info, train, evaluate, convert, package,\n",
        "    vocab, init-model, profile, validate"
      ]
    },
    {
      "metadata": {
        "id": "YC7wg1ZNAO6J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cLwAmN64AX8E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Doc.set_extension('is_greeting', default=False)\n",
        "nlp = spacy.load('en')\n",
        "doc = nlp(u'hello world')\n",
        "doc._.doc_extensions\n",
        "\n",
        "# ._ create extensibility and distinction to built-ins, code-break resilient upon update\n",
        "doc._.is_greeting = True "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0CP__iumA_8h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Customise Processing Pipeline (same nlp() as above)\n",
        "\n",
        "component = MyComponent() # See below for INIT\n",
        "\n",
        "nlp.add_pipe(component, after='tagger')\n",
        "\n",
        "doc = nlp(u'This is a sentence')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UECqAGG5CEDJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The nlp object is an instance of Language, which contains the data and annotation scheme of the language you're using and a pre-defined pipeline of components, like the tagger, parser and entity recognizer. If you're loading a model, the Language instance also has access to the model's binary data. All of this is specific to each model, and defined in the model's meta.json – for example, a Spanish NER model requires different weights, language data and pipeline components than an English parsing and tagging model. This is also why the pipeline state is always held by the Language class. spacy.load() puts this all together and returns an instance of Language with a pipeline set and access to the binary data.**\n",
        "\n",
        "```python\n",
        "doc = nlp.make_doc(u'This is a sentence')   # create a Doc from raw text\n",
        "for name, proc in nlp.pipeline:             # iterate over components in order\n",
        "    doc = proc(doc)                         # call each component on the Doc\n",
        "```\n",
        "\n",
        "**spaCy 2.0 simply list of (name, function) tuple**\n",
        "\n",
        "```python\n",
        "nlp.pipeline\n",
        "[('tagger', <spacy.pipeline.Tagger>), ('parser', <spacy.pipeline.DependencyParser>),\n",
        " ('ner', <spacy.pipeline.EntityRecognizer>)]\n",
        "```\n",
        "\n",
        "To make it more convenient to modify the pipeline, there are several built-in methods to get, add, replace, rename or remove individual components. spaCy's default pipeline components, like the tagger, parser and entity recognizer now all follow the same, consistent API and are subclasses of `Pipe`. If you're developing your own component, using the Pipe API will make it fully trainable and serializable. At a minimum, a component needs to be a callable that takes a Doc and returns it:\n",
        "\n",
        "```python\n",
        "def my_component(doc):\n",
        "    print(\"The doc is {} characters long and has {} tokens.\"\n",
        "          .format(len(doc.text), len(doc))\n",
        "    return doc\n",
        "```\n",
        "\n",
        "The component can then be added at any position of the pipeline using the `nlp.add_pipe()` method. The arguments `before, after, first, and last` let you specify component names to insert the new component before or after, or tell spaCy to insert it first (i.e. directly after tokenization) or last in the pipeline.\n",
        "\n",
        "```python\n",
        "nlp = spacy.load('en')\n",
        "nlp.add_pipe(my_component, name='print_length', last=True)\n",
        "doc = nlp(u\"This is a sentence.\")\n",
        "```\n",
        "\n",
        "**Extension attributes on Doc, Token and Span**\n",
        "\n",
        "When you implement your own pipeline components that modify the `Doc`, you often want to extend the API, so that the information you're adding is conveniently accessible. spaCy v2.0 introduces a new mechanism that lets you register your own attributes, properties and methods that become available in the `._` namespace, for example, `doc._.my_attr`. There are mostly three types of extensions that can be registered via the `set_extension()`` method:\n",
        "**Why ._?**\n",
        "Writing to a ._ attribute instead of to the Doc directly keeps a clearer separation and makes it easier to ensure backwards compatibility. For example, if you've implemented your own .coref property and spaCy claims it one day, it'll break your code. Similarly, just by looking at the code, you'll immediately know what's built-in and what's custom – for example, doc.sentiment is spaCy, while doc._.sent_score isn't.\n",
        "\n",
        "1. Attribute extensions. Set a default value for an attribute, which can be overwritten.\n",
        "2. Property extensions. Define a `getter` and an optional `setter` function.\n",
        "3. Method extensions. Assign a function that becomes available as an object method.\n",
        "\n",
        "```python\n",
        "Doc.set_extension('hello_attr', default=True)\n",
        "Doc.set_extension('hello_property', getter=get_value, setter=set_value)\n",
        "Doc.set_extension('hello_method', method=lambda doc, name: 'Hi {}!'.format(name))\n",
        "\n",
        "doc._.hello_attr            # True\n",
        "doc._.hello_property        # return value of get_value\n",
        "doc._.hello_method('Ines')  # 'Hi Ines!'\n",
        "```\n",
        "\n",
        "**WHY Extensions?**\n",
        "\n",
        "Being able to easily write custom data to the `Doc, Token and Span` means that applications using spaCy can take full advantage of the built-in data structures and the benefits of Doc objects as the **single source of truth** containing all information:\n",
        "\n",
        "- No information is lost during tokenization and parsing, so you can always relate annotations to the original string.\n",
        "- The Token and Span are views of the Doc, so they're always up-to-date and consistent.\n",
        "- Efficient C-level access is available to the underlying TokenC* array via doc.c.\n",
        "- APIs can standardise on passing around Doc objects, reading and writing from them whenever necessary. Fewer signatures makes functions more reusable and composable.\n",
        "\n",
        "**TODO - learn these examples of custom componets**\n",
        "\n",
        "- https://explosion.ai/blog/spacy-v2-pipelines-extensions\n",
        "- https://github.com/explosion/spaCy/blob/develop/examples/pipeline/custom_component_countries_api.py\n",
        "- https://github.com/explosion/spaCy/blob/develop/examples/pipeline/custom_component_entities.py\n",
        "- https://github.com/explosion/spaCy/blob/develop/examples/pipeline/custom_attr_methods.py\n",
        "- https://github.com/explosion/spaCy/blob/develop/examples/pipeline/custom_sentence_segmentation.py\n",
        "- https://github.com/explosion/spaCy/blob/develop/examples/pipeline/fix_space_entities.py\n",
        "- https://github.com/explosion/spaCy/blob/develop/examples/pipeline/multi_processing.py"
      ]
    },
    {
      "metadata": {
        "id": "jgm6quvEEYJ9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Snippet: 2.1 Pretrain Overview (experimental)\n",
        "\n",
        "Scaling down these language models to the sizes we use in spaCy posed an interesting research challenge. Language models typically use a large output layer, with one neuron per word in the vocabulary. If you're predicting over a 10,000 word vocabulary, this means you're predicting a vector with 10,000 elements. spaCy v2.1's token vectors are 96 elements wide, so a naive softmax approach would be unlikely to work: we'd be trying to predict 100 elements of output for every 1 element of input. We could make the vocabulary somewhat smaller, but every word that's out of vocabulary is a word the pretraining process will be unable to learn. Stepping back a little, the problem of so-called \"one hot\" representations posing representational issues for neural networks is actually quite familiar. This is exactly what algorithms like word2vec, GloVe and FastText set out to solve. Instead of a binary vector with one dimension per entry in the vocabulary, we can have a much denser real-valued representation of the same information.\n",
        "\n",
        "> The spacy pretrain command requires a **word vectors model as part of the input**, which it uses as the target output for each token. Instead of predicting a token's ID as a classification problem, we learn to predict the token's word vector. Inspired by names such as ELMo and BERT, we've termed this trick Language Modelling with Approximate Outputs (LMAO). Our first implementation is probably a good way to get acquainted with the idea – it's extremely short.\n",
        "\n",
        "As is often the case in research, it seems that LMAO is an idea whose time had come. Several other researchers have been working on related ideas independently. So far we've been using L2 loss in our experiments, but Kumar and Tsvetkov (2018), who were simultaneously working on a similar idea for machine translation, have developed a novel probabilistic loss using the von Mises-Fisher distribution, which they show performs significantly better than L2 in their experiments. Even more recently, Li et al. (2019) report experiments using an LMAO objective in place of the softmax layer in the ELMo pretraining system, with promising results. In our own preliminary experiments, we've found pretraining especially effective when limited training data is available. It helps most for text categorization and parsing, but is less effective for named entity recognition. We expect the pretraining to be increasingly important as we add more abstract semantic prediction models to spaCy, for tasks such as semantic role labelling, coreference resolution and named entity linking."
      ]
    },
    {
      "metadata": {
        "id": "2qHTfvwTANx2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Official: Pretrain 100,000 Reddit Comments on Core_sm and Core_lg\n",
        "\n",
        "```shell\n",
        "# Pretrain for the en_core_web_sm model. The sm model doesn't require the word vectors\n",
        "# at runtime, while the lg model does.\n",
        "python -m spacy pretrain /input/reddit-100k.jsonl en_vectors_web_lg /output\n",
        "\n",
        "# Pretrain for the en_core_web_lg model\n",
        "python -m spacy pretrain /input/reddit-100k.jsonl en_vectors_web_lg /output --use-vectors\n",
        "```\n",
        "\n",
        "We ran both pretraining jobs simultaneously on a Tesla V100, with each task training at around 50,000 tokens per second. We pretrained for 3 billion words (making several passes over the 100k comments), which took around 17 hours. The total cost of both jobs came out to about 40USD on Google Compute Engine. We haven't implemented resume logic yet, which will help decrease the cost of large scale jobs further, as it would allow the use of pre-emptible instances. This would take pretraining costs down to around 4USD per billion words of training. The spacy pretrain command saves out a weights file after each pass over the data. To use the pretrained weights, we can simply pass them as an argument to spacy train\n",
        "\n",
        "\n",
        "```shell\n",
        "python -m spacy train en /models/ /corpora/PTB_SD_3_3_0/train.gold.json\n",
        "/corpora/PTB_SD_3_3_0/dev.gold.json --n-examples 100 --pipeline parser\n",
        "--init-tok2vec pretrain-nv-model999.bin\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "5Q9hDhXyiyVZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SqdU4WOthEQk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Honnibal: How to Use Pretain resulting .bin weights\n",
        "\n",
        "If you're using the spacy train command, you can pass the trained weights with the -t2v argument. Here's an example command: \n",
        "\n",
        "```shell\n",
        "python -m spacy train en tmp/ ~/data/PTB_SD_3_3_0/train.gold.json ~/data/PTB_SD_3_3_0/dev.gold.json --pipeline parser -t2v nv-cosine-reddit17-99.bin\n",
        "```\n",
        "\n",
        "More fundamentally, the weights files being produced by the spacy pretrain command are the result of calling something like **textcat.model.tok2vec.to_bytes().** There's a couple of things to keep in mind about this. Let's say you first create a blank **TextCategorizer** object, like this:\n",
        "\n",
        "```python\n",
        "textcat = nlp.create_pipe(\"textcat\")\n",
        "```\n",
        "\n",
        "The new textcat object will start out with **textcat.model == True**. So, you won't immediately be able to call something like **textcat.model.tok2vec**. You need to create a model object first, which usually happens during the call to **nlp.begin_training()**. So, somewhere in your script after the call to begin_training(), but before you start making updates, you can add lines like this:\n",
        "\n",
        "```python\n",
        "with open(\"/path/to/model999.bin\", \"rb\") as file_:\n",
        "    textcat.model.tok2vec.from_bytes(file_.read())\n",
        "```\n",
        "\n",
        "NOTE: One more thing to be aware of. You need to make sure that the textcat.model.tok2vec instance you're loading the weights into has the same architecture and hyper-parameters as the model you used during pre-training. With the default textcat hyper-parameters, that's not quite true. **The easiest solution is to pass the flag architecture=\"simple_cnn\".**\n",
        "\n",
        "```python\n",
        "# full code\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "textcat = nlp.create_pipe(\"textcat\", config={\"architecture\": \"simple_cnn\", \"exclusive_classes\": True}))\n",
        "textcat.add_label(\"LABEL1\")\n",
        "textcat.add_label(\"LABEL2\")\n",
        "# Alternatively, instead of adding all your labels explicitly, you could pass all your examples\n",
        "# into textcat.begin_training, like this: textcat.begin_training(get_gold_tuples=lambda: my_data)\n",
        "# It's fine to add the labels and not pass in the data, though. The nlp.begin_training() method will\n",
        "# work the same as well, if you have other components in your pipeline you want to train.\n",
        "optimizer = textcat.begin_training()\n",
        "# Now that we have our model, we can load in the pretrained weights.\n",
        "with open(path_to_pretrained_weights, \"rb\") as file_:\n",
        "    textcat.model.tok2vec.from_bytes(file_.read())\n",
        "# Now we can proceed with training\n",
        "for epoch in range(nr_epoch):\n",
        "    random.shuffle(train_data)\n",
        "    for batch in minibatch(train_data, size=batch_size):\n",
        "        X, y = zip(*batch)\n",
        "        textcat.update(X, y, sgd=optimizer)\n",
        "```\n",
        "\n",
        "**NOTE LIMITATION of LMAO** by Honnibal\n",
        "\n",
        "> For dialogue generation, I'm actually not sure the spacy pretrain command will perform well. One weakness of the \"LMAO\" trick we're using is that at each word, the model only gets to predict one vector. Normally, a language model will be predicting a probability distribution over the words in the vocabulary. We're only predicting a single point in vector-space. For the purpose of getting a token representation, I think the LMAO trade-off is pretty good. But if you really do want to generate words, it's probably not so good."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KV_VPA80hLgt"
      },
      "cell_type": "markdown",
      "source": [
        "### DEMO: PRETRAIN (CLI)\n",
        "\n",
        "- See above 2.1 snippet for example\n",
        "\n",
        "- Input\n",
        "  - raw text in JOSNL format (see separate note)\n",
        "  - language model (pure vector.txt may not work?? tutorials are all based on spaCy-style model files)\n",
        "  - parameters\n",
        "  \n",
        "```python\n",
        "import srsly\n",
        "data = [{\"text\": \"Some text\"}, {\"text\": \"More...\"}]\n",
        "srsly.write_jsonl(\"/path/to/text.jsonl\", data)\n",
        "```\n",
        "\n",
        "**HONNIBAL NOTE**\n",
        "https://github.com/explosion/spaCy/issues/3448#issuecomment-475046413\n",
        "\n",
        "**Excellent Tutorial**\n",
        "https://tienduccao.github.io/posts/spacy_pretrain/"
      ]
    },
    {
      "metadata": {
        "id": "Y-d2PKmaj_TN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**generate_josonl.py** # handy csv-jsonl converter\n",
        "\n",
        "```python\n",
        "with open('corpus.jsonl', 'a+') as out:\n",
        "    with open('dataset.csv') as f:\n",
        "        for line in f.read().splitlines():\n",
        "            sentence = line[line.index(',') + 1:]\n",
        "            sentence = sentence.replace('\"', '')\n",
        "            out.write('{\"text\": ' + '\"' + sentence + '\"}\\n')\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "1IMhw64Kmw8L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test using IMDB raw text to pretran \"en_core_web_sm\"\n",
        "\n",
        "# First convert text into right JSONL for `spacy pretrain`\n",
        "\n",
        "# method-1 using above script (need editing)\n",
        "\n",
        "with open('imdb.jsonl', 'a+') as out:\n",
        "    with open('imdb_master.csv', encoding='latin-1') as f:\n",
        "        for line in f.read().splitlines():\n",
        "            sentence = line[line.index(',') + 1:]\n",
        "            sentence = sentence.replace('\"', '')\n",
        "            out.write('{\"text\": ' + '\"' + sentence + '\"}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lY9_5EebpUZu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# method-2 using spacy srsly\n",
        "import pandas as pd\n",
        "\n",
        "imdb_df = pd.read_csv('imdb_master.csv', usecols=[1,2,3], encoding='latin-1')\n",
        "imdb_df.head()\n",
        "import srsly\n",
        "imdb_jsonl = [{\"text\": sent} for sent in imdb_df['review'].values]\n",
        "imdb_jsonl[:5]\n",
        "srsly.write_jsonl(\"imdb_jsonl.jsonl\", imdb_jsonl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jpqqS90whLhE"
      },
      "cell_type": "markdown",
      "source": [
        "**BELOW PRETRAIN OOM**\n",
        "\n",
        "- Honnibal explained to a OOM problem\n",
        "\n",
        "> Unfortunately a single 130kb text could be causing OOM on your card. The vectors are already take 1gb, and the intermediate representations can get pretty big. For instance, a single convolutional layer has to build a matrix of shape (n, 3, 3, 96) where n is the number of words in the batch. There are several of these layers, and then the parser's hidden layer ends up large as well.\n",
        "\n",
        "> There are lots of places in the forward pass I think I can save memory if the backward pass isn't being run. But currently it can take a surprising amount of temporary memory during parsing. This is especially true on GPU, as the cupy library has its own allocator."
      ]
    },
    {
      "metadata": {
        "id": "ZHurrBxbAn9l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# need nightly\n",
        "!pip install spacy-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Clh2PPAZnjz_",
        "colab_type": "code",
        "outputId": "bad0e967-8195-4c4d-800d-9ef05b37bd5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_vectors_web_lg # lg has 1m vectors"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_vectors_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.0.0/en_vectors_web_lg-2.0.0.tar.gz#egg=en_vectors_web_lg==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.0.0/en_vectors_web_lg-2.0.0.tar.gz (661.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 661.8MB 59.2MB/s \n",
            "\u001b[?25hInstalling collected packages: en-vectors-web-lg\n",
            "  Running setup.py install for en-vectors-web-lg ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed en-vectors-web-lg-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_vectors_web_lg -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_vectors_web_lg\n",
            "\n",
            "    You can now load the model via spacy.load('en_vectors_web_lg')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a1YEZr0SAmKu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy validate # check version of vector model compatibility to pretrain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "37f9db2d-3de0-4129-b650-590e0c5405d0",
        "id": "PsafIsI-hLhH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy pretrain imdb_jsonl.jsonl en_vectors_web_lg weights "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using GPU\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved settings to config.json\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m✔ Loaded input texts\u001b[0m\n",
            "⠙ Loading model 'en_vectors_web_lg'...tcmalloc: large alloc 1285169152 bytes == 0x27944000 @  0x7fe0eb1711e7 0x7fe0e8cfbe51 0x7fe0e8d65b25 0x7fe0e8d666be 0x7fe0e8dff6ee 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x506369 0x7fe058245bc8 0x7fe05825338d 0x5a730c 0x503073 0x506859 0x7fe0582437e0 0x7fe058245cdc 0x7fe0582521c3 0x566103 0x7fe0594f871d 0x5030d5 0x506859 0x504c28 0x502540\n",
            "⠴ Loading model 'en_vectors_web_lg'...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/__main__.py\", line 38, in <module>\n",
            "    plac.call(commands[command], sys.argv[1:])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/plac_core.py\", line 328, in call\n",
            "    cmd, result = parser.consume(arglist)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/plac_core.py\", line 207, in consume\n",
            "    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/cli/pretrain.py\", line 90, in pretrain\n",
            "    nlp = util.load_model(vectors_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/util.py\", line 129, in load_model\n",
            "    return load_model_from_link(name, **overrides)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/util.py\", line 146, in load_model_from_link\n",
            "    return cls.load(**overrides)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/data/en_vectors_web_lg/__init__.py\", line 12, in load\n",
            "    return load_model_from_init_py(__file__, **overrides)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/util.py\", line 190, in load_model_from_init_py\n",
            "    return load_model_from_path(data_path, meta, **overrides)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/util.py\", line 173, in load_model_from_path\n",
            "    return nlp.from_disk(model_path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/language.py\", line 796, in from_disk\n",
            "    util.from_disk(path, deserializers, exclude)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/util.py\", line 611, in from_disk\n",
            "    reader(path / key)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/language.py\", line 786, in <lambda>\n",
            "    deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(p, exclude=[\"vocab\"])\n",
            "  File \"tokenizer.pyx\", line 387, in spacy.tokenizer.Tokenizer.from_disk\n",
            "  File \"tokenizer.pyx\", line 430, in spacy.tokenizer.Tokenizer.from_bytes\n",
            "  File \"/usr/lib/python3.6/re.py\", line 233, in compile\n",
            "    return _compile(pattern, flags)\n",
            "  File \"/usr/lib/python3.6/re.py\", line 301, in _compile\n",
            "    p = sre_compile.compile(pattern, flags)\n",
            "  File \"/usr/lib/python3.6/sre_compile.py\", line 562, in compile\n",
            "    p = sre_parse.parse(p, flags)\n",
            "  File \"/usr/lib/python3.6/sre_parse.py\", line 855, in parse\n",
            "    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)\n",
            "  File \"/usr/lib/python3.6/sre_parse.py\", line 416, in _parse_sub\n",
            "    not nested and not items))\n",
            "  File \"/usr/lib/python3.6/sre_parse.py\", line 527, in _parse\n",
            "    code1 = _class_escape(source, this)\n",
            "  File \"/usr/lib/python3.6/sre_parse.py\", line 336, in _class_escape\n",
            "    raise source.error('bad escape %s' % escape, len(escape))\n",
            "sre_constants.error: bad escape \\p at position 173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5ckBCGgSgyJJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4RhLsVzMjY-t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PDo4DkiaA335",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t1jKJQvwgyNT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jysAPstGwfGq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# DEMO\n",
        "\n",
        "**Key demo (Norwagian language model creation) https://github.com/explosion/spaCy/issues/3082**"
      ]
    },
    {
      "metadata": {
        "id": "PVbqv-yawrTx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TRAIN Language Model\n",
        "https://spacy.io/usage/training\n",
        "\n",
        "Flow of Training\n",
        "- Creating a vocabulary file\n",
        "  - spaCy expects that common words will be cached in a Vocab instance. The vocabulary caches lexical features. spaCy loads the vocabulary from binary data, in order to keep loading efficient. The easiest way to save out a new binary vocabulary file is to use the spacy init-model command, which expects a JSONL file with words and their lexical attributes. See the docs on the vocab JSONL format for details.\n",
        "- Training the word vectors\n",
        "  - Word2vec and related algorithms let you train useful word similarity models from unlabeled text. This is a key part of using deep learning for NLP with limited labeled data. The vectors are also useful by themselves – they power the .similarity methods in spaCy. For best results, you should pre-process the text with spaCy before training the Word2vec model. This ensures your tokenization will match. You can use our word vectors training script, which pre-processes the text with your language-specific tokenizer and trains the model using Gensim. The vectors.bin file should consist of one word and vector per line.\n",
        "  - https://github.com/explosion/spacy/tree/master/bin/train_word_vectors.py\n",
        "  - If you don’t have a large sample of text available, you can also convert word vectors produced by a variety of other tools into spaCy’s format. See the docs on converting word vectors for details.\n",
        "- Creating or converting a training corpus\n",
        "  - The easiest way to train spaCy’s tagger, parser, entity recognizer or text categorizer is to use the spacy train command-line utility. In order to use this, you’ll need training and evaluation data in the JSON format spaCy expects for training.\n",
        "  - You can now train the model using a corpus for your language annotated with If your data is in one of the supported formats, the easiest solution might be to use the spacy convert command-line utility. This supports several popular formats, including the IOB format for named entity recognition, the JSONL format produced by our annotation tool Prodigy, and the CoNLL-U format used by the Universal Dependencies corpus.\n",
        "  - One thing to keep in mind is that spaCy expects to train its models from whole documents, not just single sentences. If your corpus only contains single sentences, spaCy’s models will never learn to expect multi-sentence documents, leading to low performance on real text. To mitigate this problem, you can use the -N argument to the spacy convert command, to merge some of the sentences into longer pseudo-documents.\n",
        "- Training the tagger and parser\n",
        "  - Once you have your training and evaluation data in the format spaCy expects, you can train your model use the using spaCy’s train command. Note that training statistical models still involves a degree of trial-and-error. You may need to tune one or more settings, also called “hyper-parameters”, to achieve optimal performance. See the usage guide on training for more details.\n",
        "  \n",
        "\n",
        "\n",
        "1. From scratch \n",
        "2. Update on existing model\n",
        "\n",
        "\n",
        "> Both can be preceded by **Pretrain**"
      ]
    },
    {
      "metadata": {
        "id": "1hAS8cb7pXOw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (1) From Scratch (CLI or Code)\n",
        "\n",
        "**CLI method**\n",
        "- Input\n",
        "  - **Annotated format - supports several popular formats, including the IOB format for named entity recognition, the JSONL format produced by our annotation tool Prodigy, and the CoNLL-U format used by the Universal Dependencies corpus.**\n",
        "  - `spacy convert` into spaCy JSON format\n",
        "- Example:\n",
        "\n",
        "```shell\n",
        "git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora\n",
        "mkdir ancora-json\n",
        "python -m spacy convert UD_Spanish-AnCora/es_ancora-ud-train.conllu ancora-json\n",
        "python -m spacy convert UD_Spanish-AnCora/es_ancora-ud-dev.conllu ancora-json\n",
        "mkdir models\n",
        "python -m spacy train es models ancora-json/es_ancora-ud-train.json ancora-json/es_ancora-ud-dev.json\n",
        "```\n",
        "\n",
        "**Simple code method (Preferred)**\n",
        "\n",
        "> Instead of sequences of `Doc and GoldParse` objects, you can also use the “simple training style” and **pass raw texts and dictionaries of annotations to nlp.update.** The dictionaries can have the **keys entities, heads, deps, tags and cats.** This is generally recommended, as it removes one layer of abstraction, and avoids unnecessary imports. It also makes it easier to structure and load your training data.\n",
        "\n",
        "- Example Annotations\n",
        "\n",
        "```json\n",
        "{\n",
        "   \"entities\": [(0, 4, \"ORG\")],\n",
        "   \"heads\": [1, 1, 1, 5, 5, 2, 7, 5],\n",
        "   \"deps\": [\"nsubj\", \"ROOT\", \"prt\", \"quantmod\", \"compound\", \"pobj\", \"det\", \"npadvmod\"],\n",
        "   \"tags\": [\"PROPN\", \"VERB\", \"ADP\", \"SYM\", \"NUM\", \"NUM\", \"DET\", \"NOUN\"],\n",
        "   \"cats\": {\"BUSINESS\": 1.0},\n",
        "}\n",
        "```\n",
        "\n",
        "- Simple Training Loop\n",
        "\n",
        "```python\n",
        "TRAIN_DATA = [\n",
        "        (u\"Uber blew through $1 million a week\", {\"entities\": [(0, 4, \"ORG\")]}),\n",
        "        (u\"Google rebrands its business apps\", {\"entities\": [(0, 6, \"ORG\")]})]\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "optimizer = nlp.begin_training()\n",
        "for i in range(20):\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    for text, annotations in TRAIN_DATA:\n",
        "        nlp.update([text], [annotations], sgd=optimizer)\n",
        "nlp.to_disk(\"/model\")\n",
        "```\n",
        "\n",
        "> The above training loop leaves out a few details that can really improve accuracy – but the principle really is that simple. Once you’ve got your pipeline together and you want to tune the accuracy, you usually want to process your training examples in batches, and experiment with minibatch sizes and dropout rates, set via the drop keyword argument. See the Language and Pipe API docs for available options.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mKozDiPt0YlY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### NER\n"
      ]
    },
    {
      "metadata": {
        "id": "WIFzfYh22D1h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**(1) BUILT-IN ENTITY**\n",
        "\n",
        "**Blank Model or Load Built-in**\n",
        "\n",
        "**(2) CUSTOM ENTITY**\n",
        "\n",
        "**Training an additional entity type** \\\n",
        "\n",
        "> **In practice, you’ll need many more — a few hundred would be a good start. You will also likely need to mix in examples of other entity types, which might be obtained by running the entity recognizer over unlabelled sentences, and adding their annotations to the training set.**"
      ]
    },
    {
      "metadata": {
        "id": "9HeNfevV1dLB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training text\n",
        "\n",
        "# Sample article from BBC\n",
        "\n",
        "from goose3 import Goose"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uRXGwwgX4Qjb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = Goose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pAvou09s4Yum",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "article = g.extract(url='https://www.bbc.com/news/world-africa-47997729')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zhzrfGWF4g8W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clean_text = article.cleaned_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qWQifCFd4mBc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Using sentencizer component (rule-based matcher)\n",
        "\n",
        "sent_nlp = spacy.load('en_core_web_sm')\n",
        "sentencizer = sent_nlp.create_pipe('sentencizer')\n",
        "sent_nlp.add_pipe(sentencizer)\n",
        "sent_doc = sent_nlp(clean_text)\n",
        "sent_text = [sent for sent in sent_doc.sents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z037Wn0R6YD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d8131077-bc5a-4034-e49d-dbd89cff12df"
      },
      "cell_type": "code",
      "source": [
        "sent_text[0].text"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A large hoard of cash has been found at the home of Sudan's ousted president Omar al-Bashir and he is now being investigated for money laundering, prosecutors say.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "ZlPhhk-M6x_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sent1 = sent_text[0].text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Snwdr8kB8YEY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "b96442f2-3f0c-41fa-d9b9-5cdd8736e65f"
      },
      "cell_type": "code",
      "source": [
        "# finding loc of words\n",
        "sent1.find('Sudan') # start \n",
        "print()\n",
        "sent1.find('Sudan') + len('Sudan') # end\n",
        "print()\n",
        "\n",
        "# finding loc of words\n",
        "sent1.find('Omar al-Bashir') # start \n",
        "print()\n",
        "sent1.find('Omar al-Bashir') + len('Omar al-Bashir') # end\n",
        "print()\n",
        "\n",
        "# finding loc of words\n",
        "sent1.find('cash') # start \n",
        "print()\n",
        "sent1.find('cash') + len('cash') # end\n",
        "print()\n",
        "\n",
        "# finding loc of words\n",
        "sent1.find('money') # start \n",
        "print()\n",
        "sent1.find('money') + len('money') # end"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "129"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "134"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "id": "REGrYVWR-ke2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sent2 = sent_text[4].text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZNGRkD9-yMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "334142c3-0af8-41a5-9d01-29f65ad6dc16"
      },
      "cell_type": "code",
      "source": [
        "# finding loc of words\n",
        "sent2.find('Bashir') # start \n",
        "print()\n",
        "sent2.find('Bashir') + len('Bashir') # end"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "7B6cyS6vANkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sent3 = sent_text[6].text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FNwRYshyAQbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "e0cd3980-34f6-479e-c3ca-bc01f3dcd070"
      },
      "cell_type": "code",
      "source": [
        "# finding loc of words\n",
        "sent3.find('Dabanga') # start \n",
        "print()\n",
        "sent3.find('Dabanga') + len('Dabanga') # end\n",
        "print()\n",
        "\n",
        "# finding loc of words\n",
        "sent3.find('cash') # start \n",
        "print()\n",
        "sent3.find('cash') + len('cash') # end"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "id": "eXzNW_Z3ID32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sent4 = sent_text[-8].text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yye-QvalIKwD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "41445f54-4048-43a9-dea7-982a631e199d"
      },
      "cell_type": "code",
      "source": [
        "# finding loc of words\n",
        "sent4.find('Bashir') # start \n",
        "print()\n",
        "sent4.find('Bashir') + len('Bashir') # end\n",
        "print()\n",
        "\n",
        "# finding loc of words\n",
        "sent4.find('cash') # start \n",
        "print()\n",
        "sent4.find('cash') + len('cash') # end\n",
        "print()\n",
        "\n",
        "# finding loc of words\n",
        "sent4.find('money') # start \n",
        "print()\n",
        "sent4.find('money') + len('money') # end"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "115"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "metadata": {
        "id": "13BYGPVoJDEk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sent5 = sent_text[7].text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RXWpkXggJGQY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "97692a94-95e8-432a-b318-9adff3232f7e"
      },
      "cell_type": "code",
      "source": [
        "# finding loc of words\n",
        "sent5.find('Dabanga') # start \n",
        "print()\n",
        "sent5.find('Dabanga') + len('Dabanga') # end\n",
        "print()\n",
        "\n",
        "# finding loc of words\n",
        "sent5.find('money') # start \n",
        "print()\n",
        "sent5.find('money') + len('money') # end"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "D3EyfHvU65UE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAIN_DATA = [\n",
        "    # instead u'raw text' using sent1 made above\n",
        "    (sent1, {'entities': [\n",
        "        (17, 21, 'MONEY'),\n",
        "        (52, 57, 'LOC'),\n",
        "        (77, 91, 'PERSON'),\n",
        "        (129, 134, 'MONEY')\n",
        "    ]}),\n",
        "    (sent2, {'entities': [\n",
        "        (250, 256, 'PERSON')\n",
        "    ]}),\n",
        "    (sent3, {'entities': [\n",
        "        (64, 71, 'ORG'),\n",
        "        (154, 158, 'MONEY')\n",
        "    ]}),\n",
        "    (sent4, {'entities': [\n",
        "        (55, 61, 'PERSON'),\n",
        "        (88, 93, 'MONEY'),\n",
        "        (111, 115, 'MONEY')\n",
        "    ]}),\n",
        "    (sent5, {'entities': [\n",
        "        (6, 11, 'MONEY'),\n",
        "        (25, 32, 'ORG')\n",
        "    ]}),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KDq9a1C7Ail8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4LLY6Cou_JTJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(TRAIN_DATA, new_model_name, model=None, output_dir=None, n_iter=100):\n",
        "  \n",
        "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
        "    random.seed(0)\n",
        "    \n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "\n",
        "    # loop add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "        for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "            \n",
        "    # add separately\n",
        "    # ner.add_label(LABEL)\n",
        "    # Adding extraneous labels shouldn't mess anything up\n",
        "    # ner.add_label(\"VEGETABLE\")        \n",
        "    \n",
        "    # RESET & INIT weights randomly ONLY if training New Model\n",
        "    if model is None:\n",
        "        optimizer = nlp.begin_training()\n",
        "    #else:\n",
        "        #optimizer = nlp.resume_training() # only added V2.1 so if 2.0 delete this line\n",
        "    move_names = list(ner.move_names) # for asserting consistency later\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "    \n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "      sizes = compounding(1.0, 4.0, 1.001) # make compounding iterator\n",
        "      for itn in range(n_iter):\n",
        "          random.shuffle(TRAIN_DATA)\n",
        "          losses = {}\n",
        "          # batch up the examples using spaCy's minibatch\n",
        "          batches = minibatch(TRAIN_DATA, size=sizes)\n",
        "          for batch in batches:\n",
        "              texts, annotations = zip(*batch)\n",
        "              nlp.update(\n",
        "                  texts,  # batch of texts\n",
        "                  annotations,  # batch of annotations\n",
        "                  # sgd=optimizer, # use new or resumed weights (not used here given 2.0)\n",
        "                  drop=0.5,  # dropout - make it harder to memorise data\n",
        "                  losses=losses,\n",
        "              )\n",
        "          print(\"Losses\", losses)\n",
        "\n",
        "    #return nlp\n",
        "\n",
        "\n",
        "\n",
        "    # OPTIONAL (if not returning nlp)\n",
        "    \n",
        "    # TEST the trained model (same text??)\n",
        "    for text, _ in TRAIN_DATA:\n",
        "        doc = nlp(text)\n",
        "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n",
        "    # SAVE and LOAD\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "\n",
        "        # test the saved model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        # Check the classes have loaded back consistently\n",
        "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
        "        for text, _ in TRAIN_DATA:\n",
        "            doc = nlp2(text)\n",
        "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SG8860HVCYLv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2103
        },
        "outputId": "38d89831-7713-4215-80c3-b319eec2eb34"
      },
      "cell_type": "code",
      "source": [
        "# test run without returning nlp (auto-save-load-model and test on same text)\n",
        "\n",
        "main(TRAIN_DATA, new_model_name='money', model='en_core_web_sm', output_dir='./test_NER', n_iter=100)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model 'en_core_web_sm'\n",
            "Losses {'ner': 38.339719703242835}\n",
            "Losses {'ner': 28.49583713815015}\n",
            "Losses {'ner': 25.69930442805708}\n",
            "Losses {'ner': 23.29914193543904}\n",
            "Losses {'ner': 23.915318146356157}\n",
            "Losses {'ner': 13.630135039003822}\n",
            "Losses {'ner': 13.909836814990843}\n",
            "Losses {'ner': 11.130936764457838}\n",
            "Losses {'ner': 8.164399355363376}\n",
            "Losses {'ner': 7.844817149682352}\n",
            "Losses {'ner': 8.527891957652677}\n",
            "Losses {'ner': 3.7605356525216154}\n",
            "Losses {'ner': 2.2046004479703805}\n",
            "Losses {'ner': 0.6606404264126979}\n",
            "Losses {'ner': 2.087620396172873}\n",
            "Losses {'ner': 0.5788417879586264}\n",
            "Losses {'ner': 1.4673347812180273}\n",
            "Losses {'ner': 1.6129887940178806}\n",
            "Losses {'ner': 2.5845509727339437}\n",
            "Losses {'ner': 1.3948534087868443}\n",
            "Losses {'ner': 2.5414286015773406}\n",
            "Losses {'ner': 0.0008206936510949034}\n",
            "Losses {'ner': 1.334384932249169}\n",
            "Losses {'ner': 0.0755024031798681}\n",
            "Losses {'ner': 0.2669488861907792}\n",
            "Losses {'ner': 2.0548288943347335}\n",
            "Losses {'ner': 1.8820700421148364}\n",
            "Losses {'ner': 0.4520683824751448}\n",
            "Losses {'ner': 0.2529175525304349}\n",
            "Losses {'ner': 0.0013554146313685344}\n",
            "Losses {'ner': 1.1774782673378103e-06}\n",
            "Losses {'ner': 0.0013958896321118071}\n",
            "Losses {'ner': 0.0032259692995264917}\n",
            "Losses {'ner': 0.00037244154749324}\n",
            "Losses {'ner': 3.3289968087657815}\n",
            "Losses {'ner': 0.6512011530680983}\n",
            "Losses {'ner': 0.0007436219419390595}\n",
            "Losses {'ner': 0.0001496604280856042}\n",
            "Losses {'ner': 1.7033714647644114}\n",
            "Losses {'ner': 0.06341095163798016}\n",
            "Losses {'ner': 2.8306637807911666e-05}\n",
            "Losses {'ner': 3.0093788843020443e-06}\n",
            "Losses {'ner': 0.0016666682122946283}\n",
            "Losses {'ner': 1.4555187486024372}\n",
            "Losses {'ner': 3.199563203771572e-05}\n",
            "Losses {'ner': 7.831832493911251e-08}\n",
            "Losses {'ner': 0.0006816163418191707}\n",
            "Losses {'ner': 1.4993388128473946e-06}\n",
            "Losses {'ner': 0.0050355290046392425}\n",
            "Losses {'ner': 0.10230900623873893}\n",
            "Losses {'ner': 0.2309027387832485}\n",
            "Losses {'ner': 0.042061877472483224}\n",
            "Losses {'ner': 5.546370233400027e-05}\n",
            "Losses {'ner': 2.7711621932697656e-05}\n",
            "Losses {'ner': 1.1603537125337808e-07}\n",
            "Losses {'ner': 1.9988510385396903}\n",
            "Losses {'ner': 2.2855107346597653e-08}\n",
            "Losses {'ner': 4.066318790261611e-09}\n",
            "Losses {'ner': 0.008580409646883131}\n",
            "Losses {'ner': 1.1461207972023253e-07}\n",
            "Losses {'ner': 0.1455568336096406}\n",
            "Losses {'ner': 2.109146809970929e-06}\n",
            "Losses {'ner': 6.230582179902895e-05}\n",
            "Losses {'ner': 4.286897229622697e-07}\n",
            "Losses {'ner': 0.006709111949887978}\n",
            "Losses {'ner': 2.7345461412773058e-05}\n",
            "Losses {'ner': 1.3957607499635077e-09}\n",
            "Losses {'ner': 5.381171309966822e-06}\n",
            "Losses {'ner': 1.227700871211449e-07}\n",
            "Losses {'ner': 0.05297170346644713}\n",
            "Losses {'ner': 7.290057102830456e-07}\n",
            "Losses {'ner': 0.04244146956669147}\n",
            "Losses {'ner': 1.2182116294790575}\n",
            "Losses {'ner': 6.097364848174964e-06}\n",
            "Losses {'ner': 7.022653840673022e-07}\n",
            "Losses {'ner': 3.8381742669109266e-05}\n",
            "Losses {'ner': 3.0514836305301806e-07}\n",
            "Losses {'ner': 9.323280102042148e-08}\n",
            "Losses {'ner': 5.472526039989129e-07}\n",
            "Losses {'ner': 9.865662640732992e-05}\n",
            "Losses {'ner': 0.00014305035867590845}\n",
            "Losses {'ner': 9.357172243628928e-06}\n",
            "Losses {'ner': 0.048905836296087773}\n",
            "Losses {'ner': 2.046787406741172e-06}\n",
            "Losses {'ner': 3.068091302085015e-05}\n",
            "Losses {'ner': 0.0007213047727952302}\n",
            "Losses {'ner': 4.027798318265193e-06}\n",
            "Losses {'ner': 1.115763685065063e-05}\n",
            "Losses {'ner': 2.687374672962122e-08}\n",
            "Losses {'ner': 1.2470191686998569e-05}\n",
            "Losses {'ner': 1.246234362590734e-09}\n",
            "Losses {'ner': 3.007034624102336e-07}\n",
            "Losses {'ner': 0.001526460196092786}\n",
            "Losses {'ner': 0.0013838214071295054}\n",
            "Losses {'ner': 2.1844252503452178e-05}\n",
            "Losses {'ner': 0.000205196440157754}\n",
            "Losses {'ner': 1.2140066047320252e-05}\n",
            "Losses {'ner': 1.346735013527674e-07}\n",
            "Losses {'ner': 1.1607849643092631}\n",
            "Losses {'ner': 1.0133245328825675e-07}\n",
            "Entities [('money', 'MONEY'), ('Dabanga', 'ORG')]\n",
            "Tokens [('\\n\\n', '', 2), ('The', '', 2), ('money', 'MONEY', 3), (',', '', 2), ('which', '', 2), ('Radio', '', 2), ('Dabanga', 'ORG', 3), ('says', '', 2), ('was', '', 2), ('shown', '', 2), ('to', '', 2), ('reporters', '', 2), (',', '', 2), ('was', '', 2), ('stuffed', '', 2), ('in', '', 2), ('bags', '', 2), ('designed', '', 2), ('to', '', 2), ('contain', '', 2), ('50', '', 2), ('kg', '', 2), ('(', '', 2), ('110lbs', '', 2), (')', '', 2), ('of', '', 2), ('grain', '', 2), ('.', '', 2)]\n",
            "Entities [('Dabanga', 'ORG'), ('cash', 'MONEY')]\n",
            "Tokens [('\\n\\n', '', 2), ('A', '', 2), ('picture', '', 2), ('carried', '', 2), ('by', '', 2), ('the', '', 2), ('Netherlands', '', 2), ('-', '', 2), ('based', '', 2), ('media', '', 2), ('outlet', '', 2), ('Radio', '', 2), ('Dabanga', 'ORG', 3), ('shows', '', 2), ('men', '', 2), ('in', '', 2), ('army', '', 2), ('uniforms', '', 2), ('standing', '', 2), ('over', '', 2), ('what', '', 2), ('appears', '', 2), ('to', '', 2), ('be', '', 2), ('several', '', 2), ('sacks', '', 2), ('full', '', 2), ('of', '', 2), ('cash', 'MONEY', 3), ('.', '', 2)]\n",
            "Entities [('cash', 'MONEY'), ('Sudan', 'LOC'), ('Omar al-Bashir', 'PERSON'), ('money', 'MONEY')]\n",
            "Tokens [('A', '', 2), ('large', '', 2), ('hoard', '', 2), ('of', '', 2), ('cash', 'MONEY', 3), ('has', '', 2), ('been', '', 2), ('found', '', 2), ('at', '', 2), ('the', '', 2), ('home', '', 2), ('of', '', 2), ('Sudan', 'LOC', 3), (\"'s\", '', 2), ('ousted', '', 2), ('president', '', 2), ('Omar', 'PERSON', 3), ('al', 'PERSON', 1), ('-', 'PERSON', 1), ('Bashir', 'PERSON', 1), ('and', '', 2), ('he', '', 2), ('is', '', 2), ('now', '', 2), ('being', '', 2), ('investigated', '', 2), ('for', '', 2), ('money', 'MONEY', 3), ('laundering', '', 2), (',', '', 2), ('prosecutors', '', 2), ('say', '', 2), ('.', '', 2)]\n",
            "Entities [('Bashir', 'PERSON')]\n",
            "Tokens [('\\n', '', 2), ('•', '', 2), ('In', '', 2), ('pictures', '', 2), (':', '', 2), ('A', '', 2), ('campaign', '', 2), ('of', '', 2), ('defiance', '', 2), ('\\n', '', 2), ('•', '', 2), ('Timeline', '', 2), (':', '', 2), ('How', '', 2), ('Sudan', '', 2), ('got', '', 2), ('here', '', 2), ('\\n\\n', '', 2), ('A', '', 2), ('source', '', 2), ('in', '', 2), ('Sudan', '', 2), (\"'s\", '', 2), ('judiciary', '', 2), ('told', '', 2), ('Reuters', '', 2), ('news', '', 2), ('agency', '', 2), ('that', '', 2), ('suitcases', '', 2), ('loaded', '', 2), ('with', '', 2), ('more', '', 2), ('than', '', 2), ('$', '', 2), ('351,000', '', 2), (',', '', 2), ('€', '', 2), ('6', '', 2), ('m', '', 2), ('(', '', 2), ('$', '', 2), ('6.7', '', 2), ('m', '', 2), (';', '', 2), ('£', '', 2), ('5.2', '', 2), ('m', '', 2), (')', '', 2), ('and', '', 2), ('five', '', 2), ('billion', '', 2), ('Sudanese', '', 2), ('pounds', '', 2), ('(', '', 2), ('$', '', 2), ('105', '', 2), ('m', '', 2), (')', '', 2), ('were', '', 2), ('found', '', 2), ('at', '', 2), ('Mr', '', 2), ('Bashir', 'PERSON', 3), (\"'s\", '', 2), ('home', '', 2), ('.', '', 2)]\n",
            "Entities [('Bashir', 'PERSON'), ('money', 'MONEY'), ('cash', 'MONEY')]\n",
            "Tokens [('\\n\\n', '', 2), ('The', '', 2), ('general', '', 2), ('public', '', 2), ('prosecutor', '', 2), (\"'s\", '', 2), ('announcement', '', 2), ('that', '', 2), ('Mr', '', 2), ('Bashir', 'PERSON', 3), ('is', '', 2), ('being', '', 2), ('investigated', '', 2), ('for', '', 2), ('money', 'MONEY', 3), ('laundering', '', 2), ('after', '', 2), ('cash', 'MONEY', 3), ('was', '', 2), ('found', '', 2), ('at', '', 2), ('his', '', 2), ('home', '', 2), ('is', '', 2), ('news', '', 2), ('the', '', 2), ('demonstrators', '', 2), ('would', '', 2), ('like', '', 2), ('to', '', 2), ('hear', '', 2), ('.', '', 2)]\n",
            "Saved model to test_NER\n",
            "Loading from test_NER\n",
            "Entities [('money', 'MONEY'), ('Dabanga', 'ORG')]\n",
            "Tokens [('\\n\\n', '', 2), ('The', '', 2), ('money', 'MONEY', 3), (',', '', 2), ('which', '', 2), ('Radio', '', 2), ('Dabanga', 'ORG', 3), ('says', '', 2), ('was', '', 2), ('shown', '', 2), ('to', '', 2), ('reporters', '', 2), (',', '', 2), ('was', '', 2), ('stuffed', '', 2), ('in', '', 2), ('bags', '', 2), ('designed', '', 2), ('to', '', 2), ('contain', '', 2), ('50', '', 2), ('kg', '', 2), ('(', '', 2), ('110lbs', '', 2), (')', '', 2), ('of', '', 2), ('grain', '', 2), ('.', '', 2)]\n",
            "Entities [('Dabanga', 'ORG'), ('cash', 'MONEY')]\n",
            "Tokens [('\\n\\n', '', 2), ('A', '', 2), ('picture', '', 2), ('carried', '', 2), ('by', '', 2), ('the', '', 2), ('Netherlands', '', 2), ('-', '', 2), ('based', '', 2), ('media', '', 2), ('outlet', '', 2), ('Radio', '', 2), ('Dabanga', 'ORG', 3), ('shows', '', 2), ('men', '', 2), ('in', '', 2), ('army', '', 2), ('uniforms', '', 2), ('standing', '', 2), ('over', '', 2), ('what', '', 2), ('appears', '', 2), ('to', '', 2), ('be', '', 2), ('several', '', 2), ('sacks', '', 2), ('full', '', 2), ('of', '', 2), ('cash', 'MONEY', 3), ('.', '', 2)]\n",
            "Entities [('cash', 'MONEY'), ('Sudan', 'LOC'), ('Omar al-Bashir', 'PERSON'), ('money', 'MONEY')]\n",
            "Tokens [('A', '', 2), ('large', '', 2), ('hoard', '', 2), ('of', '', 2), ('cash', 'MONEY', 3), ('has', '', 2), ('been', '', 2), ('found', '', 2), ('at', '', 2), ('the', '', 2), ('home', '', 2), ('of', '', 2), ('Sudan', 'LOC', 3), (\"'s\", '', 2), ('ousted', '', 2), ('president', '', 2), ('Omar', 'PERSON', 3), ('al', 'PERSON', 1), ('-', 'PERSON', 1), ('Bashir', 'PERSON', 1), ('and', '', 2), ('he', '', 2), ('is', '', 2), ('now', '', 2), ('being', '', 2), ('investigated', '', 2), ('for', '', 2), ('money', 'MONEY', 3), ('laundering', '', 2), (',', '', 2), ('prosecutors', '', 2), ('say', '', 2), ('.', '', 2)]\n",
            "Entities [('Bashir', 'PERSON')]\n",
            "Tokens [('\\n', '', 2), ('•', '', 2), ('In', '', 2), ('pictures', '', 2), (':', '', 2), ('A', '', 2), ('campaign', '', 2), ('of', '', 2), ('defiance', '', 2), ('\\n', '', 2), ('•', '', 2), ('Timeline', '', 2), (':', '', 2), ('How', '', 2), ('Sudan', '', 2), ('got', '', 2), ('here', '', 2), ('\\n\\n', '', 2), ('A', '', 2), ('source', '', 2), ('in', '', 2), ('Sudan', '', 2), (\"'s\", '', 2), ('judiciary', '', 2), ('told', '', 2), ('Reuters', '', 2), ('news', '', 2), ('agency', '', 2), ('that', '', 2), ('suitcases', '', 2), ('loaded', '', 2), ('with', '', 2), ('more', '', 2), ('than', '', 2), ('$', '', 2), ('351,000', '', 2), (',', '', 2), ('€', '', 2), ('6', '', 2), ('m', '', 2), ('(', '', 2), ('$', '', 2), ('6.7', '', 2), ('m', '', 2), (';', '', 2), ('£', '', 2), ('5.2', '', 2), ('m', '', 2), (')', '', 2), ('and', '', 2), ('five', '', 2), ('billion', '', 2), ('Sudanese', '', 2), ('pounds', '', 2), ('(', '', 2), ('$', '', 2), ('105', '', 2), ('m', '', 2), (')', '', 2), ('were', '', 2), ('found', '', 2), ('at', '', 2), ('Mr', '', 2), ('Bashir', 'PERSON', 3), (\"'s\", '', 2), ('home', '', 2), ('.', '', 2)]\n",
            "Entities [('Bashir', 'PERSON'), ('money', 'MONEY'), ('cash', 'MONEY')]\n",
            "Tokens [('\\n\\n', '', 2), ('The', '', 2), ('general', '', 2), ('public', '', 2), ('prosecutor', '', 2), (\"'s\", '', 2), ('announcement', '', 2), ('that', '', 2), ('Mr', '', 2), ('Bashir', 'PERSON', 3), ('is', '', 2), ('being', '', 2), ('investigated', '', 2), ('for', '', 2), ('money', 'MONEY', 3), ('laundering', '', 2), ('after', '', 2), ('cash', 'MONEY', 3), ('was', '', 2), ('found', '', 2), ('at', '', 2), ('his', '', 2), ('home', '', 2), ('is', '', 2), ('news', '', 2), ('the', '', 2), ('demonstrators', '', 2), ('would', '', 2), ('like', '', 2), ('to', '', 2), ('hear', '', 2), ('.', '', 2)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}