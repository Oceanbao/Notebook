{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MEDIUM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Nqb3NE39hJJh",
        "iQphHmv8RUOI"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oceanbao/Notebook/blob/master/MEDIUM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqb3NE39hJJh",
        "colab_type": "text"
      },
      "source": [
        "# Bayes DL - Anomaly Auto-Encoder\n",
        "\n",
        "[Part One](https://github.com/bhgedigital/bayesian_calibration)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sBsBr1Ak0Px",
        "colab_type": "code",
        "outputId": "9b5f5049-8107-43ef-d838-c3f373bb1dcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Tensorflow Probability Installation (make sure to run this cell)  { display-mode: \"form\" }\n",
        "TFP_Installation = \"Stable TFP\" #@param [\"Most Recent TFP\", \"Stable TFP\", \"Stable TFP-GPU\", \"Most Recent TFP-GPU\", \"TFP Already Installed\"]\n",
        "\n",
        "if TFP_Installation == \"Most Recent TFP\":\n",
        "    !pip3 install -q --upgrade tf-nightly-gpu-2.0-preview tfp-nightly\n",
        "    print(\"Most recent TFP version installed\")\n",
        "elif TFP_Installation == \"Stable TFP\":\n",
        "    !pip3 install -q --upgrade tensorflow-probability\n",
        "    print(\"Up-to-date, stable  TFP version installed\")\n",
        "elif TFP_Installation == \"Stable TFP-GPU\":\n",
        "    !pip3 install -q --upgrade tensorflow-probability-gpu\n",
        "    print(\"Up-to-date, stable TFP-GPU version installed\")\n",
        "    print(\"(make sure GPU is properly configured)\")\n",
        "elif TFP_Installation == \"Most Recent TFP-GPU\":\n",
        "    !pip3 install -q tfp-nightly-gpu\n",
        "    print(\"Most recent TFP-GPU version installed\")\n",
        "    print(\"(make sure GPU is properly configured)\")\n",
        "elif TFP_Installation == \"TFP Already Installed\":\n",
        "    print(\"TFP already instaled in this environment\")\n",
        "    pass\n",
        "else:\n",
        "    print(\"Installation Error: Please select a viable TFP installation option.\")\n",
        "!pip3 install -q corner seaborn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Up-to-date, stable  TFP version installed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbiphkvWrl4j",
        "colab_type": "code",
        "outputId": "3dffbf82-2c0c-4c1f-ab30-960a6ed8f89c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "#@title Imports and Global Variables (make sure to run this cell)  { display-mode: \"form\" }\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "warning_status = \"ignore\" #@param [\"ignore\", \"always\", \"module\", \"once\", \"default\", \"error\"]\n",
        "import warnings\n",
        "warnings.filterwarnings(warning_status)\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(warning_status, category=DeprecationWarning)\n",
        "    warnings.filterwarnings(warning_status, category=UserWarning)\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import matplotlib \n",
        "matplotlib_style = 'fivethirtyeight' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']\n",
        "import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)\n",
        "import pandas as pd\n",
        "import matplotlib.axes as axes;\n",
        "matplotlib.rc('xtick', labelsize=20)     \n",
        "matplotlib.rc('ytick', labelsize=20)\n",
        "font = {'family' : 'Dejavu Sans','size'   : 20}\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "\n",
        "import seaborn as sns; sns.set_context('notebook')\n",
        "notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf']\n",
        "%config InlineBackend.figure_format = notebook_screen_res\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "              \n",
        "import tensorflow as tf\n",
        "from tensorflow_probability.python.mcmc.internal import util as mcmc_util\n",
        "tfe = tf.contrib.eager\n",
        "\n",
        "# Eager Execution\n",
        "use_tf_eager = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Use try/except so we can easily re-execute the whole notebook.\n",
        "if use_tf_eager:\n",
        "  try:\n",
        "    tf.enable_eager_execution()\n",
        "  except:\n",
        "    reset_session()\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "def default_session_options(enable_gpu_ram_resizing=True,\n",
        "                            enable_xla=False):\n",
        "  \"\"\"Creates default options for Graph-mode session.\"\"\"\n",
        "  config = tf.ConfigProto()\n",
        "  config.log_device_placement = True\n",
        "  if enable_gpu_ram_resizing:\n",
        "    # `allow_growth=True` makes it possible to connect multiple\n",
        "    # colabs to your GPU. Otherwise the colab malloc's all GPU ram.\n",
        "    config.gpu_options.allow_growth = True\n",
        "  if enable_xla:\n",
        "    # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
        "    config.graph_options.optimizer_options.global_jit_level = (\n",
        "        tf.OptimizerOptions.ON_1)\n",
        "  return config\n",
        "\n",
        "def session_options(enable_gpu_ram_resizing=True, enable_xla=True):\n",
        "    \"\"\"\n",
        "    Allowing the notebook to make use of GPUs if they're available.\n",
        "    \n",
        "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear \n",
        "    algebra that optimizes TensorFlow computations.\n",
        "    \"\"\"\n",
        "    config = tf.ConfigProto()\n",
        "    config.log_device_placement = True\n",
        "    if enable_gpu_ram_resizing:\n",
        "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
        "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
        "        config.gpu_options.allow_growth = True\n",
        "    if enable_xla:\n",
        "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
        "        config.graph_options.optimizer_options.global_jit_level = (\n",
        "            tf.OptimizerOptions.ON_1)\n",
        "    return config\n",
        "\n",
        "def reset_session(options=None):\n",
        "  \"\"\"Creates a new global, interactive session in Graph-mode.\"\"\"\n",
        "  if tf.executing_eagerly():\n",
        "    return\n",
        "  global sess\n",
        "  try:\n",
        "    tf.reset_default_graph()\n",
        "    sess.close()\n",
        "  except:\n",
        "    pass\n",
        "  if options is None:\n",
        "    options = default_session_options()\n",
        "  sess = tf.InteractiveSession(config=options)\n",
        "\n",
        "\n",
        "def evaluate(tensors):\n",
        "    \"\"\"Evaluates Tensor or EagerTensor to Numpy `ndarray`s.\n",
        "    Args:\n",
        "    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,\n",
        "        `namedtuple` or combinations thereof.\n",
        "\n",
        "    Returns:\n",
        "        ndarrays: Object with same structure as `tensors` except with `Tensor` or\n",
        "          `EagerTensor`s replaced by Numpy `ndarray`s.\n",
        "    \"\"\"\n",
        "    if tf.executing_eagerly():\n",
        "        return tf.contrib.framework.nest.pack_sequence_as(\n",
        "            tensors,\n",
        "            [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
        "             for t in tf.contrib.framework.nest.flatten(tensors)])\n",
        "    return sess.run(tensors)\n",
        "  \n",
        "def reset_sess(config=None):\n",
        "    \"\"\"\n",
        "    Convenience function to create the TF graph & session or reset them.\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = session_options()\n",
        "    global sess\n",
        "    tf.reset_default_graph()\n",
        "    try:\n",
        "        sess.close()\n",
        "    except:\n",
        "        pass\n",
        "    sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "reset_sess()\n",
        "\n",
        "\n",
        "class _TFColor(object):\n",
        "  \"\"\"Enum of colors used in TF docs.\"\"\"\n",
        "  red = '#F15854'\n",
        "  blue = '#5DA5DA'\n",
        "  orange = '#FAA43A'\n",
        "  green = '#60BD68'\n",
        "  pink = '#F17CB0'\n",
        "  brown = '#B2912F'\n",
        "  purple = '#B276B2'\n",
        "  yellow = '#DECF3F'\n",
        "  gray = '#4D4D4D'\n",
        "  def __getitem__(self, i):\n",
        "    return [\n",
        "        self.red,\n",
        "        self.orange,\n",
        "        self.green,\n",
        "        self.blue,\n",
        "        self.pink,\n",
        "        self.brown,\n",
        "        self.purple,\n",
        "        self.yellow,\n",
        "        self.gray,\n",
        "    ][i % 9]\n",
        "TFColor = _TFColor()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c612e961d148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmcmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmcmc_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Eager Execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCGt804Fr_Mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r5g7_AlhsWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_probability as tfp\n",
        "from tensorflow_probability import distributions as tfd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFGmwm_UhGqZ",
        "colab_type": "code",
        "outputId": "740a8200-8131-4736-dc65-b25f80f3c9ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "# Need to model two variables (materials)\n",
        "# Assuming Gaussian with some means and vars\n",
        "\n",
        "prio_par_logC = [-23., 1.1] # [loc, scale] for Normal Prior\n",
        "prio_par_m = [4., 0.2]\n",
        "rv_logC = tfp.Normal(loc=0., scale=1., name='logC_norm')\n",
        "rv_m = tfd.Normal(loc=0., scale=1., name='m_norm')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7fd545d7fd64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprio_par_logC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m23.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# [loc, scale] for Normal Prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprio_par_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrv_logC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logC_norm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mrv_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'm_norm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_probability' has no attribute 'Normal'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaXjmsfJh9ZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQphHmv8RUOI",
        "colab_type": "text"
      },
      "source": [
        "# WandB Visual Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BckE5sdXRYK0",
        "colab_type": "code",
        "outputId": "a5bb4e96-59fe-4b56-8a02-9117286f1d4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/f0/c0d690d66be181ac74624fda68a53e3daf3008cf1a10702b957bf0a08420/wandb-0.8.9-py2.py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 2.8MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3 (from wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 26.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
            "Collecting GitPython>=1.0.0 (from wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/c7/70bd352e8a561a9b6d1cde9aa313b9d7c871b0c94c3821f44c01f3187e1d/GitPython-3.0.2-py3-none-any.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting watchdog>=0.8.3 (from wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/e3/5a55d48a29300160779f0a0d2776d17c1b762a2039b36de528b093b87d5b/watchdog-0.9.0.tar.gz (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 24.6MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0 (from wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/aa/fc22098ed3bf7649beb8bbb4863d8a1fe2c1f9c33e5edfb853318025e9d9/sentry_sdk-0.11.1-py2.py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 22.9MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.6.1 (from wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 38.6MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0 (from wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/80/d7/2bfc9332e68d3e15ea97b9b1588b3899ad565120253d3fd71c8f7f13b4fe/shortuuid-0.5.0.tar.gz\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting gql>=0.1.0 (from wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/9c/2933b7791210e00f5c26a6243198cc03af9132c29cf85e4c22cb007f171e/gql-0.1.0.tar.gz\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Collecting gitdb2>=2.0.0 (from GitPython>=1.0.0->wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/30/a407568aa8d8f25db817cf50121a958722f3fc5f87e3a6fba1f40c0633e3/gitdb2-2.0.5-py2.py3-none-any.whl (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 22.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (3.13)\n",
            "Collecting argh>=0.24.1 (from watchdog>=0.8.3->wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/06/1c/e667a7126f0b84aaa1c56844337bf0ac12445d1beb9c8a6199a7314944bf/argh-0.26.2-py2.py3-none-any.whl\n",
            "Collecting pathtools>=0.1.1 (from watchdog>=0.8.3->wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2019.6.16)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Collecting graphql-core>=0.5.0 (from gql>=0.1.0->wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/11/bc4a7eb440124271289d93e4d208bd07d94196038fabbe2a52435a07d3d3/graphql_core-2.2.1-py2.py3-none-any.whl (250kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from gql>=0.1.0->wandb) (2.2.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Collecting smmap2>=2.0.0 (from gitdb2>=2.0.0->GitPython>=1.0.0->wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/55/d2/866d45e3a121ee15a1dc013824d58072fd5c7799c9c34d01378eb262ca8f/smmap2-2.0.5-py2.py3-none-any.whl\n",
            "Collecting rx<3,>=1.6 (from graphql-core>=0.5.0->gql>=0.1.0->wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/0f/5ef4ac78e2a538cc1b054eb86285fe0bf7a5dbaeaac2c584757c300515e2/Rx-1.6.1-py2.py3-none-any.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 31.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: subprocess32, watchdog, shortuuid, gql, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6488 sha256=aa356b96e8a627f1b48345bdbfe7640f2bcd2dfac5aa109341a9c35e52950d11\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.9.0-cp36-none-any.whl size=73652 sha256=e49b2fe9be29587acb93d66979ec64d357386186d3c57e21a11cd8b6ef4182b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/1d/d0/04cfe495619be2095eb8d89a31c42adb4e42b76495bc8f784c\n",
            "  Building wheel for shortuuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shortuuid: filename=shortuuid-0.5.0-cp36-none-any.whl size=5499 sha256=357e26f45c5b99e95c06c65492361ce6368f7e9b0092eac79f99499b4bfb1843\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/eb/fd/69e5177f67b505e44acbd1aedfbe44b91768ee0c4cd5636576\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.1.0-cp36-none-any.whl size=5540 sha256=dfde975ae5fd3702f8278c4850d60e12cc53de331520c7fcca35d791f8e5e9b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/8d/65/a3247f500d675d80a01e4d2f0ee44fe99f1faef575bc2a1664\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8785 sha256=3458474c872df4adf0f570e8e671d93e99c42c7bd318c1d141ed7bba616fd925\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 watchdog shortuuid gql pathtools\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: subprocess32, smmap2, gitdb2, GitPython, argh, pathtools, watchdog, sentry-sdk, python-dateutil, shortuuid, docker-pycreds, rx, graphql-core, gql, wandb\n",
            "  Found existing installation: python-dateutil 2.5.3\n",
            "    Uninstalling python-dateutil-2.5.3:\n",
            "      Successfully uninstalled python-dateutil-2.5.3\n",
            "Successfully installed GitPython-3.0.2 argh-0.26.2 docker-pycreds-0.4.0 gitdb2-2.0.5 gql-0.1.0 graphql-core-2.2.1 pathtools-0.1.2 python-dateutil-2.8.0 rx-1.6.1 sentry-sdk-0.11.1 shortuuid-0.5.0 smmap2-2.0.5 subprocess32-3.5.4 wandb-0.8.9 watchdog-0.9.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fARMtt2KVMtC",
        "colab_type": "code",
        "outputId": "51dfc41c-2e2a-4567-d51b-d4c76934ecf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "!wandb init"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mLet's setup this directory for W&B!\u001b[0m\n",
            "You can find your API keys in your browser here: https://app.wandb.ai/authorize\n",
            "Paste an API key from your profile: 22b21ba20dff05b42eb845be83cc2108ac59b0bf\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n",
            "What username or team should we use? [oceanbao]: \n",
            "Enter a name for your first project: Test1\n",
            "entity: oceanbao\n",
            "project: test1\n",
            "\u001b[32mThis directory is configured!  Next, track a run:\n",
            "\u001b[0m* In your training script:\n",
            "    \u001b[1mimport wandb\u001b[0m\n",
            "    \u001b[1mwandb.init()\u001b[0m\n",
            "* then `\u001b[1mpython <train.py>\u001b[0m`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XBTYmP2dx_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbawh5AkdEZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3ApMlD9jb7N",
        "colab_type": "text"
      },
      "source": [
        "# 100 times NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFvSyNwhjfn2",
        "colab_type": "text"
      },
      "source": [
        "## Profiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_dpUriajsB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cProfile\n",
        "import pstats\n",
        "import my_slow_module\n",
        "cProfile.run('my_slow_module.run()', 'restats')\n",
        "p = pstats.Stats('restats')\n",
        "p.sort_stats('cumulative').print_stats(30)\n",
        "\n",
        "# https://cython.readthedocs.io/en/latest/src/userguide/numpy_tutorial.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQY2O9Fcj1p-",
        "colab_type": "text"
      },
      "source": [
        "## Fast Loop via Cython"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHK04EsRkEln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# counting area of rectangles \n",
        "\n",
        "from random import random\n",
        "\n",
        "class Rectangle:\n",
        "    def __init__(self, w, h):\n",
        "        self.w = w\n",
        "        self.h = h\n",
        "    def area(self):\n",
        "        return self.w * self.h\n",
        "\n",
        "def check_rectangles(rectangles, threshold):\n",
        "    n_out = 0\n",
        "    for rectangle in rectangles:\n",
        "        if rectangle.area() > threshold:\n",
        "            n_out += 1\n",
        "    return n_out\n",
        "\n",
        "def main_slow():\n",
        "    n_rectangles = 10000000\n",
        "    rectangles = list(Rectangle(random(), random()) for i in range(n_rectangles))\n",
        "    n_out = check_rectangles(rectangles, threshold=0.25)\n",
        "    print(n_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKzpCYoqsse0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "4240f6dc-c7a0-4783-b56c-7c51678f9004"
      },
      "source": [
        "%%time\n",
        "main_slow()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4036133\n",
            "CPU times: user 11.8 s, sys: 1.14 s, total: 12.9 s\n",
            "Wall time: 12.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rcDdkOEkPQi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "4794ac63-c8f4-4e93-fba2-bcf9cbd41a92"
      },
      "source": [
        "import cProfile\n",
        "import pstats\n",
        "cProfile.run('main()', 'restats')\n",
        "p = pstats.Stats('restats')\n",
        "p.sort_stats('cumulative').print_stats(30)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4031279\n",
            "Sun Oct  6 05:38:16 2019    restats\n",
            "\n",
            "         50000041 function calls in 21.394 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.000    0.000   21.394   21.394 {built-in method builtins.exec}\n",
            "        1    1.024    1.024   21.394   21.394 <string>:1(<module>)\n",
            "        1    1.044    1.044   20.370   20.370 <ipython-input-2-366980302f99>:18(main)\n",
            " 10000001   10.903    0.000   15.673    0.000 <ipython-input-2-366980302f99>:20(<genexpr>)\n",
            "        1    2.247    2.247    3.651    3.651 <ipython-input-2-366980302f99>:11(check_rectangles)\n",
            " 10000000    2.946    0.000    2.946    0.000 <ipython-input-2-366980302f99>:5(__init__)\n",
            " 20000000    1.825    0.000    1.825    0.000 {method 'random' of '_random.Random' objects}\n",
            " 10000000    1.405    0.000    1.405    0.000 <ipython-input-2-366980302f99>:8(area)\n",
            "        1    0.000    0.000    0.001    0.001 {built-in method builtins.print}\n",
            "        2    0.000    0.000    0.001    0.000 /usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py:342(write)\n",
            "        3    0.000    0.000    0.001    0.000 /usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py:180(schedule)\n",
            "        3    0.000    0.000    0.000    0.000 /usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py:333(send)\n",
            "        2    0.000    0.000    0.000    0.000 /usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py:297(_schedule_flush)\n",
            "        3    0.000    0.000    0.000    0.000 /usr/lib/python3.6/threading.py:1104(is_alive)\n",
            "        3    0.000    0.000    0.000    0.000 {built-in method posix.urandom}\n",
            "        2    0.000    0.000    0.000    0.000 /usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py:284(_is_master_process)\n",
            "        3    0.000    0.000    0.000    0.000 /usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py:87(_event_pipe)\n",
            "        3    0.000    0.000    0.000    0.000 /usr/lib/python3.6/threading.py:1062(_wait_for_tstate_lock)\n",
            "        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
            "        2    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
            "        2    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
            "        3    0.000    0.000    0.000    0.000 /usr/lib/python3.6/threading.py:506(is_set)\n",
            "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pstats.Stats at 0x7f00b80f9588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jx0X8V8ksVZ",
        "colab_type": "text"
      },
      "source": [
        "> We can then store our list of rectangles in a C array of such structures that we will pass to our check_rectangle function. This function now has to accept a C array as input and thus will be defined as a Cython function by using the cdef"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG3RO3vSlfpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext Cython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZU3Rp3dn4XX",
        "colab_type": "text"
      },
      "source": [
        "### Digress: init POINTER from numpy or memoryview\n",
        "\n",
        "```python\n",
        "cdef void get_pointers(np.ndarray[int, mode='c'] numpy_array, vector[int] cpp_vector, int[::1] memory_view) nogil:\n",
        "    pointer1 = <int*>numpy_array.data\n",
        "    pointer2 = cpp_vector.data()\n",
        "    pointer3 = &memory_view[0]\n",
        "```\n",
        "\n",
        "When getting a pointer from a numpy array or memoryview, take care that the data is actually stored in C-contiguous order — otherwise you’ll get a pointer to nonsense. The type-declarations in the code above should generate runtime errors if buffers with incorrect memory layouts are passed in. To iterate over the array, the following style is preferred:\n",
        "\n",
        "```python\n",
        "cdef int c_total(const int* int_array, int length) nogil:\n",
        "    total = 0\n",
        "    for item in int_array[:length]:\n",
        "        total += item\n",
        "    return total\n",
        "```\n",
        "\n",
        "### Digress: cymem from spaCy (auto alloc/free memory in Cython)\n",
        "\n",
        "> Let's say we want a sequence of sparse matrices. We need fast access, and a Python list isn't performing well enough. So, we want a C-array or C++ vector, which means we need the sparse matrix to be a C-level struct — it can't be a Python class. We can write this easily enough in Cython:\n",
        "\n",
        "```python\n",
        "\"\"\"Example without Cymem\n",
        "\n",
        "To use an array of structs, we must carefully walk the data structure when we deallocate it.\n",
        "\"\"\"\n",
        "\n",
        "from libc.stdlib cimport calloc, free\n",
        "\n",
        "cdef struct SparseRow:\n",
        "    size_t length\n",
        "    size_t* indices\n",
        "    double* values\n",
        "\n",
        "cdef struct SparseMatrix:\n",
        "    size_t length\n",
        "    SparseRow* rows\n",
        "\n",
        "cdef class MatrixArray:\n",
        "    cdef size_t length\n",
        "    cdef SparseMatrix** matrices\n",
        "\n",
        "    def __cinit__(self, list py_matrices):\n",
        "        self.length = 0\n",
        "        self.matrices = NULL\n",
        "\n",
        "    def __init__(self, list py_matrices):\n",
        "        self.length = len(py_matrices)\n",
        "        self.matrices = <SparseMatrix**>calloc(len(py_matrices), sizeof(SparseMatrix*))\n",
        "\n",
        "        for i, py_matrix in enumerate(py_matrices):\n",
        "            self.matrices[i] = sparse_matrix_init(py_matrix)\n",
        "\n",
        "    def __dealloc__(self):\n",
        "        for i in range(self.length):\n",
        "            sparse_matrix_free(self.matrices[i])\n",
        "        free(self.matrices)\n",
        "\n",
        "\n",
        "cdef SparseMatrix* sparse_matrix_init(list py_matrix) except NULL:\n",
        "    sm = <SparseMatrix*>calloc(1, sizeof(SparseMatrix))\n",
        "    sm.length = len(py_matrix)\n",
        "    sm.rows = <SparseRow*>calloc(sm.length, sizeof(SparseRow))\n",
        "    cdef size_t i, j\n",
        "    cdef dict py_row\n",
        "    cdef size_t idx\n",
        "    cdef double value\n",
        "    for i, py_row in enumerate(py_matrix):\n",
        "        sm.rows[i].length = len(py_row)\n",
        "        sm.rows[i].indices = <size_t*>calloc(sm.rows[i].length, sizeof(size_t))\n",
        "        sm.rows[i].values = <double*>calloc(sm.rows[i].length, sizeof(double))\n",
        "        for j, (idx, value) in enumerate(py_row.items()):\n",
        "            sm.rows[i].indices[j] = idx\n",
        "            sm.rows[i].values[j] = value\n",
        "    return sm\n",
        "\n",
        "\n",
        "cdef void* sparse_matrix_free(SparseMatrix* sm) except *:\n",
        "    cdef size_t i\n",
        "    for i in range(sm.length):\n",
        "        free(sm.rows[i].i\n",
        "```\n",
        "\n",
        "> We wrap the data structure in a Python ref-counted class at as low a level as we can, given our performance constraints. This allows us to allocate and free the memory in the __cinit__ and __dealloc__ Cython special methods.\n",
        "\n",
        "> However, it's very easy to make mistakes when writing the __dealloc__ and sparse_matrix_free functions, leading to memory leaks. cymem prevents you from writing these deallocators at all. Instead, you write as follows:\n",
        "\n",
        "```python\n",
        "\"\"\"Example with Cymem.\n",
        "\n",
        "Memory allocation is hidden behind the Pool class, which remembers the\n",
        "addresses it gives out.  When the Pool object is garbage collected, all of\n",
        "its addresses are freed.\n",
        "\n",
        "We don't need to write MatrixArray.__dealloc__ or sparse_matrix_free,\n",
        "eliminating a common class of bugs.\n",
        "\"\"\"\n",
        "from cymem.cymem cimport Pool\n",
        "\n",
        "cdef struct SparseRow:\n",
        "    size_t length\n",
        "    size_t* indices\n",
        "    double* values\n",
        "\n",
        "cdef struct SparseMatrix:\n",
        "    size_t length\n",
        "    SparseRow* rows\n",
        "\n",
        "\n",
        "cdef class MatrixArray:\n",
        "    cdef size_t length\n",
        "    cdef SparseMatrix** matrices\n",
        "    cdef Pool mem\n",
        "\n",
        "    def __cinit__(self, list py_matrices):\n",
        "        self.mem = None\n",
        "        self.length = 0\n",
        "        self.matrices = NULL\n",
        "\n",
        "    def __init__(self, list py_matrices):\n",
        "        self.mem = Pool()\n",
        "        self.length = len(py_matrices)\n",
        "        self.matrices = <SparseMatrix**>self.mem.alloc(self.length, sizeof(SparseMatrix*))\n",
        "        for i, py_matrix in enumerate(py_matrices):\n",
        "            self.matrices[i] = sparse_matrix_init(self.mem, py_matrix)\n",
        "\n",
        "cdef SparseMatrix* sparse_matrix_init_cymem(Pool mem, list py_matrix) except NULL:\n",
        "    sm = <SparseMatrix*>mem.alloc(1, sizeof(SparseMatrix))\n",
        "    sm.length = len(py_matrix)\n",
        "    sm.rows = <SparseRow*>mem.alloc(sm.length, sizeof(SparseRow))\n",
        "    cdef size_t i, j\n",
        "    cdef dict py_row\n",
        "    cdef size_t idx\n",
        "    cdef double value\n",
        "    for i, py_row in enumerate(py_matrix):\n",
        "        sm.rows[i].length = len(py_row)\n",
        "        sm.rows[i].indices = <size_t*>mem.alloc(sm.rows[i].length, sizeof(size_t))\n",
        "        sm.rows[i].values = <double*>mem.alloc(sm.rows[i].length, sizeof(double))\n",
        "        for j, (idx, value) in enumerate(py_row.items()):\n",
        "            sm.rows[i].indices[j] = idx\n",
        "            sm.rows[i].values[j] = value\n",
        "    return sm\n",
        "```\n",
        "\n",
        "> All that the Pool class does is remember the addresses it gives out. When the MatrixArray object is garbage-collected, the Pool object will also be garbage collected, which triggers a call to Pool.__dealloc__. The Pool then frees all of its addresses. This saves you from walking back over your nested data structures to free them, eliminating a common class of errors.\n",
        "\n",
        "> Sometimes extern C lib has custom alloc/dealloc, but still wrap them\n",
        "\n",
        "```python\n",
        "from cymem.cymem cimport Pool, WrapMalloc, WrapFree\n",
        "cdef Pool mem = Pool(WrapMalloc(priv_malloc), WrapFree(priv_free))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eUExWk5lJ7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%cython\n",
        "from cymem.cymem cimport Pool\n",
        "from random import random\n",
        "\n",
        "cdef struct Rectangle:\n",
        "    float w\n",
        "    float h\n",
        "\n",
        "cdef int check_rectangles_cy(Rectangle* rectangles, int n_rectangles, float threshold):\n",
        "    cdef int n_out = 0\n",
        "    # C arrays contain no size information => we need to state it explicitly\n",
        "    for rectangle in rectangles[:n_rectangles]:\n",
        "        if rectangle.w * rectangle.h > threshold:\n",
        "            n_out += 1\n",
        "    return n_out\n",
        "\n",
        "def main_rectangles_fast():\n",
        "    cdef int n_rectangles = 10000000\n",
        "    cdef float threshold = 0.25\n",
        "    cdef Pool mem = Pool()\n",
        "    cdef Rectangle* rectangles = <Rectangle*>mem.alloc(n_rectangles, sizeof(Rectangle))\n",
        "    for i in range(n_rectangles):\n",
        "        rectangles[i].w = random()\n",
        "        rectangles[i].h = random()\n",
        "    n_out = check_rectangles_cy(rectangles, n_rectangles, threshold)\n",
        "    print(n_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkV8bQ9QsiyA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "5cc14bfe-84d6-4270-a8e0-56fd1053f7cf"
      },
      "source": [
        "%%time\n",
        "main_rectangles_fast()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4038253\n",
            "CPU times: user 521 ms, sys: 19 ms, total: 540 ms\n",
            "Wall time: 541 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42ljEzsxr4Ac",
        "colab_type": "text"
      },
      "source": [
        "## Clever SpaCy StringStore\n",
        "\n",
        "All the unicode strings in spaCy (the text of a token, its lower case text, its lemma form, POS tag label, parse tree dependency label, Named-Entity tags…) are stored in a single data structure called the StringStore where they are indexed by 64-bit hashes, i.e. C level uint64_t.\n",
        "\n",
        "> **The StringStore object implements a look up between Python unicode strings and 64-bit hashes.\n",
        "It is accessible from everywhere in spaCy and every object (see on the left), for example as nlp.vocab.strings, doc.vocab.strings or span.doc.vocab.string.\n",
        "When a module needs to perform fast processing on some tokens, it simply uses the C level 64-bit hashes instead of the strings. A call to the StringStore look up table will then give back the Python unicode strings associated to the hashes.**\n",
        "\n",
        "> But spaCy does more than that and also gives us access to fully populated C level structures of the document and vocabulary, which we can use in Cython loops instead of having to build our own structures.\n",
        "\n",
        "### SpaCy Data Structure\n",
        "\n",
        "The main data structure associated to a spaCy document is the Doc object which owns the sequence of tokens (“words”) of the processed string and all their annotations in a C level object called doc.c which is an array of TokenC structures.\n",
        "\n",
        "The TokenC structure contains all the informations we need about each tokens. This information is stored as 64-bit hashes that can be re-associated to unicode strings as we’ve just seen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vogh1tbTvGbe",
        "colab_type": "text"
      },
      "source": [
        "**Example of fast spaCy test processing**\n",
        "\n",
        "Task:\n",
        "\n",
        "On the left I wrote a script that builds a list of 10 documents parsed by spaCy, each with ~170k words. We could also have 170k documents with 10 words in each (like a dialog dataset) but that’s slower to create so let’s stick with 10 docs.\n",
        "\n",
        "We want to perform some NLP task on this dataset. For example, we would like to count the number of times the word “run” is used as a noun in the dataset (i.e. tagged tagged with a “NN” Part-Of-Speech tag by spaCy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHX8yjIsvdQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "import spacy\n",
        "# Build a dataset of 10 parsed document extracted from the Wikitext-2 dataset\n",
        "with urllib.request.urlopen('https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt') as response:\n",
        "   text = response.read()\n",
        "nlp = spacy.load('en')\n",
        "doc_list = list(nlp.pipe(text[:800000].decode('utf8')) for i in range(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEKi8FAQvhF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We have about 1.7 million tokens (\"words\") in our dataset:\n",
        "\n",
        "# slow python\n",
        "def slow_loop(doc_list, word, tag):\n",
        "    n_out = 0\n",
        "    for doc in doc_list:\n",
        "        for tok in doc:\n",
        "            if tok.lower_ == word and tok.tag_ == tag:\n",
        "                n_out += 1\n",
        "    return n_out\n",
        "\n",
        "def main_nlp_slow(doc_list):\n",
        "    n_out = slow_loop(doc_list, 'run', 'NN')\n",
        "    print(n_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp63-iodv2Pw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "487ac3ee-5865-4098-d282-492c8f2531e0"
      },
      "source": [
        "%%time\n",
        "# But it's also quite slow\n",
        "main_nlp_slow(doc_list)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90\n",
            "CPU times: user 1.05 s, sys: 0 ns, total: 1.05 s\n",
            "Wall time: 1.05 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_Q3lv4jwDqY",
        "colab_type": "text"
      },
      "source": [
        "**First, we have to think about the data structure. We will need a C level array for the dataset, with pointers to each document's TokenC array. We'll also need to convert the strings we use for testing to 64-bit hashes: \"run\" and \"NN\". When all the data required for our processing is in C level objects, we can then iterate at full C speed over the dataset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmwmrLpkw0n4",
        "colab_type": "text"
      },
      "source": [
        "> longer code because of need to declare and populate C struct in main_nlp_fast BEFORE calling Cython function \n",
        "\n",
        "> NOTE: If using low level struct several times in code, more elegent than populating C struct each time is to design Python code around low struct with a Cython extension type cdeftype wrapping the C struct. This is how SpaCy built to elegently combine fast, low mem, easy inteface with external Python lib and func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGTJgCButeOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%cython -+\n",
        "import numpy # Sometime we have a fail to import numpy compilation error if we don't import numpy\n",
        "from cymem.cymem cimport Pool\n",
        "from spacy.tokens.doc cimport Doc\n",
        "from spacy.typedefs cimport hash_t\n",
        "from spacy.structs cimport TokenC\n",
        "\n",
        "cdef struct DocElement:\n",
        "    TokenC* c\n",
        "    int length\n",
        "\n",
        "cdef int fast_loop(DocElement* docs, int n_docs, hash_t word, hash_t tag):\n",
        "    cdef int n_out = 0\n",
        "    for doc in docs[:n_docs]:\n",
        "        for c in doc.c[:doc.length]:\n",
        "            if c.lex.lower == word and c.tag == tag:\n",
        "                n_out += 1\n",
        "    return n_out\n",
        "\n",
        "cpdef main_nlp_fast(doc_list):\n",
        "    cdef int i, n_out, n_docs = len(doc_list)\n",
        "    cdef Pool mem = Pool()\n",
        "    cdef DocElement* docs = <DocElement*>mem.alloc(n_docs, sizeof(DocElement))\n",
        "    cdef Doc doc\n",
        "    for i, doc in enumerate(doc_list): # Populate our database structure\n",
        "        docs[i].c = doc.c\n",
        "        docs[i].length = (<Doc>doc).length\n",
        "    word_hash = doc.vocab.strings.add('run')\n",
        "    tag_hash = doc.vocab.strings.add('NN')\n",
        "    n_out = fast_loop(docs, n_docs, word_hash, tag_hash)\n",
        "    print(n_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izR1aPZFxihc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "21c97a4e-39c1-4c06-ae76-4d57c30a8649"
      },
      "source": [
        "%%time\n",
        "main_nlp_fast(doc_list)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90\n",
            "CPU times: user 24.4 ms, sys: 0 ns, total: 24.4 ms\n",
            "Wall time: 24 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL2IJQIgxoVQ",
        "colab_type": "text"
      },
      "source": [
        "**The absolute speed is also impressive for a module written in a Jupyter Notebook cell and which can interface natively with other Python modules and functions: scanning ~1,7 million words in 20 ms means we are processing a whopping 80 millions words per seconds.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn3edl3mx-k3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}