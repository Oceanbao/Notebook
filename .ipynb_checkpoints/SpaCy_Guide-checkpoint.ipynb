{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# COURSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token, Span, Lexical Attr, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellow\n",
      "world\n",
      "!\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "doc = nlp(\"Hellow world!\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "token = doc[1]\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "span = doc[1:4]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  [0, 1, 2, 3, 4]\n",
      "Text:  ['It', 'costs', '$', '5', '.']\n",
      "is_alpha: [True, True, False, False, False]\n",
      "is_punct: [False, False, False, False, True]\n",
      "like_num: [False, False, False, True, False]\n",
      "like_url [False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It costs $5.\")\n",
    "\n",
    "print('Index: ', [token.i for token in doc])\n",
    "print('Text: ', [token.text for token in doc])\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "print('like_num:', [token.like_num for token in doc])\n",
    "print('like_url', [token.like_url for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme proverty. \"\n",
    "          \"Now less than 4% are.\")\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        next_token = doc[token.i + 1]\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Model\n",
    "- POS\n",
    "- Syntatic Dep\n",
    "- NE\n",
    "\n",
    "1. **Training on Labelled Data**\n",
    "2. **Can be updated with more examples to fine-tune**\n",
    "\n",
    "**Model packages built-in**\n",
    "- Binary weights\n",
    "- Vocab\n",
    "- Metadata (language, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'direct object'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")\n",
    "spacy.explain(\"NNP\")\n",
    "spacy.explain('dobj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based Matching\n",
    "\n",
    "**Why not REGEX**\n",
    "1. Match on `Doc` not just strings\n",
    "2. Match on tokens and token attributes\n",
    "3. Use model's predictions\n",
    "4. e.g. \"duck\" (verb) vs. \"duck\" (noun)\n",
    "\n",
    "**Match patterns**\n",
    "- Lists of dictionaries, one per token\n",
    "- Match extact\n",
    "`[{'TEXT': 'iPhone'}, {'TEXT': 'X'}]`\n",
    "- Match lexical attributes\n",
    "`[{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "- Match any token attributes\n",
    "`[{'LEMMA': 'buy'}, {'POS': 'NOUN'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "# INIT matcher with SHARED VOCAB\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add pattern to matcher\n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "\n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "for matche_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matching Lexical**\n",
    "\n",
    "```python\n",
    "pattern = [\n",
    "    {'IS_DIGIT': True},\n",
    "    {'LOWER': 'fifa'},\n",
    "    {'LOWER': 'world'},\n",
    "    {'LOWER': 'cup'},\n",
    "    {'IS_PUNCT': 'True'}\n",
    "]\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "# 2018 FIFA World Cup:\n",
    "```\n",
    "\n",
    "**Matching other Token Attributes**\n",
    "\n",
    "```python\n",
    "pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "# loved dogs \n",
    "# love cats\n",
    "```\n",
    "\n",
    "**Operators and Quantifiers**\n",
    "\n",
    "```python\n",
    "pattern = [\n",
    "    {'LEMMA': 'buy'}.\n",
    "    {'POS': 'DET', 'OP': '?'}, # optional: match 0 or 1 times\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "# bought a smartphone\n",
    "# buying apps\n",
    "```\n",
    "\n",
    "- `{'OP': '!'}` Negation: match 0 times\n",
    "- `'+'` Match 1 or more times\n",
    "- `'*'` Match 0 or more times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Large-scale Data Analysis \n",
    "\n",
    "**Data Structure: Vocab, Lexemes and StringStore**\n",
    "\n",
    "- `Vocab` stores data shared across multiple documents\n",
    "- To save memory, spaCy encodes all strings to **hash value**\n",
    "- Strings are only stored once in the `StringStore` via `nlp.vocab.strings`\n",
    "- String store: **lookup table** in both directions\n",
    "\n",
    "```python\n",
    "coffee_hash = nlp.vocab.strings['coffee']\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "```\n",
    "\n",
    "- Hashes cannot be reversed - hence need to provide shared vocab!!!\n",
    "\n",
    "```python\n",
    "# Raises error if not seen string before\n",
    "string = nlp.vocab.strings[319792845301844401]\n",
    "```\n",
    "\n",
    "- Look up string and hash in `nlp.vocab.strings`\n",
    "\n",
    "```python\n",
    "doc = nlp(\"I love coffee\")\n",
    "nlp.vocab.strings['coffee'] # 319792745301814401\n",
    "nlp.vocab.strings[hash] # coffee\n",
    "\n",
    "# doc also exposes vocab and strings\n",
    "doc.vocab.strings['coffee']\n",
    "```\n",
    "\n",
    "- `Lexeme` obj is entry in vocab\n",
    "\n",
    "```python\n",
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "lexeme.text, lexeme.orth, lexeme.is_alpha # coffee 3179... True\n",
    "```\n",
    "\n",
    "- Contains **context-independent** info about word\n",
    "    - Word text: `lexeme.text` and `lexeme.orth` (the hash)\n",
    "    - Lexical attributes like `lexeme.is_alpha`\n",
    "    - NOT context-dependent POS, DEPs or NE\n",
    "    \n",
    "**DATA STRUCTURE**\n",
    "\n",
    "VOCAB, HASHES, LEXEMES\n",
    "\n",
    "- DOC (Token : I : PRON) <-nsubj- (Token : love : VERB) -dobj-> (Token : coffee : NOUN)\n",
    "- VOCAB (Lexeme : 46904...) (Lexeme : 37020...) (Lexeme : 31979...)\n",
    "- STRINGSTORE (4905... : \"I\") (37020... : \"love\") (31979... : \"coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5439657043933447811\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# look up hash for 'cat'\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# loop up cat_hash to get string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Docs, Spans, NE from Scratch**\n",
    "- spaCy under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "# manual creation doc\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world GREETING\n",
      "[('Hello world', 'GREETING')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "print(span_with_label.text, span_with_label.label_)\n",
    "\n",
    "doc.ents = [span_with_label]\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEST PRACTICES**\n",
    "\n",
    "- CONVERT RESULT TO STRINGS AS LATE AS POSSIBLE\n",
    "- USE TOKEN ATTRIBUTE IF AVAILABLE e.g. `token.i` for token index\n",
    "- ALWAYS PASS IN SHARED `vocab`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectors**\n",
    "\n",
    "`Doc.simiarlity(), Span.similarity(), Token.similarity()`\n",
    "\n",
    "**THREE WAY COMPARISON POSSIBLE**\n",
    "\n",
    "- Cosine default can be changed\n",
    "- `Doc` and `Span` default to average of `Token` vectors\n",
    "- Short phrases are better than long documents with many irrelevant words\n",
    "\n",
    "- Useful for many app: recomm, flagging duplicates, etc\n",
    "- Depends on context and what app needs to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Models and Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exact Match**\n",
    "\n",
    "COUNTRY a list of string names from json file\n",
    "\n",
    "```python\n",
    "# faster version of [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# create doc and find matches in it\n",
    "doc = nlp(TEXT) # some new text\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # create a Span with label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "    # overwrite doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "    # get span's root and head token\n",
    "    span_root_head = span.root.head\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "print([\n",
    "    (ent.text, ent.label_) for ent in doc.ents if ent.label_ ==\n",
    "    \"GPE\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Pipeline\n",
    "\n",
    "**Built-in Components**\n",
    "- **tagger** POS Token.tag\n",
    "- **parser** Dependency parser Token.dep, Token.head, Doc.sents, Doc.noun_chuncks\n",
    "- **ner** Doc.ents, Token.ent_iob, Token.ent_type\n",
    "- **textcat** Doc.cats\n",
    "\n",
    "**Custom Components**\n",
    "- Func takes `doc` and modifies it then returns it\n",
    "- can be added using `nlp.add_pipe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'animal_compoenent']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "def animal_compoenent(doc):\n",
    "    matches = matcher(doc)\n",
    "    # create Span for each match and assign label \n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # OVERWRITE doc.ents with matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(animal_compoenent, after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp(\"I have a cat and a Golden Retriever in my home in Montreal\")\n",
    "print([ (ent.text, ent.label_) for ent in doc.ents ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extension Attributes**\n",
    "\n",
    "- Add custom metadata to documents, tokens and spans\n",
    "- Accessible via `._` property\n",
    "\n",
    "```python\n",
    "doc._.title = \"My document\"\n",
    "token._.is_color = True\n",
    "span._.has_color = False\n",
    "```\n",
    "\n",
    "- Registered on global `Doc, Token, Span` using `set_extension`\n",
    "\n",
    "```python\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling and Performance**\n",
    "\n",
    "- SLOW `docs = [nlp(text) for text in BIG_TEXT]`\n",
    "- FAST `docs = list(nlp.pipe(BIG_TEXT))`\n",
    "- Passing in context \n",
    "    - `for doc, context in nlp.pipe(data, as_tuples=True): doc.text, context['page_number'] # self-defined`\n",
    "    - combined with `set_extension` in for loop to make `doc._.id and doc._.page_number` \n",
    "- Use ONLY `Tokenizer` disable other pipelines\n",
    "    - BAD: `doc = nlp(\"Hellow world')`\n",
    "    - GOOD: `doc = nlp.make_doc(\"Hellow world\")`\n",
    "    - `with nlp.disable_pipes('tagger', 'parser'): doc = nlp(text)'\n",
    "    \n",
    "```python\n",
    "# Example of pipe\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Network\n",
    "\n",
    "- Essential for custom Textcat and NER\n",
    "- Less critical for POS tagging and Dep parsing\n",
    "\n",
    "**FLOW**\n",
    "1. INIT model weights randomly with `nlp.begin_training`\n",
    "2. Predict a few examples with current weights by calling `nlp.update`\n",
    "3. Compare prediction with true labels\n",
    "4. Calculate how to change weights to improve predictions\n",
    "5. Update weigths slightly\n",
    "6. Go back to 2\n",
    "\n",
    "**NER Trainer**\n",
    "- NER tags words and phrases\n",
    "- Each token in mutually exclusive NE\n",
    "- Examples need CONTEXT!!\n",
    "    - `(\"iPhone X is coming\", {\"entities\": [(0, 8), 'GADGET')]})`\n",
    "\n",
    "**Texts with no entities also important!!**\n",
    "- `(\"I need a new phone! Any tipes?\", {\"entities\": []})`\n",
    "\n",
    "**Training Data**\n",
    "\n",
    "- Updating existing model = 00s to 000s examples\n",
    "- New category = 000s to 1,000,000 examples\n",
    "- Manual human annotators\n",
    "- Can be semi-automated - **Matcher**\n",
    "\n",
    "```python\n",
    "# make patterns for Matcher\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2) # two patterns for 'iPhone x and op ?`\n",
    "TRAINING_DATA = []\n",
    "\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # match on doc and create list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # get (start char, end char, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    # format matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # append to training\n",
    "    TRAINING_DATA.append(training_example)\n",
    "```\n",
    "\n",
    "**Training Loop**\n",
    "- Loop for times\n",
    "- Shuffle training data\n",
    "- Divide data into batches\n",
    "- Update model per batch\n",
    "- Save model\n",
    "\n",
    "```python\n",
    "for i in range(10):\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        nlp.update(texts, annotations)\n",
    "nlp.to_disk(path_to_model)\n",
    "```\n",
    "\n",
    "```python\n",
    "# new pipeline from scratch\n",
    "nlp = spacy.blank('en')\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "ner.add_label('GADGET')\n",
    "\n",
    "nlp.begin_training()\n",
    "for itn in range(10):\n",
    "    random.shuffle(examples)\n",
    "    losses = {}\n",
    "    for batch in spacy.util.minibatch(examples, size=2):\n",
    "        ...\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "        print(losses)\n",
    "```\n",
    "\n",
    "**BEST PRACTICES**\n",
    "- Existing model can overfit new data\n",
    "    - e.g. if only update with `WEBSITE`, it can 'unlearn' what a `PERSON` is\n",
    "    - Aka 'catastrophic forgetting' problem\n",
    "- Solution 1: Mix in previously correct preditions\n",
    "    - also include `PERSON` examples\n",
    "    - Run existing spaCy model over data and extract all other relevant entities !!\n",
    "\n",
    "- Models cannot learn everything\n",
    "    - predictions based on **local context**\n",
    "    - model can struggle to learn if decision hard to make based on context\n",
    "    - **label scheme needs be consistent and not too specific**\n",
    "        - e.g. `CLOTHING` is better than `ADULT_CLOTHING` and etc\n",
    "- Solution 2: Plan label scheme carefully\n",
    "    - pick categories reflecting local context\n",
    "    - more generic better than specific\n",
    "    - **use rules to go from generic labels to specific categories**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RsJPhaFPFWrE",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKGKYh2LFWrF"
   },
   "source": [
    "### Containers\n",
    "\n",
    "- DOC \n",
    "    - container of sequence of tokens & annotations\n",
    "    - owns data\n",
    "    - made via `Tokenizer`\n",
    "    - mod (inplace) via COMPONENTs of PIPELINE\n",
    "        - `Language` object coordinates COMPONENTS\n",
    "        - raw text -> pipeline -> annotated document\n",
    "        - orchestrate training & serialisation\n",
    "    - SPAN\n",
    "        - view pointer\n",
    "        - slice of `Doc`\n",
    "    - TOKEN\n",
    "        - view pointer\n",
    "        - {word, punctuation, symbol, whitespace, etc}\n",
    "\n",
    "- VOCAB\n",
    "    - look-up tables / meta \n",
    "    - LEXEME\n",
    "        - entry in vocabulary\n",
    "        - word type without annotations (opposed to token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB 0 2 ORG\n"
     ]
    }
   ],
   "source": [
    "# Overriding Labels in Span()\n",
    "\n",
    "from spacy.tokens import Span\n",
    "\n",
    "doc = nlp(u\"FB is hiring a new VP of global policy\")\n",
    "doc.ents = [Span(doc, 0, 1, label=doc.vocab.strings[u\"ORG\"])]\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kxh35Df93aj"
   },
   "source": [
    "### Pipeline\n",
    "\n",
    "- `Language`\n",
    "    - text-processing pipe\n",
    "    - load once per process as `nlp`\n",
    "    - pass instance around application\n",
    "- `Tokenizer`\n",
    "    - segment text & create `Doc`\n",
    "- `Lemmatizer`\n",
    "    - determine base forms of words\n",
    "- `Morphology`\n",
    "    - assign linguistic features \n",
    "    - {lemmas, noun case, verb tense, etc}\n",
    "    - based on word, POS\n",
    "- `Tagger`\n",
    "    - annotate POS on `Doc`\n",
    "- `DependencyParser`\n",
    "    - annotate syntactic dependencies on `Doc`\n",
    "- `EntityRecognizer`\n",
    "    - annotate NER\n",
    "- `TextCategorizer`\n",
    "    - assign categories or labels to `Doc`\n",
    "- `Matcher` \n",
    "    - match seq of tok based on pattern rules (similar to Regex)\n",
    "- `PhraseMatcher`\n",
    "    - match seq of tok based on prahses\n",
    "- `EntityRuler`\n",
    "    - add entity `span` to `Doc` using token-based rules or exact phrase mathces\n",
    "- `Sentencizer`\n",
    "    - custom sent boundary detection logic (no need dependency parsing)\n",
    "- Other func\n",
    "    - auto-apply sth to `Doc`, e.g. merge spans of tokens\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-QRhjEV96FA"
   },
   "source": [
    "### Other Classes\n",
    "\n",
    "- `Vocab`\t\n",
    "    - A lookup table for the vocabulary that allows you to access Lexeme objects.\n",
    "- `StringStore`\t\n",
    "    - Map strings to and from hash values.\n",
    "- `Vectors`\t\n",
    "    - Container class for vector data keyed by string.\n",
    "- `GoldParse`\t\n",
    "    - Collection for training annotations.\n",
    "- `GoldCorpus`\t\n",
    "    - An annotated corpus, using the JSON file format. Manages annotations for tagging, dependency parsing and NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmpwObT5FWrG"
   },
   "outputs": [],
   "source": [
    "# Example Text Pipeline\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tokens = nlp(u\"Some\\nspaces  and\\ttab characters\")\n",
    "tokens_text = [t.text for t in tokens]\n",
    "assert tokens_text == [\"Some\", \"\\n\", \"spaces\", \" \", \"and\", \"\\t\", \"tab\", \"characters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-KlUDIdXFWrJ",
    "outputId": "2972c169-ef1c-4c06-fc3f-9f1edbb5b641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adverb'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use spacy.explain to get the description for the string representation of a tag. \n",
    "import spacy\n",
    "spacy.explain(\"RB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7yktxlYFWrP",
    "outputId": "2af53fe6-51c4-4848-d0eb-cfb3787fcd26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'particle'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can also use spacy.explain to get the description for the string representation of a label.\n",
    "spacy.explain(\"prt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ym5AZWhsFWrU",
    "outputId": "0b04962a-1a2c-4de2-98fc-13159079c5de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Any named language'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dependency\n",
    "spacy.explain(\"LANGUAGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGBdydOHFWrY"
   },
   "source": [
    "### Models and Training Data\n",
    "\n",
    "#### JSON input format for training\n",
    "\n",
    "`convert` commmand converts `.conllu` format used by the Universal Dependencies corpora to training format\n",
    "\n",
    "```json\n",
    "[{\n",
    "    \"id\": int,                      # ID of the document within the corpus\n",
    "    \"paragraphs\": [{                # list of paragraphs in the corpus\n",
    "        \"raw\": string,              # raw text of the paragraph\n",
    "        \"sentences\": [{             # list of sentences in the paragraph\n",
    "            \"tokens\": [{            # list of tokens in the sentence\n",
    "                \"id\": int,          # index of the token in the document\n",
    "                \"dep\": string,      # dependency label\n",
    "                \"head\": int,        # offset of token head relative to token index\n",
    "                \"tag\": string,      # part-of-speech tag\n",
    "                \"orth\": string,     # verbatim text of the token\n",
    "                \"ner\": string       # BILUO label, e.g. \"O\" or \"B-ORG\"\n",
    "            }],\n",
    "            \"brackets\": [{          # phrase structure (NOT USED by current models)\n",
    "                \"first\": int,       # index of first token\n",
    "                \"last\": int,        # index of last token\n",
    "                \"label\": string     # phrase label\n",
    "            }]\n",
    "        }]\n",
    "    }]\n",
    "}]\n",
    "```\n",
    "> EXAMPLE: dep, POS, NER from Wall Street Journal portion of Penn Treebank\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "      \"id\": 42,\n",
    "      \"paragraphs\": [\n",
    "        {\n",
    "          \"raw\": \"In an Oct. 19 review of \\\"The Misanthrope\\\" at Chicago's Goodman Theatre (\\\"Revitalized Classics Take the Stage in Windy City,\\\" Leisure & Arts), the role of Celimene, played by Kim Cattrall, was mistakenly attributed to Christina Haag. Ms. Haag plays Elianti.\",\n",
    "          \"sentences\": [\n",
    "            {\n",
    "              \"tokens\": [\n",
    "                {\n",
    "                  \"head\": 44,\n",
    "                  \"dep\": \"prep\",\n",
    "                  \"tag\": \"IN\",\n",
    "                  \"orth\": \"In\",\n",
    "                  \"ner\": \"O\",\n",
    "                  \"id\": 0\n",
    "                },\n",
    "                {\n",
    "                  \"head\": 3,\n",
    "                  \"dep\": \"det\",\n",
    "                  \"tag\": \"DT\",\n",
    "                  \"orth\": \"an\",\n",
    "                  \"ner\": \"O\",\n",
    "                  \"id\": 1\n",
    "                },\n",
    "                {\n",
    "                  \"head\": 2,\n",
    "                  \"dep\": \"nmod\",\n",
    "                  \"tag\": \"NNP\",\n",
    "                  \"orth\": \"Oct.\",\n",
    "                  \"ner\": \"B-DATE\",\n",
    "                  \"id\": 2\n",
    "                },\n",
    "```\n",
    "\n",
    "#### Lexical data for Vocab\n",
    "\n",
    "- CLI `spacy init-model` to populate vocab loading in **JSONL** file `--jsonl-loc` option\n",
    "- first line defines lang and setting\n",
    "- rest of lines JSON objects desc lexemes\n",
    "- attr set as attributes `Lexeme`\n",
    "- `vocab` output ready-to-use model with `Vocab` containing lexical data\n",
    "\n",
    "**FIRST LINE**\n",
    "\n",
    "```json\n",
    "{\"lang\": \"en\", \"settings\": {\"oov_prob\": -20.502029418945312}}\n",
    "{\"orth\": \".\", \"id\": 1, \"lower\": \".\", \"norm\": \".\", \"shape\": \".\", \"prefix\": \".\", \"suffix\": \".\", \"length\": 1, \"cluster\": \"8\", \"prob\": -3.0678977966308594, \"is_alpha\": false, \"is_ascii\": true, \"is_digit\": false, \"is_lower\": false, \"is_punct\": true, \"is_space\": false, \"is_title\": false, \"is_upper\": false, \"like_url\": false, \"like_num\": false, \"like_email\": false, \"is_stop\": false, \"is_oov\": false, \"is_quote\": false, \"is_left_punct\": false, \"is_right_punct\": false}\n",
    "{\"orth\": \",\", \"id\": 2, \"lower\": \",\", \"norm\": \",\", \"shape\": \",\", \"prefix\": \",\", \"suffix\": \",\", \"length\": 1, \"cluster\": \"4\", \"prob\": -3.4549596309661865, \"is_alpha\": false, \"is_ascii\": true, \"is_digit\": false, \"is_lower\": false, \"is_punct\": true, \"is_space\": false, \"is_title\": false, \"is_upper\": false, \"like_url\": false, \"like_num\": false, \"like_email\": false, \"is_stop\": false, \"is_oov\": false, \"is_quote\": false, \"is_left_punct\": false, \"is_right_punct\": false}\n",
    "{\"orth\": \"the\", \"id\": 3, \"lower\": \"the\", \"norm\": \"the\", \"shape\": \"xxx\", \"prefix\": \"t\", \"suffix\": \"the\", \"length\": 3, \"cluster\": \"11\", \"prob\": -3.528766632080078, \"is_alpha\": true, \"is_ascii\": true, \"is_digit\": false, \"is_lower\": true, \"is_punct\": false, \"is_space\": false, \"is_title\": false, \"is_upper\": false, \"like_url\": false, \"like_num\": false, \"like_email\": false, \"is_stop\": false, \"is_oov\": false, \"is_quote\": false, \"is_left_punct\": false, \"is_right_punct\": false}\n",
    "{\"orth\": \"I\", \"id\": 4, \"lower\": \"i\", \"norm\": \"I\", \"shape\": \"X\", \"prefix\": \"I\", \"suffix\": \"I\", \"length\": 1, \"cluster\": \"346\", \"prob\": -3.791565179824829, \"is_alpha\": true, \"is_ascii\": true, \"is_digit\": false, \"is_lower\": false, \"is_punct\": false, \"is_space\": false, \"is_title\": true, \"is_upper\": true, \"like_url\": false, \"like_num\": false, \"like_email\": false, \"is_stop\": false, \"is_oov\": false, \"is_quote\": false, \"is_left_punct\": false, \"is_right_punct\": false}\n",
    "{\"orth\": \"to\", \"id\": 5, \"lower\": \"to\", \"norm\": \"to\", \"shape\": \"xx\", \"prefix\": \"t\", \"suffix\": \"to\", \"length\": 2, \"cluster\": \"12\", \"prob\": -3.8560216426849365, \"is_alpha\": true, \"is_ascii\": true, \"is_digit\": false, \"is_lower\": true, \"is_punct\": false, \"is_space\": false, \"is_title\": false, \"is_upper\": false, \"like_url\": false, \"like_num\": false, \"like_email\": false, \"is_stop\": false, \"is_oov\": false, \"is_quote\": false, \"is_left_punct\": false, \"is_right_punct\": false}\n",
    "...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJb4NWspFWrZ",
    "toc-hr-collapsed": true
   },
   "source": [
    "## CLI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aoDZpBIJFWra"
   },
   "source": [
    "### Download\n",
    "\n",
    "- **Model** \n",
    "    - installed as Python Packages like any module\n",
    "    - Chinese (None model yet) (dependencies = Jieba)\n",
    "    - en_core_web_sm/md/lg\n",
    "    - en_vectors_web_lg (631MB, 300-Dim 1070971 unique vectors)\n",
    "\n",
    "> **It’s not recommended to use this command as part of an automated process. If you know which model your project needs, you should consider a direct download via pip, or uploading the model to a local PyPi installation and fetching it straight from there. This will also allow you to add it as a versioned package dependency to your project.**\n",
    "\n",
    "```bash\n",
    "python -m spacy download [model] [--direct]\n",
    "```\n",
    "> **As of v2.0, spaCy expects all shortcut links to be loadable model packages. If you want to load a data directory, call spacy.load() or Language.from_disk() with the path, or use the package command to create a model package.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ezu-qsjk9Uif"
   },
   "source": [
    "\n",
    "### Info & Validate\n",
    "\n",
    "```bash\n",
    "python -m spacy info [model] [--markdown]\n",
    "```\n",
    "\n",
    "- find all models installed (packages and symlinks) and check compatibility with spaCy\n",
    "- run after `pip install -U spacy` to ensure\n",
    "- useful to detect off-sync links\n",
    "- use in production build process (1 for error)\n",
    "\n",
    "```bash\n",
    "python -m spacy validate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyDQeDn59bp6"
   },
   "source": [
    "### Convert\n",
    "\n",
    "- convert files into spaCy's JSON for `train`\n",
    "\n",
    "```bash\n",
    "python -m spacy convert [input_file] [output_dir] [--file-type] [--converter]\n",
    "[--n-sents] [--morphology] [--lang]\n",
    "```\n",
    "\n",
    "- default `jsonl` format\n",
    "- options\n",
    "    - `auto`: auto pick converter based on file ext\n",
    "    - `conll, conllu, conllubio`: Unniversal Dependences\n",
    "    - `ner`: tab-based NER\n",
    "    - `iob`: IOB or IOB2 NER\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YfXsfdGW9eaF"
   },
   "source": [
    "### Train\n",
    "\n",
    "- Input as JSON\n",
    "- each epoch, a model will be saved to DIR\n",
    "- **accuracy and details added to `meta.json` to allow packaging model using `package` CLI**\n",
    "- `--pipeline tagger, parser` will only train tagger and parse\n",
    "\n",
    "```bash\n",
    "python -m spacy train [lang] [output_path] [train_path] [dev_path]\n",
    "[--base-model] [--pipeline] [--vectors] [--n-iter] [--n-examples] [--use-gpu]\n",
    "[--version] [--meta-path] [--init-tok2vec] [--parser-multitasks]\n",
    "[--entity-multitasks] [--gold-preproc] [--noise-level] [--learn-tokens]\n",
    "[--verbose]\n",
    "```\n",
    "\n",
    "**DETAIL OPTIONS**\n",
    "\n",
    "`output_path`\tpositional\n",
    "- Directory to store model in. Will be created if it doesn’t exist.\n",
    "\n",
    "`train_path`\tpositional\n",
    "- Location of JSON-formatted training data. Can be a file or a directory of files.\n",
    "\n",
    "`dev_path`\tpositional\n",
    "- Location of JSON-formatted development data for evaluation. Can be a file or a directory of files.\n",
    "\n",
    "`--base-model, -b`\toption\t\n",
    "- Optional name of base model to update. Can be any loadable spaCy model.\n",
    "\n",
    "`--pipeline, -p`\toption\t\n",
    "- Comma-separated names of pipeline components to train. Defaults to 'tagger,parser,ner'.\n",
    "\n",
    "`--vectors, -v`\toption\t\n",
    "- Model to load vectors from.\n",
    "\n",
    "`--n-iter, -n`\toption\t\n",
    "- Number of iterations (default: 30).\n",
    "\n",
    "`--n-examples, -ns`\toption\t\n",
    "- Number of examples to use (defaults to 0 for all examples).\n",
    "\n",
    "`--use-gpu, -g`\toption\t\n",
    "- Whether to use GPU. Can be either 0, 1 or -1.\n",
    "\n",
    "`--version, -V`\toption\t\n",
    "- Model version. Will be written out to the model’s meta.json after training.\n",
    "\n",
    "`--meta-path, -m`\toption\t\n",
    "- Optional path to model meta.json. All relevant properties like lang, pipeline and spacy_version will be overwritten.\n",
    "\n",
    "`--init-tok2vec, -t2v` option\t\n",
    "- Path to pretrained weights for the token-to-vector parts of the models. See spacy pretrain. Experimental.\n",
    "\n",
    "`--parser-multitasks, -pt`\toption\t\n",
    "- Side objectives for parser CNN, e.g. 'dep' or 'dep,tag\n",
    "`--entity-multitasks, -et`\toption\t\n",
    "- Side objectives for NER CNN, e.g. 'dep' or 'dep,tag'\n",
    "\n",
    "`--noise-level, -nl`\toption\t\n",
    "- Float indicating the amount of corruption for data augmentation.\n",
    "\n",
    "`--gold-preproc, -G`\tflag\t\n",
    "- Use gold preprocessing.\n",
    "\n",
    "`--learn-tokens, -T`\tflag\n",
    "- Make parser learn gold-standard tokenization by merging ] subtokens. Typically used for languages like Chinese.\n",
    "\n",
    "`--verbose, -VV` flag\t\n",
    "- Show more detailed messages during training.\n",
    "\n",
    "`--help, -h`\tflag\t\n",
    "- Show help message and available arguments.\n",
    "\n",
    "**CUSTOM ENV VARIABLE**\n",
    "\n",
    "```bash\n",
    "token_vector_width=256 learn_rate=0.0001 spacy train [...]\n",
    "\n",
    "alias train-parser=\"python -m spacy train en /output /data /train /dev -n 1000\"\n",
    "token_vector_width=256 train-parser\n",
    "```\n",
    "\n",
    "`dropout_from`\n",
    "- Initial dropout rate.\t0.2\n",
    "\n",
    "`dropout_to`\n",
    "- Final dropout rate.\t0.2\n",
    "\n",
    "`dropout_decay`\n",
    "- Rate of dropout change.\t0.0\n",
    "\n",
    "`batch_from`\n",
    "- Initial batch size.\t1\n",
    "\n",
    "`batch_to`\n",
    "- Final batch size.\t64\n",
    "\n",
    "`batch_compound`\n",
    "- Rate of batch size acceleration.\t1.001\n",
    "\n",
    "`token_vector_width`\n",
    "- Width of embedding tables and convolutional layers.\t128\n",
    "\n",
    "`embed_size`\n",
    "- Number of rows in embedding tables.\t7500\n",
    "\n",
    "`hidden_width`\n",
    "- Size of the parser’s and NER’s hidden layers.\t128\n",
    "\n",
    "`learn_rate`\n",
    "- Learning rate.\t0.001\n",
    "\n",
    "`optimizer_B1`\n",
    "- Momentum for the Adam solver.\t0.9\n",
    "\n",
    "`optimizer_B2`\n",
    "- Adagrad-momentum for the Adam solver.\t0.999\n",
    "\n",
    "`optimizer_eps`\n",
    "- Epsilon value for the Adam solver.\t1e-08\n",
    "\n",
    "`L2_penalty`\n",
    "- L2 regularization penalty.\t1e-06\n",
    "\n",
    "`grad_norm_clip`\t\n",
    "- Gradient L2 norm constraint.\t1.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SqLmxDuv9jqd"
   },
   "source": [
    "### Pretrain\n",
    "\n",
    "- pretrain on `tok2vec` layer of pipeline component\n",
    "- using LMAO\n",
    "    - load pre-trained vector\n",
    "    - train a component like CNN, BiLSTM, etc\n",
    "    - predict vectors matching pre-trained ones\n",
    "    - weights saved per epoch\n",
    "    - pass a path to one of these weights files to `spacy train`\n",
    "- esp helful in little labelled data\n",
    "- experimental now, result varies\n",
    "- piping to train must ensure all settings identical\n",
    "\n",
    "```bash\n",
    "python -m spacy pretrain [texts_loc] [vectors_model] [output_dir] [--width]\n",
    "[--depth] [--embed-rows] [--dropout] [--seed] [--n-iter] [--use-vectors]\n",
    "```\n",
    "\n",
    "`texts_loc`\tpositional\n",
    "- Path to JSONL file with raw texts to learn from, with text provided as the key \"text\". See here for details.\n",
    "\n",
    "`vectors_model`\tpositional\n",
    "- Name or path to spaCy model with vectors to learn from.\n",
    "\n",
    "`output_dir`\tpositional\n",
    "- Directory to write models to on each epoch.\n",
    "\n",
    "`--width, -cw`\toption\n",
    "- Width of CNN layers.\n",
    "\n",
    "`--depth, -cd`\toption\n",
    "- Depth of CNN layers.\n",
    "\n",
    "`--embed-rows, -er`\toption\n",
    "- Number of embedding rows.\n",
    "\n",
    "`--dropout, -d`\toption\n",
    "- Dropout rate.\n",
    "\n",
    "`--batch-size, -bs`\toption\n",
    "- Number of words per training batch.\n",
    "\n",
    "`--max-length, -xw`\toption\n",
    "- Maximum words per example. Longer examples are discarded.\n",
    "\n",
    "`--min-length, -nw`\toption\n",
    "- Minimum words per example. Shorter examples are discarded.\n",
    "\n",
    "`--seed, -s`\toption\n",
    "- Seed for random number generators.\n",
    "\n",
    "`--n-iter, -i`\toption\n",
    "- Number of iterations to pretrain.\n",
    "\n",
    "`--use-vectors, -uv`\tflag\n",
    "- Whether to use the static vectors as input features.\n",
    "\n",
    "**JSONL format raw text**\n",
    "\n",
    "   > raw text can be provided as a .jsonl (newline-delimited JSON) file containing one input text per line (roughly paragraph length is good). Optionally, custom tokenization can be provided.<br>\n",
    "    > Our utility library `srsly` provides a handy `write_jsonl` helper that takes a file path and list of dictionaries and writes out JSONL-formatted data.\n",
    "\n",
    "```python\n",
    "import srsly\n",
    "data = [{\"text\": \"Some text\"}, {\"text\": \"More...\"}]\n",
    "srsly.write_jsonl(\"/path/to/text.jsonl\", data)\n",
    "``` \n",
    "<br>\n",
    "\n",
    "- Example\n",
    "\n",
    "```python\n",
    "{\"text\": \"Can I ask where you work now and what you do, and if you enjoy it?\"}\n",
    "{\"text\": \"They may just pull out of the Seattle market completely, at least until they have autonomous vehicles.\"}\n",
    "{\"text\": \"My cynical view on this is that it will never be free to the public. Reason: what would be the draw of joining the military? Right now their selling point is free Healthcare and Education. Ironically both are run horribly and most, that I've talked to, come out wishing they never went in.\"}\n",
    "```\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igcv_Mca9mZC"
   },
   "source": [
    "### INIT-Model\n",
    "\n",
    "- **Converting word vectors for use in spaCy**\n",
    "- create new model DIR from raw data like word freq, Brown clusteres and word vectors \n",
    "- similar to `spacy model`\n",
    "- output = model containing vocab and vectors\n",
    "\n",
    "> As of v2.1.0, the --freqs-loc and --clusters-loc are deprecated and have been replaced with the --jsonl-loc argument, which lets you pass in a a newline-delimited JSON (JSONL) file containing one lexical entry per line. For more details on the format, see the annotation specs.\n",
    "\n",
    "**EXAMPLE Using other Vectors**\n",
    "\n",
    "```shell\n",
    "wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.la.300.vec.gz\n",
    "python -m spacy init-model en /tmp/la_vectors_wiki_lg --vectors-loc cc.la.300.vec.gz\n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m spacy init-model [lang] [output_dir] [--jsonl-loc] [--vectors-loc]\n",
    "[--prune-vectors]\n",
    "```\n",
    "\n",
    "```python\n",
    "nlp_latin = spacy.load(\"/tmp/la_vectors_wiki_lg\")\n",
    "doc1 = nlp_latin(u\"Caecilius est in horto\")\n",
    "doc2 = nlp_latin(u\"servus est in atrio\")\n",
    "doc1.similarity(doc2)\n",
    "```\n",
    "\n",
    "`lang`\n",
    "- positional\tModel language ISO code, e.g. en.\n",
    "\n",
    "`output_dir`\n",
    "- positional\tModel output directory. Will be created if it doesn’t exist.\n",
    "\n",
    "`--jsonl-loc, -j`\n",
    "- option\tOptional location of JSONL-formatted vocabulary file with lexical attributes.\n",
    "\n",
    "`--vectors-loc, -v`\n",
    "- option\tOptional location of vectors file. Should be a tab-separated file in Word2Vec format where the first column contains the word and the remaining columns the values. File can be provided in .txt format or as a zipped text file in .zip or .tar.gz format.\n",
    "\n",
    "`--prune-vectors, -V`\n",
    "- flag\tNumber of vectors to prune the vocabulary to. Defaults to -1 for no pruning.\n",
    "\n",
    "**Optimizing vector coverage **\n",
    "\n",
    "- To help you strike a good balance between coverage and memory usage, spaCy’s Vectors class lets you map multiple keys to the same row of the table. If you’re using the spacy init-model command to create a vocabulary, pruning the vectors will be taken care of automatically if you set the --prune-vectors flag. You can also do it manually in the following steps:\n",
    "  1. Start with a word vectors model that covers a huge vocabulary. For instance, the en_vectors_web_lg model provides 300-dimensional GloVe vectors for over 1 million terms of English.\n",
    "  2. If your vocabulary has values set for the Lexeme.prob attribute, the lexemes will be sorted by descending probability to determine which vectors to prune. Otherwise, lexemes will be sorted by their order in the Vocab.\n",
    "  3. Call Vocab.prune_vectors with the number of vectors you want to keep.\n",
    "  \n",
    "  ```python\n",
    "  nlp = spacy.load('en_vectors_web_lg')\n",
    "  n_vectors = 105000  # number of vectors to keep\n",
    "  removed_words = nlp.vocab.prune_vectors(n_vectors)\n",
    "\n",
    "  assert len(nlp.vocab.vectors) <= n_vectors  # unique vectors have been pruned\n",
    "  assert nlp.vocab.vectors.n_keys > n_vectors  # but not the total entries\n",
    "  ```\n",
    "  > **Vocab.prune_vectors reduces the current vector table to a given number of unique entries, and returns a dictionary containing the removed words, mapped to (string, score) tuples, where string is the entry the removed word was mapped to, and score the similarity score between the two words.**\n",
    "  \n",
    "  - In the example above, the vector for “Shore” was removed and remapped to the vector of “coast”, which is deemed about 73% similar. “Leaving” was remapped to the vector of “leaving”, which is identical.\n",
    "  - If you’re using the init-model command, you can set the --prune-vectors option to easily reduce the size of the vectors as you add them to a spaCy model:\n",
    "  \n",
    "  ```shell\n",
    "  python -m spacy init-model /tmp/la_vectors_web_md --vectors-loc la.300d.vec.tgz --prune-vectors 10000\n",
    "  ```\n",
    "  \n",
    "  - This will create a spaCy model with vectors for the first 10,000 words in the vectors model. All other words in the vectors model are mapped to the closest vector among those retained.\n",
    "  \n",
    "**Loading GloVe vectors**\n",
    "\n",
    "- spaCy comes with built-in support for loading GloVe vectors from a directory. The Vectors.from_glove method assumes a binary format, the vocab provided in a vocab.txt, and the naming scheme of vectors.{size}.[fd.bin]. For example:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.vocab.vectors.from_glove(\"/path/to/vectors\")\n",
    "```\n",
    "\n",
    "- If your instance of Language already contains vectors, they will be overwritten. To create your own GloVe vectors model package like spaCy’s en_vectors_web_lg, you can call nlp.to_disk, and then package the model using the package command.\n",
    "\n",
    "**Storing Vectors on GPU (Chain or PyTorch) https://spacy.io/usage/vectors-similarity#gpu**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzFiF3oP9o_5"
   },
   "source": [
    "### Evaluate\n",
    "\n",
    "- accuracy and speed on JSON-annotated data\n",
    "- print results with displaCy\n",
    "\n",
    "```bash\n",
    "python -m spacy evaluate [model] [data_path] [--displacy-path] [--displacy-limit]\n",
    "[--gpu-id] [--gold-preproc]\n",
    "```\n",
    "\n",
    "`model`\n",
    "- positional\tModel to evaluate. Can be a package or shortcut link name, or a path to a model data directory.\n",
    "\n",
    "`data_path`\n",
    "- positional\tLocation of JSON-formatted evaluation data.\n",
    "\n",
    "`--displacy-path, -dp`\n",
    "- option\tDirectory to output rendered parses as HTML. If not set, no visualizations will be generated.\n",
    "\n",
    "`--displacy-limit, -dl`\n",
    "- option\tNumber of parses to generate per file. Defaults to 25. Keep in mind that a significantly higher number might cause the .html files to render slowly.\n",
    "\n",
    "`--gpu-id, -g`\n",
    "- option\tGPU to use, if any. Defaults to -1 for CPU.\n",
    "\n",
    "`--gold-preproc, -G`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAL3Ark_9qiD"
   },
   "source": [
    "### Package\n",
    "\n",
    "- generate model package from existing data DIR\n",
    "- if path to `meta.json`, used\n",
    "- else data entered from CLI\n",
    "- `python setup.py sdist` from newly created DIR to turn model into installable archive file\n",
    "\n",
    "```bash\n",
    "python -m spacy package [input_dir] [output_dir] [--meta-path] [--create-meta] [--force]\n",
    "```\n",
    "\n",
    "**Example**\n",
    "\n",
    "```bash\n",
    "python -m spacy package /input /output\n",
    "cd /output/en_model-0.0.0\n",
    "python setup.py sdist\n",
    "pip install dist/en_model-0.0.0.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5_ROUqgFWra"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5wzLaofFWrf"
   },
   "source": [
    "# Serialisation & Packaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Model\n",
    "\n",
    "- Custom `tokenizer` self-serialised as JSON\n",
    "- **custom componenet** not - best way to wrap as Python package\n",
    "- `spacy package` in model dir saved -> create all files needed to package (`__init__.py` consisting of `load()` for calling `spacy.load` equal:\n",
    "    ```python\n",
    "    import en_core_web_sm\n",
    "    nlp = en_core_web_sm.load()\n",
    "    ```\n",
    "- **QUICK-DIRTY** way to add all custom code to `__init__.py` and add `CustomEntityRecognizer` to global factories:\n",
    "    ```python\n",
    "    from spacy.language import Language\n",
    "    # add custom NER to global factories\n",
    "    Language.factories['CustomEntityRecognizer'] = CustomEntityRecognizer\n",
    "    ```\n",
    "- **Everything** needs to be available from within the package; also specify additional dependencies in `setup.py`, adding files for modules etc\n",
    "- Once done, `python setup.py sdist` to build package adding `.tar.gz` to `/dist` for `pip install` usage within dir later\n",
    "\n",
    "#### More Elegantly\n",
    "\n",
    "**ADD/SERIALISATION**\n",
    "- by add `to_disk, from_disk, to_bytes, from_bytes` methods\n",
    "- `nlp.from_disk` iterates over pipeline and checks for methods:\n",
    "    ```python\n",
    "    from pipe_name, proc in nlp.pipeline:\n",
    "        if hasattr(proc, 'from_disk'):\n",
    "            proc.from_disk(model_path/pipe_name)\n",
    "    ```\n",
    "- under `/path/to/model/custom_ner/`:\n",
    "\n",
    "    ```python\n",
    "    class CustomEntityRecognizer:\n",
    "        name = 'custom_ner'\n",
    "        def __init__(self, nlp):\n",
    "            self.vocab = nlp.vocab\n",
    "            self.some_data = None\n",
    "        def __call__(self, spacy_doc, *args, **kwargs):\n",
    "            return predcit_single(spacy_doc)\n",
    "        def from_disk(self, path, *kwargs):\n",
    "            # do sth here and load all data needed\n",
    "            data_path = path/'some_data.json'\n",
    "            with data_path.open() as f:\n",
    "                self.some_data = json.load(f)\n",
    "    ```\n",
    "\n",
    "**TL;DR**\n",
    "\n",
    "1. Save out model with custom tokenizer only\n",
    "2. `spacy package` in saved dir, edit meta etc\n",
    "3. Edit `__init__.py` include custom component, custom NER model etc and add entry to global factories\n",
    "4. `python setup.py sdist` within package dir to build package\n",
    "5. Install `.tar.gz` model created in `/dist` \n",
    "\n",
    "\n",
    "### Factories via ENTRY POINT\n",
    "\n",
    "```python\n",
    "# SERIALIZE\n",
    "\n",
    "bytes_data = nlp.to_bytes()\n",
    "lang = nlp.meta[\"lang\"]  # \"en\"\n",
    "pipeline = nlp.meta[\"pipeline\"]  # [\"tagger\", \"parser\", \"ner\"]\n",
    "\n",
    "# DESERIALIZE\n",
    "\n",
    "nlp = spacy.blank(lang)\n",
    "for pipe_name in pipeline:\n",
    "    pipe = nlp.create_pipe(pipe_name)\n",
    "    nlp.add_pipe(pipe)\n",
    "nlp.from_bytes(bytes_data)\n",
    "```\n",
    "\n",
    "```python\n",
    "# SPACY.LOAD UNDER THE HOOD\n",
    "\n",
    "lang = \"en\"\n",
    "pipeline = [\"tagger\", \"parser\", \"ner\"]\n",
    "data_path = \"path/to/en_core_web_sm/en_core_web_sm-2.0.0\"\n",
    "\n",
    "cls = spacy.util.get_lang_class(lang)   # 1. Get Language instance, e.g. English()\n",
    "nlp = cls()                             # 2. Initialize it\n",
    "for name in pipeline:\n",
    "    component = nlp.create_pipe(name)   # 3. Create the pipeline components\n",
    "    nlp.add_pipe(component)             # 4. Add the component to the pipeline\n",
    "nlp.from_disk(model_data_path)          # 5. Load in the binary data\n",
    "\n",
    "\n",
    "#THE PIPELINE UNDER THE HOOD\n",
    "\n",
    "doc = nlp.make_doc(u\"This is a sentence\")   # create a Doc from raw text\n",
    "for name, proc in nlp.pipeline:             # iterate over components in order\n",
    "    doc = proc(doc)                         # apply each component\n",
    "    \n",
    "```\n",
    "\n",
    "**Using Pickle**\n",
    "\n",
    "- pickling `Doc` or `EntityRecognizer` beware of all requiring common `vocab` (including string2has mappings, label schemes and optional vectors) - CANNOT be too large\n",
    "\n",
    "```python\n",
    "# PICKLING OBJECTS WITH SHARED DATA\n",
    "\n",
    "doc1 = nlp(u\"Hello world\")\n",
    "doc2 = nlp(u\"This is a test\")\n",
    "\n",
    "doc1_data = pickle.dumps(doc1)\n",
    "doc2_data = pickle.dumps(doc2)\n",
    "print(len(doc1_data) + len(doc2_data))  # 6636116 😞\n",
    "\n",
    "doc_data = pickle.dumps([doc1, doc2])print(len(doc_data))  # 3319761 😃\n",
    "```\n",
    "\n",
    "### Example\n",
    "- `EntityRuler` component, patterns saved as `.jsonl` if pipeline to_disk, and to a bytestring if pipeline to_bytes - allowing saving out model with rule-based ENR and incluidng all rules WITH the model data\n",
    "\n",
    "```python\n",
    "class CustomComponent(object):\n",
    "    name = \"my_component\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # Do something to the doc here\n",
    "        return doc\n",
    "\n",
    "    def add(self, data):\n",
    "        # Add something to the component's data\n",
    "        self.data.append(data)\n",
    "\n",
    "    def to_disk(self, path):\n",
    "        # This will receive the directory path + /my_component\n",
    "        data_path = path / \"data.json\"\n",
    "        with data_path.open(\"w\", encoding=\"utf8\") as f:\n",
    "            f.write(json.dumps(self.data))\n",
    "\n",
    "    def from_disk(self, path, **cfg):\n",
    "        # This will receive the directory path + /my_component\n",
    "        data_path = path / \"data.json\"\n",
    "        with data_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "            self.data = json.loads(f)\n",
    "        return self\n",
    "```\n",
    "\n",
    "- After adding Component to pipeline and adding some data to it, cna seriliase `nlp` object to dir\n",
    "- will call custom component's `to_disk` method\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "my_component = CustomComponent()\n",
    "my_component.add({\"hello\": \"world\"})\n",
    "nlp.add_pipe(my_component)\n",
    "nlp.to_disk(\"/path/to/model\")\n",
    "```\n",
    "\n",
    "```bash\n",
    "DIRECTORY STRUCTURE\n",
    "└── /path/to/model\n",
    "    ├── my_component     # data serialized by \"my_component\"\n",
    "    |   └── data.json\n",
    "    ├── ner              # data for \"ner\" component\n",
    "    ├── parser           # data for \"parser\" component\n",
    "    ├── tagger           # data for \"tagger\" component\n",
    "    ├── vocab            # model vocabulary\n",
    "    ├── meta.json        # model meta.json with name, language and pipeline\n",
    "    └── tokenizer        # tokenization rules\n",
    "```\n",
    "\n",
    "**NOTE on loading components**\n",
    "- `meta.json` check to look up compoentn name in internal factories\n",
    "- ensure spacy to INIT `my_component` :\n",
    "\n",
    "```python\n",
    "from spacy.language import Language\n",
    "Language.factories[\"my_component\"] = lambda nlp, **cfg: CustomComponent()\n",
    "```\n",
    "\n",
    "#### ENTRY POINT\n",
    "\n",
    "- specificall, `nlp.create_pipe` and look up in **factories**\n",
    "- Must write to `Language.factories` **BEFORE** loading model\n",
    "\n",
    "```python\n",
    "pipe = nlp.create_pipe(\"custom_component\")  # fails 👎\n",
    "\n",
    "Language.factories[\"custom_component\"] = CustomComponentFactory\n",
    "pipe = nlp.create_pipe(\"custom_component\")  # works 👍\n",
    "```\n",
    "\n",
    "- this is messy and often requires INIT code shipped with model\n",
    "- Using **ENTRY POINT** model pkg and ext pkg can define own `spacy_factories` to add and INIT\n",
    "- automated in package in same ENV exposes spacy entry points - **SNEK example**\n",
    "\n",
    "```bash\n",
    "PACKAGE DIRECTORY STRUCTURE\n",
    "├── snek.py   # the extension code\n",
    "└── setup.py  # setup file for pip installation\n",
    "```\n",
    "\n",
    "```python\n",
    "# SNEK.PY\n",
    "snek = \"\"\"\n",
    "    --..,_                     _,.--.\n",
    "       `'.'.                .'`__ o  `;__.\n",
    "          '.'.            .'.'`  '---'`  `\n",
    "            '.`'--....--'`.'\n",
    "              `'--....--'`\n",
    "\"\"\"\n",
    "\n",
    "class SnekFactory(object):\n",
    "    def __init__(self, nlp, **cfg):\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        print(snek)\n",
    "        return doc\n",
    "```\n",
    "\n",
    "- adding entry to factories need exposing it in `setup.py` via `entry_point`:\n",
    "\n",
    "```python\n",
    "# SETUP.PY\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name=\"snek\",\n",
    "    entry_points={\n",
    "        \"spacy_factories\": [\n",
    "            \"snek = snek:SnekFactory\"\n",
    "         ]\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "- Entry Point lets spacy name `snek` found in module `snek` (i.e. `snek.py`) as `SnekFactory`\n",
    "- same package can expose multiple EP \n",
    "- to make them available to spaCy, install via `python setup.py develop`\n",
    "- now from spacy::\n",
    "\n",
    "```python\n",
    ">>> from spacy.lang.en import English\n",
    ">>> nlp = English()\n",
    ">>> snek = nlp.create_pipe(\"snek\")  # this now works! 🐍🎉\n",
    ">>> nlp.add_pipe(snek)\n",
    ">>> doc = nlp(u\"I am snek\")\n",
    "    --..,_                     _,.--.\n",
    "       `'.'.                .'`__ o  `;__.\n",
    "          '.'.            .'.'`  '---'`  `\n",
    "            '.`'--....--'`.'\n",
    "              `'--....--'`\n",
    "```\n",
    "\n",
    "**ADVANCED COMPONENTS WITH SETTINGS `**cfg`**\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_snek_sm\", snek_style=\"cute\")\n",
    "\n",
    "# how\n",
    "SNEKS = {\"basic\": snek, \"cute\": cute_snek}  # collection of sneks\n",
    "\n",
    "class SnekFactory(object):\n",
    "    def __init__(self, nlp, **cfg):\n",
    "        self.nlp = nlp\n",
    "        self.snek_style = cfg.get(\"snek_style\", \"basic\")\n",
    "        self.snek = SNEKS[self.snek_style]\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        print(self.snek)\n",
    "        return doc\n",
    "\n",
    "    def to_disk(self, path):\n",
    "    snek_path = path / \"snek.txt\"\n",
    "    with snek_path.open(\"w\", encoding=\"utf8\") as snek_file:\n",
    "        snek_file.write(self.snek)\n",
    "\n",
    "    def from_disk(self, path, **cfg):\n",
    "        snek_path = path / \"snek.txt\"\n",
    "        with snek_path.open(\"r\", encoding=\"utf8\") as snek_file:\n",
    "            self.snek = snek_file.read()\n",
    "        return self\n",
    "```\n",
    "\n",
    "**CUSTOM LANGUAGE CLASSES VIA ENTRY POINT**\n",
    "\n",
    "- `SnekLanguage` class for custom model BUT not modifying code to add a language\n",
    "\n",
    "```python\n",
    "#SNEK.PY\n",
    "\n",
    "from spacy.language import Language\n",
    "from spacy.attrs import LANG\n",
    "\n",
    "class SnekDefaults(Language.Defaults):\n",
    "    lex_attr_getters = dict(Language.Defaults.lex_attr_getters)\n",
    "    lex_attr_getters[LANG] = lambda text: \"snk\"\n",
    "\n",
    "\n",
    "class SnekLanguage(Language):\n",
    "    lang = \"snk\"\n",
    "    Defaults = SnekDefaults\n",
    "    # Some custom snek language stuff here\n",
    "```\n",
    "\n",
    "- Alongside `spacy_factories` also EP opton for `spacy_language` mapping language codes to language-specific `Language` subclasses:\n",
    "\n",
    "```python\n",
    "#SETUP.PY\n",
    "\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name=\"snek\",\n",
    "    entry_points={\n",
    "        \"spacy_factories\": [\n",
    "            \"snek = snek:SnekFactory\"\n",
    "         ]\n",
    "+       \"spacy_languages\": [\n",
    "+           \"sk = snek:SnekLanguage\"\n",
    "+       ]\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "- Then load custom `sk` language and resolved to `SnekLanguage` via custom EP\n",
    "- e.g. `meta.json` specifying `\"lang\": \"snk\"\n",
    "\n",
    "```python\n",
    "from spacy.util import get_lang_class\n",
    "\n",
    "SnekLanguage = get_lang_class(\"snk\")\n",
    "nlp = SnekLanguage()\n",
    "```\n",
    "\n",
    "**Distribution of Model**\n",
    "\n",
    "- `Language.to_disk()`\n",
    "- Dir created writing out WHOLE pipeline\n",
    "- Deploy via wrapping as Python package\n",
    "- **CLI** \n",
    "\n",
    "```bash\n",
    "python -m spacy package /home/me/data/en_example_model /home/me/my_models\n",
    "\n",
    "# creating\n",
    "DIRECTORY STRUCTURE\n",
    "└── /\n",
    "    ├── MANIFEST.in                   # to include meta.json\n",
    "    ├── meta.json                     # model meta data\n",
    "    ├── setup.py                      # setup file for pip installation\n",
    "    └── en_example_model              # model directory\n",
    "        ├── __init__.py               # init for pip installation\n",
    "        └── en_example_model-1.0.0    # model data\n",
    "```\n",
    "\n",
    "- eware of directories need to be name per naming conventions of `lang_name` and `lang_name-version`\n",
    "\n",
    "**Custom Model Setup**\n",
    "\n",
    "- `load()` method coming with model package tempaltes will handle assembling and returning `Language` object with loaded pipeline and data\n",
    "- If requiring custom pipeline component / custom language class => **ship code with model**\n",
    "- For examples of this, check out the implementations of spaCy’s [`load_model_from_init_py`](https://spacy.io/api/top-level#util.load_model_from_init_py) and [`load_model_from_path`](https://spacy.io/api/top-level#util.load_model_from_path) utility functions.\n",
    "\n",
    "**Building Model PKG**\n",
    "\n",
    "- `python setup.py sdist`\n",
    "- `pip install /path/to/ex_xxx.tar.gz`\n",
    "- **Loading only binary data** => `nlp = spacy.blank('en').from_disk('/path/to/data')\n",
    "\n",
    "Publishing a new version of spaCy often means re-training all available models, which is [quite a lot](https://spacy.io/usage/models#languages). To make this run smoothly, we’re using an automated build process and a [`spacy train`](https://spacy.io/api/cli#train) template that looks like this:\n",
    "\n",
    "```bash\n",
    "python -m spacy train {lang} {models_dir}/{name} {train_data} {dev_data} -m meta/{name}.json -V {version} -g {gpu_id} -n {n_epoch} -ns {n_sents}\n",
    "```\n",
    "\n",
    "In a directory `meta`, we keep `meta.json` templates for the individual models, containing all relevant information that doesn’t change across versions, like the name, description, author info and training data sources. When we train the model, we pass in the file to the meta template as the `--meta` argument, and specify the current model version as the `--version` argument\n",
    "\n",
    "On each epoch, the model is saved out with a `meta.json` using our template and added properties, like the `pipeline`, `accuracy` scores and the `spacy_version` used to train the model. After training completion, the best model is selected automatically and packaged using the [`package`](https://spacy.io/api/cli#package) command. Since a full meta file is already present on the trained model, no further setup is required to build a valid model package.\n",
    "\n",
    "```bash\n",
    "python -m spacy package -f {best_model} dist/\n",
    "cd dist/{model_name}\n",
    "python setup.py sdist\n",
    "```\n",
    "\n",
    "This process allows us to quickly trigger the model training and build process for all available models and languages, and generate the correct meta data automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "customer_feedback = open(\"customer_feedback_627.txt\").read()\n",
    "doc = nlp(customer_feedback)\n",
    "doc.to_disk(\"/tmp/customer_feedback_627.bin\")\n",
    "\n",
    "new_doc = Doc(Vocab()).from_disk(\"/tmp/customer_feedback_627.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oF6d9LJSFWri"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for doc in textcat.pipe(docs, batch_size=50):\n",
    "    pass\n",
    "\n",
    "scores = textcat.predict([doc1, doc2])\n",
    "\n",
    "textcat.set_annotations([doc1, doc2], scores)\n",
    "\n",
    "losses = {}\n",
    "optimizer = nlp.begin_training()\n",
    "textcat.update([doc1, doc2], [gold1, gold2], losses=losses, sgd=optimizer)\n",
    "\n",
    "optimizer = textcat.begin_training(pipeline=nlp.pipeline)\n",
    "# An optional optimizer. Should take two arguments weights and gradient, and an optional ID. Will be created via TextCategorizer if not set.\n",
    "\n",
    "# demo\n",
    "optimizer = nlp.begin_training(get_data)\n",
    "for itn in range(100):\n",
    "    random.shuffle(train_data)\n",
    "    for raw_text, entity_offsets in train_data:\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        gold = GoldParse(doc, entities=entity_offsets)\n",
    "        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)\n",
    "nlp.to_disk(\"/model\")\n",
    "\n",
    "\n",
    "# recommended simple training format\n",
    "{\n",
    "   \"entities\": [(0, 4, \"ORG\")],\n",
    "   \"heads\": [1, 1, 1, 5, 5, 2, 7, 5],\n",
    "   \"deps\": [\"nsubj\", \"ROOT\", \"prt\", \"quantmod\", \"compound\", \"pobj\", \"det\", \"npadvmod\"],\n",
    "   \"tags\": [\"PROPN\", \"VERB\", \"ADP\", \"SYM\", \"NUM\", \"NUM\", \"DET\", \"NOUN\"],\n",
    "   \"cats\": {\"BUSINESS\": 1.0},\n",
    "}\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        nlp.update([text], [annotations], sgd=optimizer)\n",
    "nlp.to_disk(\"/model\")\n",
    "\n",
    "# BATCH HEURISTIC\n",
    "def get_batches(train_data, model_type):\n",
    "    max_batch_sizes = {\"tagger\": 32, \"parser\": 16, \"ner\": 16, \"textcat\": 64}\n",
    "    max_batch_size = max_batch_sizes[model_type]\n",
    "    if len(train_data) < 1000:\n",
    "        max_batch_size /= 2\n",
    "    if len(train_data) < 500:\n",
    "        max_batch_size /= 2\n",
    "    batch_size = compounding(1, max_batch_size, 1.001)\n",
    "    batches = minibatch(train_data, size=batch_size)\n",
    "    return batches\n",
    "```\n",
    "\n",
    "> This will set the batch size to start at 1, and increase each batch until it reaches a maximum size. The tagger, parser and entity recognizer all take whole sentences as input, so they’re learning a lot of labels in a single example. You therefore need smaller batches for them. The batch size for the text categorizer should be somewhat larger, especially if your documents are long.\n",
    "\n",
    "> By default spaCy uses the Adam solver, with default settings (learning rate 0.001, beta1=0.9, beta2=0.999). Some researchers have said they found these settings terrible on their problems – but they’ve always performed very well in training spaCy’s models, in combination with the rest of our recipe. You can change these settings directly, by modifying the corresponding attributes on the optimizer object. You can also set environment variables, to adjust the defaults.\n",
    "\n",
    "> There are two other key hyper-parameters of the solver: L2 regularization, and gradient clipping (max_grad_norm). Gradient clipping is a hack that’s not discussed often, but everybody seems to be using. It’s quite important in helping to ensure the network doesn’t diverge, which is a fancy way of saying “fall over during training”. The effect is sort of similar to setting the learning rate low. It can also compensate for a large batch size (this is a good example of how the choices of all these hyper-parameters intersect).\n",
    "\n",
    "> For small datasets, it’s useful to set a high dropout rate at first, and decay it down towards a more reasonable value. This helps avoid the network immediately overfitting, while still encouraging it to learn some of the more interesting things in your data. spaCy comes with a decaying utility function to facilitate this. You might try setting:\n",
    "\n",
    "```python\n",
    "from spacy.util import decaying\n",
    "dropout = decaying(0.6, 0.2, 1e-4)\n",
    "```\n",
    "\n",
    "> The trick is to store the moving average of the weights during training. We don’t optimize this average – we just track it. Then when we want to actually use the model, we use the averages, not the most recent value. In spaCy (and Thinc) this is done by using a context manager, use_params, to temporarily replace the weights:\n",
    "\n",
    "```python\n",
    "with nlp.use_params(optimizer.averages):\n",
    "    nlp.to_disk(\"/model\")\n",
    "```\n",
    "\n",
    "> The context manager is handy because you naturally want to evaluate and save the model at various points during training (e.g. after each epoch). After evaluating and saving, the context manager will exit and the weights will be restored, so you resume training from the most recent value, rather than the average. By evaluating the model after each epoch, you can remove one hyper-parameter from consideration (the number of epochs). Having one less magic number to guess is extremely nice – so having the averaging under a context manager is very convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Rule-Based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Based\n",
    "\n",
    "**Adding 3 patterns**\n",
    "\n",
    "- 'hello' or 'HELLO'\n",
    "- is_punct flag == True\n",
    "- lowercase == \"world\"\n",
    "\n",
    "```python\n",
    "[{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "```\n",
    "\n",
    "**Important note**\n",
    "\n",
    "> When writing patterns, keep in mind that each dictionary represents one token. If spaCy’s tokenization doesn’t match the tokens defined in a pattern, the pattern is not going to produce any results. When developing complex patterns, make sure to check examples against spaCy’s tokenization:\n",
    "\n",
    "```python\n",
    "doc = nlp(u\"A complex-example,!\")\n",
    "print([token.text for token in doc])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# vocab must be shared with document the matcher operates on\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", None, pattern) # first arg is ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15578876784678163569 HelloWorld 0 3 Hello, World\n"
     ]
    }
   ],
   "source": [
    "# load text using same nlp object (hence same vocab space)\n",
    "doc = nlp(u\"Hello, World! Hello world!\")\n",
    "# operate on doc\n",
    "matches = matcher(doc) \n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15578876784678163569 HelloWorld 0 3 Hello, World\n",
      "15578876784678163569 HelloWorld 4 6 Hello world\n"
     ]
    }
   ],
   "source": [
    "# Optionally, we could also choose to add more than one pattern, \n",
    "# for example to also match sequences without punctuation between “hello” and “world”:\n",
    "\n",
    "matcher.add(\"HelloWorld\", None,\n",
    "            [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}],\n",
    "            [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default, the matcher will only return the matches and not do anything else, like merge entities or assign labels. This is all up to you and can be defined individually for each pattern, by passing in a callback function as the `on_match` argument on `add()`. This is useful, because it lets you write entirely custom and pattern-specific logic. For example, you might want to **merge some patterns into one token, while adding entity labels for other pattern types**. You shouldn’t have to create different matchers for each of those processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Available token attributes**\n",
    "\n",
    "`ORTH` \n",
    "- unicode\tThe exact verbatim text of a token.\n",
    "\n",
    "`TEXT` \n",
    "- V2.1\tunicode\tThe exact verbatim text of a token.\n",
    "\n",
    "`LOWER` \n",
    "- unicode\tThe lowercase form of the token text.\n",
    "\n",
    "`LENGTH` \n",
    "- int\tThe length of the token text.\n",
    "\n",
    "`IS_ALPHA` \n",
    "- , IS_ASCII, IS_DIGIT\tbool\tToken text consists of alphanumeric characters, ASCII characters, digits.\n",
    "\n",
    "`IS_LOWER` \n",
    "- , IS_UPPER, IS_TITLE\tbool\tToken text is in lowercase, uppercase, titlecase.\n",
    "\n",
    "`IS_PUNCT` \n",
    "- , IS_SPACE, IS_STOP\tbool\tToken is punctuation, whitespace, stop word.\n",
    "\n",
    "`LIKE_NUM` \n",
    "- , LIKE_URL, LIKE_EMAIL\tbool\tToken text resembles a number, URL, email.\n",
    "\n",
    "`POS` \n",
    "- , TAG, DEP, LEMMA, SHAPE\tunicode\tThe token’s simple and extended part-of-speech tag, dependency label, lemma, shape.\n",
    "\n",
    "`ENT_TYPE` \n",
    "- unicode\tThe token’s entity label.\n",
    "\n",
    "`_` \n",
    "- V2.1\tdict\tProperties in custom extension attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extended pattern syntax and attributes V2.1**\n",
    "\n",
    "> Instead of mapping to a single value, token patterns can also map to a `dictionary of properties`. For example, to specify that the value of a lemma should be part of a list of values, or to set a minimum character length. The following rich comparison attributes are available:\n",
    "\n",
    "```python\n",
    "# Matches \"love cats\" or \"likes flowers\"\n",
    "pattern1 = [{\"LEMMA\": {\"IN\": [\"like\", \"love\"]}},\n",
    "            {\"POS\": \"NOUN\"}] # NOT_IN, ==, >=, etc\n",
    "\n",
    "# Matches tokens of length >= 10\n",
    "pattern2 = [{\"LENGTH\": {\">=\": 10}}]\n",
    "```\n",
    "\n",
    "**REGEX**\n",
    "\n",
    "In some cases, only matching tokens and token attributes isn’t enough – for example, you might want to match different spellings of a word, without having to add a new pattern for each spelling.\n",
    "\n",
    "```python\n",
    "pattern = [{\"TEXT\": {\"REGEX\": \"^([Uu](\\.?|nited) ?[Ss](\\.?|tates)\"}},\n",
    "           {\"LOWER\": \"president\"}]\n",
    "```\n",
    "\n",
    "> 'REGEX' as an operator (instead of a top-level property that only matches on the token’s text) allows defining rules for any string value, including custom attributes\n",
    "\n",
    "```python\n",
    "# Match tokens with fine-grained POS tags starting with 'V'\n",
    "pattern = [{\"TAG\": {\"REGEX\": \"^V\"}}]\n",
    "\n",
    "# Match custom attribute values with regular expressions\n",
    "pattern = [{\"_\": {\"country\": {\"REGEX\": \"^([Uu](\\.?|nited) ?[Ss](\\.?|tates)\"}}}]\n",
    "```\n",
    "**Operators and quantifiers**\n",
    "\n",
    "The matcher also lets you use quantifiers, specified as the `OP` key. Quantifiers let you define sequences of tokens to be matched, e.g. one or more punctuation marks, or specify optional tokens. Note that there are no nested or scoped quantifiers – instead, you can build those behaviors with `on_match` callbacks.\n",
    "\n",
    "`!` \n",
    "- Negate the pattern, by requiring it to match exactly 0 times.\n",
    "\n",
    "`?` \n",
    "- Make the pattern optional, by allowing it to match 0 or 1 times.\n",
    "\n",
    "`+` \n",
    "- Require the pattern to match 1 or more times.\n",
    "\n",
    "`*` \n",
    "- Allow the pattern to match zero or more times.\n",
    "\n",
    "```python\n",
    "pattern = [{\"LOWER\": \"hello\"},\n",
    "           {\"IS_PUNCT\": True, \"OP\": \"?\"}]\n",
    "```\n",
    "\n",
    "**Wildcard**\n",
    "- empty dictionary, `{}` as a wildcard representing any token. \n",
    "- useful if you know the context of what you’re trying to match, but very little about the specific token and its characters.\n",
    "- For example, let’s say you’re trying to extract people’s user names from your data. All you know is that they are listed as `“User name: {username}“`. The name itself may contain any character, but no whitespace – so you’ll know it will be handled as one token.\n",
    "\n",
    "```python\n",
    "[{\"ORTH\": \"User\"}, {\"ORTH\": \"name\"}, {\"ORTH\": \":\"}, {}]\n",
    "```\n",
    "\n",
    "**Adding on_match rules**\n",
    "\n",
    "- See below demo\n",
    "> match all mentions of “Google I/O” (which spaCy tokenizes as `['Google', 'I', '/', 'O']`). To be safe, you only match on the uppercase versions, in case someone has written it as “Google i/o”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"EVENT\")\n",
    "    doc.ents += (entity,)\n",
    "    print(entity.text)\n",
    "\n",
    "pattern = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
    "matcher.add(\"GoogleIO\", add_event_ent, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"This is a text about Google I/O.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">This is a text about Google I/O.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "html = displacy.render(doc, style=\"ent\", page=True,\n",
    "                options={\"ents\": [\"EVENT\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Using linguistic annotations**\n",
    "\n",
    "- analysing user comments and you want to find out what people are saying about Facebook. \n",
    "- finding adjectives following “Facebook is” or “Facebook was”.\n",
    "\n",
    "```python\n",
    "[{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"ADJ\"}]\n",
    "```\n",
    "- quick overview of the results, collect all sentences containing a match and render them with the displaCy visualizer. \n",
    "- In the callback function, you’ll have access to the start and end of each match, as well as the parent Doc. \n",
    "- determine doc[start : end.sent], and calculate the start and end of the matched span within the sentence.\n",
    "- Using displaCy in “manual” mode lets you pass in a list of dictionaries containing the text and entities to render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"I'd say that Facebook is evil.\",\n",
       "  'ents': [{'start': 13, 'end': 29, 'label': 'MATCH'}]},\n",
       " {'text': 'Facebook is pretty cool, right?',\n",
       "  'ents': [{'start': 0, 'end': 23, 'label': 'MATCH'}]}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]  # Matched span\n",
    "    sent = span.sent  # Sentence containing matched span\n",
    "    # Append mock entity for match in displaCy style to matched_sents\n",
    "    # get the match span by ofsetting the start and end of the span with the\n",
    "    # start and end of the sentence in the doc\n",
    "    match_ents = [{\n",
    "        \"start\": span.start_char - sent.start_char,\n",
    "        \"end\": span.end_char - sent.start_char,\n",
    "        \"label\": \"MATCH\",\n",
    "    }]\n",
    "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
    "           {\"POS\": \"ADJ\"}]\n",
    "matcher.add(\"FacebookIs\", collect_sents, pattern)  # add pattern\n",
    "doc = nlp(u\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "matched_sents\n",
    "\n",
    "# error to be fixed\n",
    "# Serve visualization of sentences containing match with displaCy\n",
    "# set manual=True to make displaCy render straight from a dictionary\n",
    "# (if you're not running the code within a Jupyer environment, you can\n",
    "# use displacy.serve instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Phrase Matching\n",
    "\n",
    "- match large terminology lists to use `PhraseMatcher` and create `Doc` objects instead of token patterns\n",
    "- The Doc patterns can contain single or multiple tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terminology_list = [u\"Barack Obama\", u\"Angela Merkel\", u\"Washington, D.C.\"]\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terminology_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Barack Obama, Angela Merkel, Washington, D.C.]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angela Merkel\n",
      "Barack Obama\n",
      "Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "matcher.add(\"TerminologyList\", None, *patterns)\n",
    "\n",
    "doc = nlp(u\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "          u\"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed on creating pattern**\n",
    "\n",
    "```bash\n",
    "- patterns = [nlp(term) for term in LOTS_OF_TERMS]\n",
    "+ patterns = [nlp.make_doc(term) for term in LOTS_OF_TERMS]\n",
    "+ patterns = list(nlp.tokenizer.pipe(LOTS_OF_TERMS))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matching on other token attributes**\n",
    "\n",
    "- By default, the PhraseMatcher will match on the `verbatim token text, e.g. Token.text.`\n",
    "- By setting the attr argument on initialization, you can change which token attribute the matcher should use when comparing the phrase pattern to the matched Doc. \n",
    "- For example, using the attribute `LOWER` lets you match on Token.lower and create case-insensitive match patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(name) for name in [u\"Angela Merkel\", u\"Barack Obama\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Angela Merkel, Barack Obama]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched based on lowercase token text: angela merkel\n",
      "Matched based on lowercase token text: barack Obama\n"
     ]
    }
   ],
   "source": [
    "matcher.add(\"Names\", None, *patterns)\n",
    "\n",
    "doc = nlp(u\"angela merkel and us president barack Obama\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on lowercase token text:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matching on SHAPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched based on token shape: 192.168.1.1\n",
      "Matched based on token shape: 192.168.2.1\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
    "matcher.add(\"IP\", None, nlp(u\"127.0.0.1\"), nlp(u\"127.127.0.0\"))\n",
    "\n",
    "doc = nlp(u\"Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on token shape:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In theory, the same also works for attributes like POS. For example, a pattern nlp(\"I like cats\") matched based on its part-of-speech tag would return a match for “I love dogs”. You could also match on boolean flags like IS_PUNCT to match phrases with the same sequence of punctuation and non-punctuation tokens as the pattern. But this can easily get confusing and doesn’t have much of an advantage over writing one or two token patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based ER\n",
    "\n",
    "- The `EntityRuler` is an exciting new **component** that lets you **add named entities based on pattern dictionaries, and makes it easy to combine rule-based and statistical named entity recognition for even more powerful models.**\n",
    "\n",
    "**Entity Pattern**\n",
    "\n",
    "1. **Phrase patterns** for exact string matches (string).\n",
    "```python \n",
    "{\"label\": \"ORG\", \"pattern\": \"Apple\"}\n",
    "```\n",
    "2. **Token patterns** with one dictionary describing one token (list).\n",
    "```python\n",
    "{\"label\": \"GPE\", \"pattern\": [{\"lower\": \"san\"}, {\"lower\": \"francisco\"}]}\n",
    "```\n",
    "\n",
    "**Using the entity ruler**\n",
    "- The `EntityRuler` is a **pipeline component** that’s typically added via `nlp.add_pipe`. \n",
    "- When the nlp object is called on a text, it will **find matches in the doc and add them as entities to the `doc.ents`, using the specified pattern label as the entity label.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG'), ('San Francisco', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = English()\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"lower\": \"san\"}, {\"lower\": \"francisco\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(u\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The entity ruler is designed to **integrate with spaCy’s existing statistical models and enhance the named entity recognizer.**\n",
    "> If it’s **added before the `\"ner\"` component,** the entity recognizer will **respect the existing entity spans** and **adjust its predictions around it.**\n",
    "> This can significantly improve accuracy in some cases. **If it’s added after the \"ner\" component, the entity ruler will only add spans to the `doc.ents` if they don’t overlap with existing entities predicted by the model.**\n",
    "> To **overwrite overlapping entities**, you can set `overwrite_ents=True` on initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MyCorp Inc.', 'ORG'), ('U.S.', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(u\"MyCorp Inc. is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pattern FILE**\n",
    "\n",
    "The `to_disk` and `from_disk` let you save and load patterns to and from JSONL (newline-delimited JSON) files, containing one pattern object per line.\n",
    "\n",
    "```python\n",
    "# pattern.jsonl\n",
    "{\"label\": \"ORG\", \"pattern\": \"Apple\"}\n",
    "{\"label\": \"GPE\", \"pattern\": [{\"lower\": \"san\"}, {\"lower\": \"francisco\"}]}\n",
    "\n",
    "ruler.to_disk(\"./patterns.jsonl\")\n",
    "new_ruler = EntityRuler(nlp).from_disk(\"./patterns.jsonl\")\n",
    "```\n",
    "\n",
    "> When you save out an `nlp` object that has an `EntityRuler` added to its pipeline, its patterns are automatically exported to the model directory:\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = EntityRuler(nlp)\n",
    "ruler.add_patterns([{\"label\": \"ORG\", \"pattern\": \"Apple\"}])\n",
    "nlp.add_pipe(ruler)\n",
    "nlp.to_disk(\"/path/to/model\")\n",
    "```\n",
    "\n",
    "> The saved model now **includes the \"entity_ruler\" in its \"pipeline\" setting in the meta.json,** and the model directory contains a file `entityruler.jsonl` with the patterns. When you load the model back in, all pipeline components will be restored and deserialized – including the entity ruler. **This lets you ship powerful model packages with binary weights and rules included!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Model and Rules\n",
    "\n",
    "You can combine statistical and rule-based components in a variety of ways. Rule-based components can be used to improve the accuracy of statistical models, by presetting tags, entities or sentence boundaries for specific tokens. The statistical models will usually respect these preset annotations, which sometimes improves the accuracy of other decisions. You can also use rule-based components after a statistical model to correct common errors. Finally, rule-based components can reference the attributes set by statistical models, in order to implement more abstract logic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleIO Google I/O\n",
      "HAPPY 😀\n",
      "HAPPY 😀😀\n",
      "HAPPY 😀\n",
      "Sentiment 0.30000001192092896\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def set_sentiment(matcher, doc, i, matches):\n",
    "    doc.sentiment += 0.1\n",
    "\n",
    "pattern1 = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
    "pattern2 = [[{\"ORTH\": emoji, \"OP\": \"+\"}] for emoji in [\"😀\", \"😂\", \"🤣\", \"😍\"]]\n",
    "matcher.add(\"GoogleIO\", None, pattern1)  # Match \"Google I/O\" or \"Google i/o\"\n",
    "matcher.add(\"HAPPY\", set_sentiment, *pattern2)  # Match one or more happy emoji\n",
    "\n",
    "doc = nlp(u\"A text about Google I/O 😀😀\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(string_id, span.text)\n",
    "print(\"Sentiment\", doc.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Expanding Named Entity**\n",
    "\n",
    "- For example, the corpus spaCy’s English models were trained on defines a `PERSON` entity as just the **person name, without titles like “Mr” or “Dr”.**\n",
    "- This makes sense, because it makes it easier to resolve the entity type back to a knowledge base. \n",
    "- But what if your application needs the full names, including the titles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alex Smith', 'PERSON'), ('first', 'ORDINAL'), ('Acme Corp Inc.', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While you could try and teach the model a new definition of the PERSON entity by **updating it with more examples of spans that include the title, this might not be the most efficient approach.**\n",
    "- The existing model was trained on over **2 million words**, so in order to completely change the definition of an entity type, you might need a lot of training examples. \n",
    "- However, if you already have the predicted PERSON entities, you can use a **rule-based approach** that checks whether they come with a title and if so, expands the entity span by one token. \n",
    "- After all, what all titles in this example have in common is that if they occur, they occur in the previous token right before the person entity.\n",
    "\n",
    "> modify `Doc` and its `doc.ents` and returns it. This is **exactly what a pipeline component does**, so in order to let it run automatically when processing a text with the nlp object, we can use `nlp.add_pipe` to add it to the current pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dr Alex Smith', 'PERSON'), ('first', 'ORDINAL'), ('Acme Corp Inc.', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def expand_person_entities(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and ent.start != 0:\n",
    "            prev_token = doc[ent.start - 1]\n",
    "            if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
    "                new_ents.append(new_ent)\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "# Add the component after the named entity recognizer\n",
    "nlp.add_pipe(expand_person_entities, after='ner')\n",
    "\n",
    "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An alternative approach would be to an **extension attribute** like `._.person_title` and add it to `Span` objects (which includes entity spans in `doc.ents`). The **advantage here is that the entity text stays intact and can still be used to look up the name in a knowledge base.** The following function takes a Span object, checks the previous token if it’s a `PERSON` entity and returns the title if one is found. The Span.doc attribute gives us easy access to the span’s parent document.\n",
    "```python\n",
    "def get_person_title(span):\n",
    "    if span.label_ == \"PERSON\" and span.start != 0:\n",
    "        prev_token = span.doc[span.start - 1]\n",
    "        if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "            return prev_token.text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alex Smith', 'PERSON', 'Dr'), ('first', 'ORDINAL', None), ('Acme Corp Inc.', 'ORG', None)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_person_title(span):\n",
    "    if span.label_ == \"PERSON\" and span.start != 0:\n",
    "        prev_token = span.doc[span.start - 1]\n",
    "        if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "            return prev_token.text\n",
    "\n",
    "# Register the Span extension as 'person_title'\n",
    "Span.set_extension(\"person_title\", getter=get_person_title)\n",
    "\n",
    "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_, ent._.person_title) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Using entities, part-of-speech tags and the dependency parse**\n",
    "\n",
    "- Let’s say you want to parse **professional biographies and extract the person names and company names, and whether it’s a company they’re currently working at, or a previous company.**\n",
    "- One approach could be to try and train a named entity recognizer to predict CURRENT_ORG and PREVIOUS_ORG – but this distinction is very subtle and something the entity recognizer may struggle to learn. Nothing about “Acme Corp Inc.” is inherently “current” or “previous”.\n",
    "- However, the **syntax** of the sentence holds some very important clues: we can check for **trigger words like “work”, whether they’re past tense or present tense, whether company names are attached to it and whether the person is the subject.**\n",
    "- All of this information is available in the part-of-speech tags and the dependency parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alex Smith', 'PERSON'), ('Acme Corp Inc.', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a909aad5c4c7445ba5ab085012f6bcee-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Alex</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Smith</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">worked</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VBD</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">IN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Acme</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Corp</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Inc.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a909aad5c4c7445ba5ab085012f6bcee-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a909aad5c4c7445ba5ab085012f6bcee-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a909aad5c4c7445ba5ab085012f6bcee-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a909aad5c4c7445ba5ab085012f6bcee-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a909aad5c4c7445ba5ab085012f6bcee-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a909aad5c4c7445ba5ab085012f6bcee-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a909aad5c4c7445ba5ab085012f6bcee-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a909aad5c4c7445ba5ab085012f6bcee-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">subtok</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,179.0 L762,167.0 778,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a909aad5c4c7445ba5ab085012f6bcee-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a909aad5c4c7445ba5ab085012f6bcee-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">subtok</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a909aad5c4c7445ba5ab085012f6bcee-0-5\" stroke-width=\"2px\" d=\"M595,177.0 C595,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a909aad5c4c7445ba5ab085012f6bcee-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep', options={'fine_grained': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this example, “worked” is the root of the sentence and is a past tense verb. \n",
    "    - Its subject is “Alex Smith”, the person who worked. “at Acme Corp Inc.” is a prepositional phrase attached to the verb “worked”. \n",
    "    - To extract this relationship, we can start by looking at the predicted PERSON entities, find their heads and check whether they’re attached to a trigger word like “work”. Next, we can check for prepositional phrases attached to the head and whether they contain an ORG entity. \n",
    "    - Finally, to determine whether the company affiliation is current, we can check the head’s part-of-speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': Alex Smith, 'orgs': [Acme Corp Inc.], 'past': True}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"41bd36957b4a44e0a3f45d662df3dd5c-0\" class=\"displacy\" width=\"750\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Alex Smith</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">worked</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VBD</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">IN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Acme Corp Inc.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-41bd36957b4a44e0a3f45d662df3dd5c-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-41bd36957b4a44e0a3f45d662df3dd5c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-41bd36957b4a44e0a3f45d662df3dd5c-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-41bd36957b4a44e0a3f45d662df3dd5c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-41bd36957b4a44e0a3f45d662df3dd5c-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-41bd36957b4a44e0a3f45d662df3dd5c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import merge_entities\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_person_orgs(doc):\n",
    "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    for ent in person_entities:\n",
    "        head = ent.root.head\n",
    "        if head.lemma_ == \"work\":\n",
    "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "            for prep in preps:\n",
    "                orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n",
    "                print({'person': ent, 'orgs': orgs, 'past': head.tag_ == \"VBD\"})\n",
    "    return doc\n",
    "\n",
    "# To make the entities easier to work with, we'll merge them into single tokens\n",
    "nlp.add_pipe(merge_entities)\n",
    "nlp.add_pipe(extract_person_orgs)\n",
    "\n",
    "doc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n",
    "# If you're not in a Jupyter / IPython environment, use displacy.serve\n",
    "displacy.render(doc, options={'fine_grained': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you change the sentence structure above, for example to “was working”, you’ll notice that our current logic fails and doesn’t correctly detect the company as a past organization. That’s because the root is a participle and the tense information is in the attached auxiliary “was”:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To solve this, we can adjust the rules to also check for the above construction:\n",
    "```python\n",
    "def extract_person_orgs(doc):\n",
    "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    for ent in person_entities:\n",
    "        head = ent.root.head\n",
    "        if head.lemma_ == \"work\":\n",
    "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "            for prep in preps:\n",
    "                orgs = [t for t in prep.children if t.ent_type_ == \"ORG\"]\n",
    "                aux = [token for token in head.children if token.dep_ == \"aux\"]\n",
    "                past_aux = any(t.tag_ == \"VBD\" for t in aux)\n",
    "                past = head.tag_ == \"VBD\" or head.tag_ == \"VBG\" and past_aux\n",
    "                print({'person': ent, 'orgs': orgs, 'past': past})\n",
    "    return doc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Alex Smith, worked, at, Acme Corp Inc.]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok for tok in doc if tok.has_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CUSTOM SIMILARITY HOOKS\n",
    "\n",
    "class SimilarityModel(object):\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        doc.user_hooks[\"similarity\"] = self.similarity\n",
    "        doc.user_span_hooks[\"similarity\"] = self.similarity\n",
    "        doc.user_token_hooks[\"similarity\"] = self.similarity\n",
    "\n",
    "    def similarity(self, obj1, obj2):\n",
    "        y = self._model([obj1.vector, obj2.vector])\n",
    "        return float(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.user_hooks['vector'] = np.array(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-da973cf9aec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.vector.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "print(doc.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PIPE Streamer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "customer_feedback = open(\"customer_feedback_627.txt\").read()\n",
    "doc = nlp(customer_feedback)\n",
    "doc.to_disk(\"/tmp/customer_feedback_627.bin\")\n",
    "\n",
    "new_doc = Doc(Vocab()).from_disk(\"/tmp/customer_feedback_627.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiTzXPWDFWrk"
   },
   "source": [
    "# Extra EcoSystem Lib\n",
    "\n",
    "### ADAM - Wikipedia Q&A\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/5hirish/adam_qas.git\n",
    "cd adam_qas\n",
    "pip install -r requirements.txt\n",
    "python -m qas.adam 'When was linux kernel version 4.0 released ?'\n",
    "```\n",
    "\n",
    "### AllenNLP\n",
    "\n",
    "- use to develop pipeline components ådding annotations to `Doc`\n",
    "\n",
    "### ExcelCy - Excel Integration with spaCy. Training NER using XLSX from PDF, DOCX, PPT, PNG or JPG.\n",
    "\n",
    "```bash\n",
    "from excelcy import ExcelCy\n",
    "# collect sentences, annotate Entities and train NER using spaCy\n",
    "excelcy = ExcelCy.execute(file_path='https://github.com/kororo/excelcy/raw/master/tests/data/test_data_01.xlsx')\n",
    "# use the nlp object as per spaCy API\n",
    "doc = excelcy.nlp('Google rebrands its business apps')\n",
    "# or save it for faster bootstrap for application\n",
    "excelcy.nlp.to_disk('/model')\n",
    "```\n",
    "\n",
    "### explacy - visualise spaCy parse\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "import explacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "explacy.print_parse_info(nlp, 'The salad was surprisingly tasty.')\n",
    "```\n",
    "\n",
    "### spacy_hunspell - spell checker\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "from spacy_hunspell import spaCyHunSpell\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "hunspell = spaCyHunSpell(nlp, 'mac')\n",
    "nlp.add_pipe(hunspell)\n",
    "doc = nlp('I can haz cheezeburger.')\n",
    "haz = doc[2]\n",
    "haz._.hunspell_spell  # False\n",
    "haz._.hunspell_suggest  # ['ha', 'haze', 'hazy', 'has', 'hat', 'had', 'hag', 'ham', 'hap', 'hay', 'haw', 'ha z']\n",
    "```\n",
    "\n",
    "### spacy-lookup \n",
    "- powerful NER matcher for large dictionaries using FlashText module\n",
    "-  The extension sets the custom `Doc, Token and Span` attributes `._.is_entity, ._.entity_type, ._.has_entities and ._.entities.` Named Entities are matched using the python module flashtext, and looked up in the data provided by different dictionaries.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "from spacy_lookup import Entity\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "entity = Entity(keywords_list=['python', 'java platform'])\n",
    "nlp.add_pipe(entity, last=True)\n",
    "\n",
    "doc = nlp(u\"I am a product manager for a java and python.\")\n",
    "assert doc._.has_entities == True\n",
    "assert doc[2:5]._.has_entities == True\n",
    "assert doc[0]._.is_entity == False\n",
    "assert doc[3]._.is_entity == True\n",
    "print(doc._.entities)\n",
    "```\n",
    "\n",
    "### spacy-vis using Hierplane\n",
    "\n",
    "- local installation https://github.com/DeNeutoy/spacy-vis\n",
    "\n",
    "```bash\n",
    "docker run -p 8080:8080 -it markn/spacy-vis bash bin/serve\n",
    "```\n",
    "\n",
    "### textpipe - clean and extracat metadata\n",
    "\n",
    "```python\n",
    "from textpipe import doc, pipeline\n",
    "sample_text = 'Sample text! <!DOCTYPE>'\n",
    "document = doc.Doc(sample_text)\n",
    "print(document.clean)\n",
    "'Sample text!'\n",
    "print(document.language)\n",
    "# 'en'\n",
    "print(document.nwords)\n",
    "# 2\n",
    "\n",
    "pipe = pipeline.Pipeline(['CleanText', 'NWords'])\n",
    "print(pipe(sample_text))\n",
    "# {'CleanText': 'Sample text!', 'NWords': 2}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PYey5mrSFWrm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbCuMK-Pdm6F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NrGa8A7FdnO8"
   },
   "source": [
    "# KEY SOURCE CODE - spacy_pipe.pyx\n",
    "\n",
    "```python\n",
    "# cython: infer_types=True\n",
    "# cython: profile=True\n",
    "# coding: utf8\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "cimport numpy as np\n",
    "\n",
    "import numpy\n",
    "import srsly\n",
    "from collections import OrderedDict\n",
    "from thinc.api import chain\n",
    "from thinc.v2v import Affine, Maxout, Softmax\n",
    "from thinc.misc import LayerNorm\n",
    "from thinc.neural.util import to_categorical, copy_array\n",
    "\n",
    "from ..tokens.doc cimport Doc\n",
    "from ..syntax.nn_parser cimport Parser\n",
    "from ..syntax.ner cimport BiluoPushDown\n",
    "from ..syntax.arc_eager cimport ArcEager\n",
    "from ..morphology cimport Morphology\n",
    "from ..vocab cimport Vocab\n",
    "\n",
    "from ..syntax import nonproj\n",
    "from ..attrs import POS, ID\n",
    "from ..parts_of_speech import X\n",
    "from .._ml import Tok2Vec, build_tagger_model\n",
    "from .._ml import build_text_classifier, build_simple_cnn_text_classifier\n",
    "from .._ml import build_bow_text_classifier\n",
    "from .._ml import link_vectors_to_models, zero_init, flatten\n",
    "from .._ml import masked_language_model, create_default_optimizer\n",
    "from ..errors import Errors, TempErrors\n",
    "from .. import util\n",
    "\n",
    "\n",
    "def _load_cfg(path):\n",
    "    if path.exists():\n",
    "        return srsly.read_json(path)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "class Pipe(object):\n",
    "    \"\"\"This class is not instantiated directly. Components inherit from it, and\n",
    "    it defines the interface that components should follow to function as\n",
    "    components in a spaCy analysis pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    name = None\n",
    "\n",
    "    @classmethod\n",
    "    def Model(cls, *shape, **kwargs):\n",
    "        \"\"\"Initialize a model for the pipe.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __init__(self, vocab, model=True, **cfg):\n",
    "        \"\"\"Create a new pipe instance.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipe to one document. The document is\n",
    "        modified in-place, and returned.\n",
    "\n",
    "        Both __call__ and pipe should delegate to the `predict()`\n",
    "        and `set_annotations()` methods.\n",
    "        \"\"\"\n",
    "        self.require_model()\n",
    "        scores, tensors = self.predict([doc])\n",
    "        self.set_annotations([doc], scores, tensors=tensors)\n",
    "        return doc\n",
    "\n",
    "    def require_model(self):\n",
    "        \"\"\"Raise an error if the component's model is not initialized.\"\"\"\n",
    "        if getattr(self, \"model\", None) in (None, True, False):\n",
    "            raise ValueError(Errors.E109.format(name=self.name))\n",
    "\n",
    "    def pipe(self, stream, batch_size=128, n_threads=-1):\n",
    "        \"\"\"Apply the pipe to a stream of documents.\n",
    "\n",
    "        Both __call__ and pipe should delegate to the `predict()`\n",
    "        and `set_annotations()` methods.\n",
    "        \"\"\"\n",
    "        for docs in util.minibatch(stream, size=batch_size):\n",
    "            docs = list(docs)\n",
    "            scores, tensors = self.predict(docs)\n",
    "            self.set_annotations(docs, scores, tensor=tensors)\n",
    "            yield from docs\n",
    "\n",
    "    def predict(self, docs):\n",
    "        \"\"\"Apply the pipeline's model to a batch of docs, without\n",
    "        modifying them.\n",
    "        \"\"\"\n",
    "        self.require_model()\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def set_annotations(self, docs, scores, tensors=None):\n",
    "        \"\"\"Modify a batch of documents, using pre-computed scores.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, docs, golds, drop=0.0, sgd=None, losses=None):\n",
    "        \"\"\"Learn from a batch of documents and gold-standard information,\n",
    "        updating the pipe's model.\n",
    "\n",
    "        Delegates to predict() and get_loss().\n",
    "        \"\"\"\n",
    "        self.require_model()\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def rehearse(self, docs, sgd=None, losses=None, **config):\n",
    "        pass\n",
    "\n",
    "    def get_loss(self, docs, golds, scores):\n",
    "        \"\"\"Find the loss and gradient of loss for the batch of\n",
    "        documents and their predicted scores.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def add_label(self, label):\n",
    "        \"\"\"Add an output label, to be predicted by the model.\n",
    "\n",
    "        It's possible to extend pre-trained models with new labels,\n",
    "        but care should be taken to avoid the \"catastrophic forgetting\"\n",
    "        problem.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        return create_default_optimizer(self.model.ops, **self.cfg.get(\"optimizer\", {}))\n",
    "\n",
    "    def begin_training(\n",
    "        self, get_gold_tuples=lambda: [], pipeline=None, sgd=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize the pipe for training, using data exampes if available.\n",
    "        If no model has been initialized yet, the model is added.\"\"\"\n",
    "        if self.model is True:\n",
    "            self.model = self.Model(**self.cfg)\n",
    "        link_vectors_to_models(self.vocab)\n",
    "        if sgd is None:\n",
    "            sgd = self.create_optimizer()\n",
    "        return sgd\n",
    "\n",
    "    def use_params(self, params):\n",
    "        \"\"\"Modify the pipe's model, to use the given parameter values.\"\"\"\n",
    "        with self.model.use_params(params):\n",
    "            yield\n",
    "\n",
    "    def to_bytes(self, exclude=tuple(), **kwargs):\n",
    "        \"\"\"Serialize the pipe to a bytestring.\n",
    "\n",
    "        exclude (list): String names of serialization fields to exclude.\n",
    "        RETURNS (bytes): The serialized object.\n",
    "        \"\"\"\n",
    "        serialize = OrderedDict()\n",
    "        serialize[\"cfg\"] = lambda: srsly.json_dumps(self.cfg)\n",
    "        if self.model not in (True, False, None):\n",
    "            serialize[\"model\"] = self.model.to_bytes\n",
    "        serialize[\"vocab\"] = self.vocab.to_bytes\n",
    "        exclude = util.get_serialization_exclude(serialize, exclude, kwargs)\n",
    "        return util.to_bytes(serialize, exclude)\n",
    "\n",
    "    def from_bytes(self, bytes_data, exclude=tuple(), **kwargs):\n",
    "        \"\"\"Load the pipe from a bytestring.\"\"\"\n",
    "\n",
    "        def load_model(b):\n",
    "            # TODO: Remove this once we don't have to handle previous models\n",
    "            if self.cfg.get(\"pretrained_dims\") and \"pretrained_vectors\" not in self.cfg:\n",
    "                self.cfg[\"pretrained_vectors\"] = self.vocab.vectors.name\n",
    "            if self.model is True:\n",
    "                self.model = self.Model(**self.cfg)\n",
    "            self.model.from_bytes(b)\n",
    "\n",
    "        deserialize = OrderedDict()\n",
    "        deserialize[\"cfg\"] = lambda b: self.cfg.update(srsly.json_loads(b))\n",
    "        deserialize[\"vocab\"] = lambda b: self.vocab.from_bytes(b)\n",
    "        deserialize[\"model\"] = load_model\n",
    "        exclude = util.get_serialization_exclude(deserialize, exclude, kwargs)\n",
    "        util.from_bytes(bytes_data, deserialize, exclude)\n",
    "        return self\n",
    "\n",
    "    def to_disk(self, path, exclude=tuple(), **kwargs):\n",
    "        \"\"\"Serialize the pipe to disk.\"\"\"\n",
    "        serialize = OrderedDict()\n",
    "        serialize[\"cfg\"] = lambda p: srsly.write_json(p, self.cfg)\n",
    "        serialize[\"vocab\"] = lambda p: self.vocab.to_disk(p)\n",
    "        if self.model not in (None, True, False):\n",
    "            serialize[\"model\"] = lambda p: p.open(\"wb\").write(self.model.to_bytes())\n",
    "        exclude = util.get_serialization_exclude(serialize, exclude, kwargs)\n",
    "        util.to_disk(path, serialize, exclude)\n",
    "\n",
    "    def from_disk(self, path, exclude=tuple(), **kwargs):\n",
    "        \"\"\"Load the pipe from disk.\"\"\"\n",
    "\n",
    "        def load_model(p):\n",
    "            # TODO: Remove this once we don't have to handle previous models\n",
    "            if self.cfg.get(\"pretrained_dims\") and \"pretrained_vectors\" not in self.cfg:\n",
    "                self.cfg[\"pretrained_vectors\"] = self.vocab.vectors.name\n",
    "            if self.model is True:\n",
    "                self.model = self.Model(**self.cfg)\n",
    "            self.model.from_bytes(p.open(\"rb\").read())\n",
    "\n",
    "        deserialize = OrderedDict()\n",
    "        deserialize[\"cfg\"] = lambda p: self.cfg.update(_load_cfg(p))\n",
    "        deserialize[\"vocab\"] = lambda p: self.vocab.from_disk(p)\n",
    "        deserialize[\"model\"] = load_model\n",
    "        exclude = util.get_serialization_exclude(deserialize, exclude, kwargs)\n",
    "        util.from_disk(path, deserialize, exclude)\n",
    "        return self\n",
    "\n",
    "\n",
    "class Tensorizer(Pipe):\n",
    "    \"\"\"Pre-train position-sensitive vectors for tokens.\"\"\"\n",
    "\n",
    "    name = \"tensorizer\"\n",
    "\n",
    "    @classmethod\n",
    "    def Model(cls, output_size=300, **cfg):\n",
    "        \"\"\"Create a new statistical model for the class.\n",
    "\n",
    "        width (int): Output size of the model.\n",
    "        embed_size (int): Number of vectors in the embedding table.\n",
    "        **cfg: Config parameters.\n",
    "        RETURNS (Model): A `thinc.neural.Model` or similar instance.\n",
    "        \"\"\"\n",
    "        input_size = util.env_opt(\"token_vector_width\", cfg.get(\"input_size\", 96))\n",
    "        return zero_init(Affine(output_size, input_size, drop_factor=0.0))\n",
    "\n",
    "    def __init__(self, vocab, model=True, **cfg):\n",
    "        \"\"\"Construct a new statistical model. Weights are not allocated on\n",
    "        initialisation.\n",
    "\n",
    "        vocab (Vocab): A `Vocab` instance. The model must share the same\n",
    "            `Vocab` instance with the `Doc` objects it will process.\n",
    "        model (Model): A `Model` instance or `True` allocate one later.\n",
    "        **cfg: Config parameters.\n",
    "\n",
    "        EXAMPLE:\n",
    "            >>> from spacy.pipeline import TokenVectorEncoder\n",
    "            >>> tok2vec = TokenVectorEncoder(nlp.vocab)\n",
    "            >>> tok2vec.model = tok2vec.Model(128, 5000)\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        self.input_models = []\n",
    "        self.cfg = dict(cfg)\n",
    "        self.cfg.setdefault(\"cnn_maxout_pieces\", 3)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Add context-sensitive vectors to a `Doc`, e.g. from a CNN or LSTM\n",
    "        model. Vectors are set to the `Doc.tensor` attribute.\n",
    "\n",
    "        docs (Doc or iterable): One or more documents to add vectors to.\n",
    "        RETURNS (dict or None): Intermediate computations.\n",
    "        \"\"\"\n",
    "        tokvecses = self.predict([doc])\n",
    "        self.set_annotations([doc], tokvecses)\n",
    "        return doc\n",
    "\n",
    "    def pipe(self, stream, batch_size=128, n_threads=-1):\n",
    "        \"\"\"Process `Doc` objects as a stream.\n",
    "\n",
    "        stream (iterator): A sequence of `Doc` objects to process.\n",
    "        batch_size (int): Number of `Doc` objects to group.\n",
    "        YIELDS (iterator): A sequence of `Doc` objects, in order of input.\n",
    "        \"\"\"\n",
    "        for docs in util.minibatch(stream, size=batch_size):\n",
    "            docs = list(docs)\n",
    "            tensors = self.predict(docs)\n",
    "            self.set_annotations(docs, tensors)\n",
    "            yield from docs\n",
    "\n",
    "    def predict(self, docs):\n",
    "        \"\"\"Return a single tensor for a batch of documents.\n",
    "\n",
    "        docs (iterable): A sequence of `Doc` objects.\n",
    "        RETURNS (object): Vector representations for each token in the docs.\n",
    "        \"\"\"\n",
    "        self.require_model()\n",
    "        inputs = self.model.ops.flatten([doc.tensor for doc in docs])\n",
    "        outputs = self.model(inputs)\n",
    "        return self.model.ops.unflatten(outputs, [len(d) for d in docs])\n",
    "\n",
    "    def set_annotations(self, docs, tensors):\n",
    "        \"\"\"Set the tensor attribute for a batch of documents.\n",
    "\n",
    "        docs (iterable): A sequence of `Doc` objects.\n",
    "        tensors (object): Vector representation for each token in the docs.\n",
    "        \"\"\"\n",
    "        for doc, tensor in zip(docs, tensors):\n",
    "            if tensor.shape[0] != len(doc):\n",
    "                raise ValueError(Errors.E076.format(rows=tensor.shape[0], words=len(doc)))\n",
    "            doc.tensor = tensor\n",
    "\n",
    "    def update(self, docs, golds, state=None, drop=0.0, sgd=None, losses=None):\n",
    "        \"\"\"Update the model.\n",
    "\n",
    "        docs (iterable): A batch of `Doc` objects.\n",
    "        golds (iterable): A batch of `GoldParse` objects.\n",
    "        drop (float): The droput rate.\n",
    "        sgd (callable): An optimizer.\n",
    "        RETURNS (dict): Results from the update.\n",
    "        \"\"\"\n",
    "        self.require_model()\n",
    "        if isinstance(docs, Doc):\n",
    "            docs = [docs]\n",
    "        inputs = []\n",
    "        bp_inputs = []\n",
    "        for tok2vec in self.input_models:\n",
    "            tensor, bp_tensor = tok2vec.begin_update(docs, drop=drop)\n",
    "            inputs.append(tensor)\n",
    "            bp_inputs.append(bp_tensor)\n",
    "        inputs = self.model.ops.xp.hstack(inputs)\n",
    "        scores, bp_scores = self.model.begin_update(inputs, drop=drop)\n",
    "        loss, d_scores = self.get_loss(docs, golds, scores)\n",
    "        d_inputs = bp_scores(d_scores, sgd=sgd)\n",
    "        d_inputs = self.model.ops.xp.split(d_inputs, len(self.input_models), axis=1)\n",
    "        for d_input, bp_input in zip(d_inputs, bp_inputs):\n",
    "            bp_input(d_input, sgd=sgd)\n",
    "        if losses is not None:\n",
    "            losses.setdefault(self.name, 0.0)\n",
    "            losses[self.name] += loss\n",
    "        return loss\n",
    "\n",
    "    def get_loss(self, docs, golds, prediction):\n",
    "        ids = self.model.ops.flatten([doc.to_array(ID).ravel() for doc in docs])\n",
    "        target = self.vocab.vectors.data[ids]\n",
    "        d_scores = (prediction - target) / prediction.shape[0]\n",
    "        loss = (d_scores ** 2).sum()\n",
    "        return loss, d_scores\n",
    "\n",
    "    def begin_training(self, gold_tuples=lambda: [], pipeline=None, sgd=None, **kwargs):\n",
    "        \"\"\"Allocate models, pre-process training data and acquire an\n",
    "        optimizer.\n",
    "\n",
    "        gold_tuples (iterable): Gold-standard training data.\n",
    "        pipeline (list): The pipeline the model is part of.\n",
    "        \"\"\"\n",
    "        if pipeline is not None:\n",
    "            for name, model in pipeline:\n",
    "                if getattr(model, \"tok2vec\", None):\n",
    "                    self.input_models.append(model.tok2vec)\n",
    "        if self.model is True:\n",
    "            self.model = self.Model(**self.cfg)\n",
    "        link_vectors_to_models(self.vocab)\n",
    "        if sgd is None:\n",
    "            sgd = self.create_optimizer()\n",
    "        return sgd\n",
    "\n",
    "\n",
    "class Tagger(Pipe):\n",
    "    \"\"\"Pipeline component for part-of-speech tagging.\n",
    "\n",
    "    DOCS: https://spacy.io/api/tagger\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"tagger\"\n",
    "\n",
    "    def __init__(self, vocab, model=True, **cfg):\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        self._rehearsal_model = None\n",
    "        self.cfg = OrderedDict(sorted(cfg.items()))\n",
    "        self.cfg.setdefault(\"cnn_maxout_pieces\", 2)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return tuple(self.vocab.morphology.tag_names)\n",
    "\n",
    "    @property\n",
    "    def tok2vec(self):\n",
    "        if self.model in (None, True, False):\n",
    "            return None\n",
    "        else:\n",
    "            return chain(self.model.tok2vec, flatten)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        tags, tokvecs = self.predict([doc])\n",
    "        self.set_annotations([doc], tags, tensors=tokvecs)\n",
    "        return doc\n",
    "\n",
    "    def pipe(self, stream, batch_size=128, n_threads=-1):\n",
    "        for docs in util.minibatch(stream, size=batch_size):\n",
    "            docs = list(docs)\n",
    "            tag_ids, tokvecs = self.predict(docs)\n",
    "            self.set_annotations(docs, tag_ids, tensors=tokvecs)\n",
    "            yield from docs\n",
    "\n",
    "    def predict(self, docs):\n",
    "        self.require_model()\n",
    "        if not any(len(doc) for doc in docs):\n",
    "            # Handle case where there are no tokens in any docs.\n",
    "            n_labels = len(self.labels)\n",
    "            guesses = [self.model.ops.allocate((0, n_labels)) for doc in docs]\n",
    "            tokvecs = self.model.ops.allocate((0, self.model.tok2vec.nO))\n",
    "            return guesses, tokvecs\n",
    "        tokvecs = self.model.tok2vec(docs)\n",
    "        scores = self.model.softmax(tokvecs)\n",
    "        guesses = []\n",
    "        for doc_scores in scores:\n",
    "            doc_guesses = doc_scores.argmax(axis=1)\n",
    "            if not isinstance(doc_guesses, numpy.ndarray):\n",
    "                doc_guesses = doc_guesses.get()\n",
    "            guesses.append(doc_guesses)\n",
    "        return guesses, tokvecs\n",
    "\n",
    "    def set_annotations(self, docs, batch_tag_ids, tensors=None):\n",
    "        if isinstance(docs, Doc):\n",
    "            docs = [docs]\n",
    "        cdef Doc doc\n",
    "        cdef int idx = 0\n",
    "        cdef Vocab vocab = self.vocab\n",
    "        for i, doc in enumerate(docs):\n",
    "            doc_tag_ids = batch_tag_ids[i]\n",
    "            if hasattr(doc_tag_ids, \"get\"):\n",
    "                doc_tag_ids = doc_tag_ids.get()\n",
    "            for j, tag_id in enumerate(doc_tag_ids):\n",
    "                # Don't clobber preset POS tags\n",
    "                if doc.c[j].tag == 0 and doc.c[j].pos == 0:\n",
    "                    # Don't clobber preset lemmas\n",
    "                    lemma = doc.c[j].lemma\n",
    "                    vocab.morphology.assign_tag_id(&doc.c[j], tag_id)\n",
    "                    if lemma != 0 and lemma != doc.c[j].lex.orth:\n",
    "                        doc.c[j].lemma = lemma\n",
    "                idx += 1\n",
    "            if tensors is not None and len(tensors):\n",
    "                if isinstance(doc.tensor, numpy.ndarray) \\\n",
    "                and not isinstance(tensors[i], numpy.ndarray):\n",
    "                    doc.extend_tensor(tensors[i].get())\n",
    "                else:\n",
    "                    doc.extend_tensor(tensors[i])\n",
    "            doc.is_tagged = True\n",
    "\n",
    "    def update(self, docs, golds, drop=0., sgd=None, losses=None):\n",
    "        self.require_model()\n",
    "        if losses is not None and self.name not in losses:\n",
    "            losses[self.name] = 0.\n",
    "\n",
    "        tag_scores, bp_tag_scores = self.model.begin_update(docs, drop=drop)\n",
    "        loss, d_tag_scores = self.get_loss(docs, golds, tag_scores)\n",
    "        bp_tag_scores(d_tag_scores, sgd=sgd)\n",
    "\n",
    "        if losses is not None:\n",
    "            losses[self.name] += loss\n",
    "\n",
    "    def rehearse(self, docs, drop=0., sgd=None, losses=None):\n",
    "        \"\"\"Perform a 'rehearsal' update, where we try to match the output of\n",
    "        an initial model.\n",
    "        \"\"\"\n",
    "        if self._rehearsal_model is None:\n",
    "            return\n",
    "        guesses, backprop = self.model.begin_update(docs, drop=drop)\n",
    "        target = self._rehearsal_model(docs)\n",
    "        gradient = guesses - target\n",
    "        backprop(gradient, sgd=sgd)\n",
    "        if losses is not None:\n",
    "            losses.setdefault(self.name, 0.0)\n",
    "            losses[self.name] += (gradient**2).sum()\n",
    "\n",
    "    def get_loss(self, docs, golds, scores):\n",
    "        scores = self.model.ops.flatten(scores)\n",
    "        tag_index = {tag: i for i, tag in enumerate(self.labels)}\n",
    "        cdef int idx = 0\n",
    "        correct = numpy.zeros((scores.shape[0],), dtype=\"i\")\n",
    "        guesses = scores.argmax(axis=1)\n",
    "        known_labels = numpy.ones((scores.shape[0], 1), dtype=\"f\")\n",
    "        for gold in golds:\n",
    "            for tag in gold.tags:\n",
    "                if tag is None:\n",
    "                    correct[idx] = guesses[idx]\n",
    "                elif tag in tag_index:\n",
    "                    correct[idx] = tag_index[tag]\n",
    "                else:\n",
    "                    correct[idx] = 0\n",
    "                    known_labels[idx] = 0.\n",
    "                idx += 1\n",
    "        correct = self.model.ops.xp.array(correct, dtype=\"i\")\n",
    "        d_scores = scores - to_categorical(correct, nb_classes=scores.shape[1])\n",
    "        d_scores *= self.model.ops.asarray(known_labels)\n",
    "        loss = (d_scores**2).sum()\n",
    "        d_scores = self.model.ops.unflatten(d_scores, [len(d) for d in docs])\n",
    "        return float(loss), d_scores\n",
    "\n",
    "    def begin_training(self, get_gold_tuples=lambda: [], pipeline=None, sgd=None,\n",
    "                       **kwargs):\n",
    "        orig_tag_map = dict(self.vocab.morphology.tag_map)\n",
    "        new_tag_map = OrderedDict()\n",
    "        for raw_text, annots_brackets in get_gold_tuples():\n",
    "            for annots, brackets in annots_brackets:\n",
    "                ids, words, tags, heads, deps, ents = annots\n",
    "                for tag in tags:\n",
    "                    if tag in orig_tag_map:\n",
    "                        new_tag_map[tag] = orig_tag_map[tag]\n",
    "                    else:\n",
    "                        new_tag_map[tag] = {POS: X}\n",
    "        cdef Vocab vocab = self.vocab\n",
    "        if new_tag_map:\n",
    "            vocab.morphology = Morphology(vocab.strings, new_tag_map,\n",
    "                                          vocab.morphology.lemmatizer,\n",
    "                                          exc=vocab.morphology.exc)\n",
    "        self.cfg[\"pretrained_vectors\"] = kwargs.get(\"pretrained_vectors\")\n",
    "        if self.model is True:\n",
    "            for hp in [\"token_vector_width\", \"conv_depth\"]:\n",
    "                if hp in kwargs:\n",
    "                    self.cfg[hp] = kwargs[hp]\n",
    "            self.model = self.Model(self.vocab.morphology.n_tags, **self.cfg)\n",
    "        link_vectors_to_models(self.vocab)\n",
    "        if sgd is None:\n",
    "            sgd = self.create_optimizer()\n",
    "        return sgd\n",
    "\n",
    "    @classmethod\n",
    "    def Model(cls, n_tags, **cfg):\n",
    "        if cfg.get(\"pretrained_dims\") and not cfg.get(\"pretrained_vectors\"):\n",
    "            raise ValueError(TempErrors.T008)\n",
    "        return build_tagger_model(n_tags, **cfg)\n",
    "\n",
    "    def add_label(self, label, values=None):\n",
    "        if label in self.labels:\n",
    "            return 0\n",
    "        if self.model not in (True, False, None):\n",
    "            # Here's how the model resizing will work, once the\n",
    "            # neuron-to-tag mapping is no longer controlled by\n",
    "            # the Morphology class, which sorts the tag names.\n",
    "            # The sorting makes adding labels difficult.\n",
    "            # smaller = self.model._layers[-1]\n",
    "            # larger = Softmax(len(self.labels)+1, smaller.nI)\n",
    "            # copy_array(larger.W[:smaller.nO], smaller.W)\n",
    "            # copy_array(larger.b[:smaller.nO], smaller.b)\n",
    "            # self.model._layers[-1] = larger\n",
    "            raise ValueError(TempErrors.T003)\n",
    "        tag_map = dict(self.vocab.morphology.tag_map)\n",
    "        if values is None:\n",
    "            values = {POS: \"X\"}\n",
    "        tag_map[label] = values\n",
    "        self.vocab.morphology = Morphology(\n",
    "            self.vocab.strings, tag_map=tag_map,\n",
    "            lemmatizer=self.vocab.morphology.lemmatizer,\n",
    "            exc=self.vocab.morphology.exc)\n",
    "        return 1\n",
    "\n",
    "    def use_params(self, params):\n",
    "        with self.model.use_params(params):\n",
    "            yield\n",
    "\n",
    "    def to_bytes(self, exclude=tuple(), **kwargs):\n",
    "        serialize = OrderedDict()\n",
    "        if self.model not in (None, True, False):\n",
    "            serialize[\"model\"] = self.model.to_bytes\n",
    "        serialize[\"vocab\"] = self.vocab.to_bytes\n",
    "        serialize[\"cfg\"] = lambda: srsly.json_dumps(self.cfg)\n",
    "        tag_map = OrderedDict(sorted(self.vocab.morphology.tag_map.items()))\n",
    "        serialize[\"tag_map\"] = lambda: srsly.msgpack_dumps(tag_map)\n",
    "        exclude = util.get_serialization_exclude(serialize, exclude, kwargs)\n",
    "        return util.to_bytes(serialize, exclude)\n",
    "\n",
    "    def from_bytes(self, bytes_data, exclude=tuple(), **kwargs):\n",
    "        def load_model(b):\n",
    "            # TODO: Remove this once we don't have to handle previous models\n",
    "            if self.cfg.get(\"pretrained_dims\") and \"pretrained_vectors\" not in self.cfg:\n",
    "                self.cfg[\"pretrained_vectors\"] = self.vocab.vectors.name\n",
    "            if self.model is True:\n",
    "                token_vector_width = util.env_opt(\n",
    "                    \"token_vector_width\",\n",
    "                    self.cfg.get(\"token_vector_width\", 96))\n",
    "                self.model = self.Model(self.vocab.morphology.n_tags, **self.cfg)\n",
    "            self.model.from_bytes(b)\n",
    "\n",
    "        def load_tag_map(b):\n",
    "            tag_map = srsly.msgpack_loads(b)\n",
    "            self.vocab.morphology = Morphology(\n",
    "                self.vocab.strings, tag_map=tag_map,\n",
    "                lemmatizer=self.vocab.morphology.lemmatizer,\n",
    "                exc=self.vocab.morphology.exc)\n",
    "\n",
    "        deserialize = OrderedDict((\n",
    "            (\"vocab\", lambda b: self.vocab.from_bytes(b)),\n",
    "            (\"tag_map\", load_tag_map),\n",
    "            (\"cfg\", lambda b: self.cfg.update(srsly.json_loads(b))),\n",
    "            (\"model\", lambda b: load_model(b)),\n",
    "        ))\n",
    "        exclude = util.get_serialization_exclude(deserialize, exclude, kwargs)\n",
    "        util.from_bytes(bytes_data, deserialize, exclude)\n",
    "        return self\n",
    "\n",
    "    def to_disk(self, path, exclude=tuple(), **kwargs):\n",
    "        tag_map = OrderedDict(sorted(self.vocab.morphology.tag_map.items()))\n",
    "        serialize = OrderedDict((\n",
    "            (\"vocab\", lambda p: self.vocab.to_disk(p)),\n",
    "            (\"tag_map\", lambda p: srsly.write_msgpack(p, tag_map)),\n",
    "            (\"model\", lambda p: p.open(\"wb\").write(self.model.to_bytes())),\n",
    "            (\"cfg\", lambda p: srsly.write_json(p, self.cfg))\n",
    "        ))\n",
    "        exclude = util.get_serialization_exclude(serialize, exclude, kwargs)\n",
    "        util.to_disk(path, serialize, exclude)\n",
    "\n",
    "    def from_disk(self, path, exclude=tuple(), **kwargs):\n",
    "        def load_model(p):\n",
    "            # TODO: Remove this once we don't have to handle previous models\n",
    "            if self.cfg.get(\"pretrained_dims\") and \"pretrained_vectors\" not in self.cfg:\n",
    "                self.cfg[\"pretrained_vectors\"] = self.vocab.vectors.name\n",
    "            if self.model is True:\n",
    "                self.model = self.Model(self.vocab.morphology.n_tags, **self.cfg)\n",
    "            with p.open(\"rb\") as file_:\n",
    "                self.model.from_bytes(file_.read())\n",
    "\n",
    "        def load_tag_map(p):\n",
    "            tag_map = srsly.read_msgpack(p)\n",
    "            self.vocab.morphology = Morphology(\n",
    "                self.vocab.strings, tag_map=tag_map,\n",
    "                lemmatizer=self.vocab.morphology.lemmatizer,\n",
    "                exc=self.vocab.morphology.exc)\n",
    "\n",
    "        deserialize = OrderedDict((\n",
    "            (\"cfg\", lambda p: self.cfg.update(_load_cfg(p))),\n",
    "            (\"vocab\", lambda p: self.vocab.from_disk(p)),\n",
    "            (\"tag_map\", load_tag_map),\n",
    "            (\"model\", load_model),\n",
    "        ))\n",
    "        exclude = util.get_serialization_exclude(deserialize, exclude, kwargs)\n",
    "        util.from_disk(path, deserialize, exclude)\n",
    "        return self\n",
    "\n",
    "\n",
    "class MultitaskObjective(Tagger):\n",
    "    \"\"\"Experimental: Assist training of a parser or tagger, by training a\n",
    "    side-objective.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"nn_labeller\"\n",
    "\n",
    "    def __init__(self, vocab, model=True, target='dep_tag_offset', **cfg):\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        if target == \"dep\":\n",
    "            self.make_label = self.make_dep\n",
    "        elif target == \"tag\":\n",
    "            self.make_label = self.make_tag\n",
    "        elif target == \"ent\":\n",
    "            self.make_label = self.make_ent\n",
    "        elif target == \"dep_tag_offset\":\n",
    "            self.make_label = self.make_dep_tag_offset\n",
    "        elif target == \"ent_tag\":\n",
    "            self.make_label = self.make_ent_tag\n",
    "        elif target == \"sent_start\":\n",
    "            self.make_label = self.make_sent_start\n",
    "        elif hasattr(target, \"__call__\"):\n",
    "            self.make_label = target\n",
    "        else:\n",
    "            raise ValueError(Errors.E016)\n",
    "        self.cfg = dict(cfg)\n",
    "        self.cfg.setdefault(\"cnn_maxout_pieces\", 2)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self.cfg.setdefault(\"labels\", {})\n",
    "\n",
    "    @labels.setter\n",
    "    def labels(self, value):\n",
    "        self.cfg[\"labels\"] = value\n",
    "\n",
    "    def set_annotations(self, docs, dep_ids, tensors=None):\n",
    "        pass\n",
    "\n",
    "    def begin_training(self, get_gold_tuples=lambda: [], pipeline=None, tok2vec=None,\n",
    "                       sgd=None, **kwargs):\n",
    "        gold_tuples = nonproj.preprocess_training_data(get_gold_tuples())\n",
    "        for raw_text, annots_brackets in gold_tuples:\n",
    "            for annots, brackets in annots_brackets:\n",
    "                ids, words, tags, heads, deps, ents = annots\n",
    "                for i in range(len(ids)):\n",
    "                    label = self.make_label(i, words, tags, heads, deps, ents)\n",
    "                    if label is not None and label not in self.labels:\n",
    "                        self.labels[label] = len(self.labels)\n",
    "        if self.model is True:\n",
    "            token_vector_width = util.env_opt(\"token_vector_width\")\n",
    "            self.model = self.Model(len(self.labels), tok2vec=tok2vec)\n",
    "        link_vectors_to_models(self.vocab)\n",
    "        if sgd is None:\n",
    "            sgd = self.create_optimizer()\n",
    "        return sgd\n",
    "\n",
    "    @classmethod\n",
    "    def Model(cls, n_tags, tok2vec=None, **cfg):\n",
    "        token_vector_width = util.env_opt(\"token_vector_width\", 96)\n",
    "        softmax = Softmax(n_tags, token_vector_width*2)\n",
    "        model = chain(\n",
    "            tok2vec,\n",
    "            LayerNorm(Maxout(token_vector_width*2, token_vector_width, pieces=3)),\n",
    "            softmax\n",
    "        )\n",
    "        model.tok2vec = tok2vec\n",
    "        model.softmax = softmax\n",
    "        return model\n",
    "\n",
    "    def predict(self, docs):\n",
    "        self.require_model()\n",
    "        tokvecs = self.model.tok2vec(docs)\n",
    "        scores = self.model.softmax(tokvecs)\n",
    "        return tokvecs, scores\n",
    "\n",
    "    def get_loss(self, docs, golds, scores):\n",
    "        if len(docs) != len(golds):\n",
    "            raise ValueError(Errors.E077.format(value=\"loss\", n_docs=len(docs),\n",
    "                                                n_golds=len(golds)))\n",
    "        cdef int idx = 0\n",
    "        correct = numpy.zeros((scores.shape[0],), dtype=\"i\")\n",
    "        guesses = scores.argmax(axis=1)\n",
    "        for i, gold in enumerate(golds):\n",
    "            for j in range(len(docs[i])):\n",
    "                # Handes alignment for tokenization differences\n",
    "                label = self.make_label(j, gold.words, gold.tags,\n",
    "                                        gold.heads, gold.labels, gold.ents)\n",
    "                if label is None or label not in self.labels:\n",
    "                    correct[idx] = guesses[idx]\n",
    "                else:\n",
    "                    correct[idx] = self.labels[label]\n",
    "                idx += 1\n",
    "        correct = self.model.ops.xp.array(correct, dtype=\"i\")\n",
    "        d_scores = scores - to_categorical(correct, nb_classes=scores.shape[1])\n",
    "        loss = (d_scores**2).sum()\n",
    "        return float(loss), d_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dep(i, words, tags, heads, deps, ents):\n",
    "        if deps[i] is None or heads[i] is None:\n",
    "            return None\n",
    "        return deps[i]\n",
    "\n",
    "    @staticmethod\n",
    "    def make_tag(i, words, tags, heads, deps, ents):\n",
    "        return tags[i]\n",
    "\n",
    "    @staticmethod\n",
    "    def make_ent(i, words, tags, heads, deps, ents):\n",
    "        if ents is None:\n",
    "            return None\n",
    "        return ents[i]\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dep_tag_offset(i, words, tags, heads, deps, ents):\n",
    "        if deps[i] is None or heads[i] is None:\n",
    "            return None\n",
    "        offset = heads[i] - i\n",
    "        offset = min(offset, 2)\n",
    "        offset = max(offset, -2)\n",
    "        return \"%s-%s:%d\" % (deps[i], tags[i], offset)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_ent_tag(i, words, tags, heads, deps, ents):\n",
    "        if ents is None or ents[i] is None:\n",
    "            return None\n",
    "        else:\n",
    "            return \"%s-%s\" % (tags[i], ents[i])\n",
    "\n",
    "    @staticmethod\n",
    "    def make_sent_start(target, words, tags, heads, deps, ents, cache=True, _cache={}):\n",
    "        \"\"\"A multi-task objective for representing sentence boundaries,\n",
    "        using BILU scheme. (O is impossible)\n",
    "\n",
    "        The implementation of this method uses an internal cache that relies\n",
    "        on the identity of the heads array, to avoid requiring a new piece\n",
    "        of gold data. You can pass cache=False if you know the cache will\n",
    "        do the wrong thing.\n",
    "        \"\"\"\n",
    "        assert len(words) == len(heads)\n",
    "        assert target < len(words), (target, len(words))\n",
    "        if cache:\n",
    "            if id(heads) in _cache:\n",
    "                return _cache[id(heads)][target]\n",
    "            else:\n",
    "                for key in list(_cache.keys()):\n",
    "                    _cache.pop(key)\n",
    "            sent_tags = [\"I-SENT\"] * len(words)\n",
    "            _cache[id(heads)] = sent_tags\n",
    "        else:\n",
    "            sent_tags = [\"I-SENT\"] * len(words)\n",
    "\n",
    "        def _find_root(child):\n",
    "            seen = set([child])\n",
    "            while child is not None and heads[child] != child:\n",
    "                seen.add(child)\n",
    "                child = heads[child]\n",
    "            return child\n",
    "\n",
    "        sentences = {}\n",
    "        for i in range(len(words)):\n",
    "            root = _find_root(i)\n",
    "            if root is None:\n",
    "                sent_tags[i] = None\n",
    "            else:\n",
    "                sentences.setdefault(root, []).append(i)\n",
    "        for root, span in sorted(sentences.items()):\n",
    "            if len(span) == 1:\n",
    "                sent_tags[span[0]] = \"U-SENT\"\n",
    "            else:\n",
    "                sent_tags[span[0]] = \"B-SENT\"\n",
    "                sent_tags[span[-1]] = \"L-SENT\"\n",
    "        return sent_tags[target]\n",
    "\n",
    "\n",
    "class ClozeMultitask(Pipe):\n",
    "    @classmethod\n",
    "    def Model(cls, vocab, tok2vec, **cfg):\n",
    "        output_size = vocab.vectors.data.shape[1]\n",
    "        output_layer = chain(\n",
    "            LayerNorm(Maxout(output_size, tok2vec.nO, pieces=3)),\n",
    "            zero_init(Affine(output_size, output_size, drop_factor=0.0))\n",
    "        )\n",
    "        model = chain(tok2vec, output_layer)\n",
    "        model = masked_language_model(vocab, model)\n",
    "        model.tok2vec = tok2vec\n",
    "        model.output_layer = output_layer\n",
    "        return model\n",
    "\n",
    "    def __init__(self, vocab, model=True, **cfg):\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def set_annotations(self, docs, dep_ids, tensors=None):\n",
    "        pass\n",
    "\n",
    "    def begin_training(self, get_gold_tuples=lambda: [], pipeline=None,\n",
    "                        tok2vec=None, sgd=None, **kwargs):\n",
    "        link_vectors_to_models(self.vocab)\n",
    "        if self.model is True:\n",
    "            self.model = self.Model(self.vocab, tok2vec)\n",
    "        X = self.model.ops.allocate((5, self.model.tok2vec.nO))\n",
    "        self.model.output_layer.begin_training(X)\n",
    "        if sgd is None:\n",
    "            sgd = self.create_optimizer()\n",
    "        return sgd\n",
    "\n",
    "    def predict(self, docs):\n",
    "        self.require_model()\n",
    "        tokvecs = self.model.tok2vec(docs)\n",
    "        vectors = self.model.output_layer(tokvecs)\n",
    "        return tokvecs, vectors\n",
    "\n",
    "    def get_loss(self, docs, vectors, prediction):\n",
    "        # The simplest way to implement this would be to vstack the\n",
    "        # token.vector values, but that's a bit inefficient, especially on GPU.\n",
    "        # Instead we fetch the index into the vectors table for each of our tokens,\n",
    "        # and look them up all at once. This prevents data copying.\n",
    "        ids = self.model.ops.flatten([doc.to_array(ID).ravel() for doc in docs])\n",
    "        target = vectors[ids]\n",
    "        gradient = (prediction - target) / prediction.shape[0]\n",
    "        loss = (gradient**2).sum()\n",
    "        return float(loss), gradient\n",
    "\n",
    "    def update(self, docs, golds, drop=0., sgd=None, losses=None):\n",
    "        pass\n",
    "\n",
    "    def rehearse(self, docs, drop=0., sgd=None, losses=None):\n",
    "        self.require_model()\n",
    "        if losses is not None and self.name not in losses:\n",
    "            losses[self.name] = 0.\n",
    "        predictions, bp_predictions = self.model.begin_update(docs, drop=drop)\n",
    "        loss, d_predictions = self.get_loss(docs, self.vocab.vectors.data, predictions)\n",
    "        bp_predictions(d_predictions, sgd=sgd)\n",
    "\n",
    "        if losses is not None:\n",
    "            losses[self.name] += loss\n",
    "\n",
    "\n",
    "class TextCategorizer(Pipe):\n",
    "    \"\"\"Pipeline component for text classification.\n",
    "\n",
    "    DOCS: https://spacy.io/api/textcategorizer\n",
    "    \"\"\"\n",
    "    name = 'textcat'\n",
    "\n",
    "    @classmethod\n",
    "    def Model(cls, nr_class=1, **cfg):\n",
    "        embed_size = util.env_opt(\"embed_size\", 2000)\n",
    "        if \"token_vector_width\" in cfg:\n",
    "            token_vector_width = cfg[\"token_vector_width\"]\n",
    "        else:\n",
    "            token_vector_width = util.env_opt(\"token_vector_width\", 96)\n",
    "        if cfg.get(\"architecture\") == \"simple_cnn\":\n",
    "            tok2vec = Tok2Vec(token_vector_width, embed_size, **cfg)\n",
    "            return build_simple_cnn_text_classifier(tok2vec, nr_class, **cfg)\n",
    "        elif cfg.get(\"architecture\") == \"bow\":\n",
    "            return build_bow_text_classifier(nr_class, **cfg)\n",
    "        else:\n",
    "            return build_text_classifier(nr_class, **cfg)\n",
    "\n",
    "    @property\n",
    "    def tok2vec(self):\n",
    "        if self.model in (None, True, False):\n",
    "            return None\n",
    "        else:\n",
    "            return self.model.tok2vec\n",
    "\n",
    "    def __init__(self, vocab, model=True, **cfg):\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        self._rehearsal_model = None\n",
    "        self.cfg = dict(cfg)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return tuple(self.cfg.setdefault(\"labels\", []))\n",
    "\n",
    "    @labels.setter\n",
    "    def labels(self, value):\n",
    "        self.cfg[\"labels\"] = tuple(value)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        scores, tensors = self.predict([doc])\n",
    "        self.set_annotations([doc], scores, tensors=tensors)\n",
    "        return doc\n",
    "\n",
    "    def pipe(self, stream, batch_size=128, n_threads=-1):\n",
    "        for docs in util.minibatch(stream, size=batch_size):\n",
    "            docs = list(docs)\n",
    "            scores, tensors = self.predict(docs)\n",
    "            self.set_annotations(docs, scores, tensors=tensors)\n",
    "            yield from docs\n",
    "\n",
    "    def predict(self, docs):\n",
    "        self.require_model()\n",
    "        scores = self.model(docs)\n",
    "        scores = self.model.ops.asarray(scores)\n",
    "        tensors = [doc.tensor for doc in docs]\n",
    "        return scores, tensors\n",
    "\n",
    "    def set_annotations(self, docs, scores, tensors=None):\n",
    "        for i, doc in enumerate(docs):\n",
    "            for j, label in enumerate(self.labels):\n",
    "                doc.cats[label] = float(scores[i, j])\n",
    "\n",
    "    def update(self, docs, golds, state=None, drop=0., sgd=None, losses=None):\n",
    "        scores, bp_scores = self.model.begin_update(docs, drop=drop)\n",
    "        loss, d_scores = self.get_loss(docs, golds, scores)\n",
    "        bp_scores(d_scores, sgd=sgd)\n",
    "        if losses is not None:\n",
    "            losses.setdefault(self.name, 0.0)\n",
    "            losses[self.name] += loss\n",
    "\n",
    "    def rehearse(self, docs, drop=0., sgd=None, losses=None):\n",
    "        if self._rehearsal_model is None:\n",
    "            return\n",
    "        scores, bp_scores = self.model.begin_update(docs, drop=drop)\n",
    "        target = self._rehearsal_model(docs)\n",
    "        gradient = scores - target\n",
    "        bp_scores(gradient, sgd=sgd)\n",
    "        if losses is not None:\n",
    "            losses.setdefault(self.name, 0.0)\n",
    "            losses[self.name] += (gradient**2).sum()\n",
    "\n",
    "    def get_loss(self, docs, golds, scores):\n",
    "        truths = numpy.zeros((len(golds), len(self.labels)), dtype=\"f\")\n",
    "        not_missing = numpy.ones((len(golds), len(self.labels)), dtype=\"f\")\n",
    "        for i, gold in enumerate(golds):\n",
    "            for j, label in enumerate(self.labels):\n",
    "                if label in gold.cats:\n",
    "                    truths[i, j] = gold.cats[label]\n",
    "                else:\n",
    "                    not_missing[i, j] = 0.\n",
    "        truths = self.model.ops.asarray(truths)\n",
    "        not_missing = self.model.ops.asarray(not_missing)\n",
    "        d_scores = (scores-truths) / scores.shape[0]\n",
    "        d_scores *= not_missing\n",
    "        mean_square_error = (d_scores**2).sum(axis=1).mean()\n",
    "        return float(mean_square_error), d_scores\n",
    "\n",
    "    def add_label(self, label):\n",
    "        if label in self.labels:\n",
    "            return 0\n",
    "        if self.model not in (None, True, False):\n",
    "            # This functionality was available previously, but was broken.\n",
    "            # The problem is that we resize the last layer, but the last layer\n",
    "            # is actually just an ensemble. We're not resizing the child layers\n",
    "            # - a huge problem.\n",
    "            raise ValueError(Errors.E116)\n",
    "            # smaller = self.model._layers[-1]\n",
    "            # larger = Affine(len(self.labels)+1, smaller.nI)\n",
    "            # copy_array(larger.W[:smaller.nO], smaller.W)\n",
    "            # copy_array(larger.b[:smaller.nO], smaller.b)\n",
    "            # self.model._layers[-1] = larger\n",
    "        self.labels = tuple(list(self.labels) + [label])\n",
    "        return 1\n",
    "\n",
    "    def begin_training(self, get_gold_tuples=lambda: [], pipeline=None, sgd=None, **kwargs):\n",
    "        if self.model is True:\n",
    "            self.cfg[\"pretrained_vectors\"] = kwargs.get(\"pretrained_vectors\")\n",
    "            self.model = self.Model(len(self.labels), **self.cfg)\n",
    "            link_vectors_to_models(self.vocab)\n",
    "        if sgd is None:\n",
    "            sgd = self.create_optimizer()\n",
    "        return sgd\n",
    "\n",
    "\n",
    "cdef class DependencyParser(Parser):\n",
    "    \"\"\"Pipeline component for dependency parsing.\n",
    "\n",
    "    DOCS: https://spacy.io/api/dependencyparser\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"parser\"\n",
    "    TransitionSystem = ArcEager\n",
    "\n",
    "    @property\n",
    "    def postprocesses(self):\n",
    "        return [nonproj.deprojectivize]\n",
    "\n",
    "    def add_multitask_objective(self, target):\n",
    "        if target == \"cloze\":\n",
    "            cloze = ClozeMultitask(self.vocab)\n",
    "            self._multitasks.append(cloze)\n",
    "        else:\n",
    "            labeller = MultitaskObjective(self.vocab, target=target)\n",
    "            self._multitasks.append(labeller)\n",
    "\n",
    "    def init_multitask_objectives(self, get_gold_tuples, pipeline, sgd=None, **cfg):\n",
    "        for labeller in self._multitasks:\n",
    "            tok2vec = self.model.tok2vec\n",
    "            labeller.begin_training(get_gold_tuples, pipeline=pipeline,\n",
    "                                    tok2vec=tok2vec, sgd=sgd)\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return (DependencyParser, (self.vocab, self.moves, self.model), None, None)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        # Get the labels from the model by looking at the available moves\n",
    "        return tuple(set(move.split(\"-\")[1] for move in self.move_names))\n",
    "\n",
    "\n",
    "cdef class EntityRecognizer(Parser):\n",
    "    \"\"\"Pipeline component for named entity recognition.\n",
    "\n",
    "    DOCS: https://spacy.io/api/entityrecognizer\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"ner\"\n",
    "    TransitionSystem = BiluoPushDown\n",
    "    nr_feature = 6\n",
    "\n",
    "    def add_multitask_objective(self, target):\n",
    "        if target == \"cloze\":\n",
    "            cloze = ClozeMultitask(self.vocab)\n",
    "            self._multitasks.append(cloze)\n",
    "        else:\n",
    "            labeller = MultitaskObjective(self.vocab, target=target)\n",
    "            self._multitasks.append(labeller)\n",
    "\n",
    "    def init_multitask_objectives(self, get_gold_tuples, pipeline, sgd=None, **cfg):\n",
    "        for labeller in self._multitasks:\n",
    "            tok2vec = self.model.tok2vec\n",
    "            labeller.begin_training(get_gold_tuples, pipeline=pipeline,\n",
    "                                    tok2vec=tok2vec)\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return (EntityRecognizer, (self.vocab, self.moves, self.model),\n",
    "                None, None)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        # Get the labels from the model by looking at the available moves, e.g.\n",
    "        # B-PERSON, I-PERSON, L-PERSON, U-PERSON\n",
    "        return tuple(set(move.split(\"-\")[1] for move in self.move_names\n",
    "                if move[0] in (\"B\", \"I\", \"L\", \"U\")))\n",
    "\n",
    "\n",
    "class EntityLinker(Pipe):\n",
    "    name = 'entity_linker'\n",
    "\n",
    "    @classmethod\n",
    "    def Model(cls, nr_class=1, **cfg):\n",
    "        # TODO: non-dummy EL implementation\n",
    "        return None\n",
    "\n",
    "    def __init__(self, model=True, **cfg):\n",
    "        self.model = False\n",
    "        self.cfg = dict(cfg)\n",
    "        self.kb = self.cfg[\"kb\"]\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        self.set_annotations([doc], scores=None, tensors=None)\n",
    "        return doc\n",
    "\n",
    "    def pipe(self, stream, batch_size=128, n_threads=-1):\n",
    "        \"\"\"Apply the pipe to a stream of documents.\n",
    "        Both __call__ and pipe should delegate to the `predict()`\n",
    "        and `set_annotations()` methods.\n",
    "        \"\"\"\n",
    "        for docs in util.minibatch(stream, size=batch_size):\n",
    "            docs = list(docs)\n",
    "            self.set_annotations(docs, scores=None, tensors=None)\n",
    "            yield from docs\n",
    "\n",
    "    def set_annotations(self, docs, scores, tensors=None):\n",
    "        \"\"\"\n",
    "        Currently implemented as taking the KB entry with highest prior probability for each named entity\n",
    "        TODO: actually use context etc\n",
    "        \"\"\"\n",
    "        for i, doc in enumerate(docs):\n",
    "            for ent in doc.ents:\n",
    "                candidates = self.kb.get_candidates(ent.text)\n",
    "                if candidates:\n",
    "                    best_candidate = max(candidates, key=lambda c: c.prior_prob)\n",
    "                    for token in ent:\n",
    "                        token.ent_kb_id_ = best_candidate.entity_\n",
    "\n",
    "    def get_loss(self, docs, golds, scores):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def add_label(self, label):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "class Sentencizer(object):\n",
    "    \"\"\"Segment the Doc into sentences using a rule-based strategy.\n",
    "\n",
    "    DOCS: https://spacy.io/api/sentencizer\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"sentencizer\"\n",
    "    default_punct_chars = [\".\", \"!\", \"?\"]\n",
    "\n",
    "    def __init__(self, punct_chars=None, **kwargs):\n",
    "        \"\"\"Initialize the sentencizer.\n",
    "\n",
    "        punct_chars (list): Punctuation characters to split on. Will be\n",
    "            serialized with the nlp object.\n",
    "        RETURNS (Sentencizer): The sentencizer component.\n",
    "\n",
    "        DOCS: https://spacy.io/api/sentencizer#init\n",
    "        \"\"\"\n",
    "        self.punct_chars = punct_chars or self.default_punct_chars\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the sentencizer to a Doc and set Token.is_sent_start.\n",
    "\n",
    "        doc (Doc): The document to process.\n",
    "        RETURNS (Doc): The processed Doc.\n",
    "\n",
    "        DOCS: https://spacy.io/api/sentencizer#call\n",
    "        \"\"\"\n",
    "        start = 0\n",
    "        seen_period = False\n",
    "        for i, token in enumerate(doc):\n",
    "            is_in_punct_chars = token.text in self.punct_chars\n",
    "            token.is_sent_start = i == 0\n",
    "            if seen_period and not token.is_punct and not is_in_punct_chars:\n",
    "                doc[start].is_sent_start = True\n",
    "                start = token.i\n",
    "                seen_period = False\n",
    "            elif is_in_punct_chars:\n",
    "                seen_period = True\n",
    "        if start < len(doc):\n",
    "            doc[start].is_sent_start = True\n",
    "        return doc\n",
    "\n",
    "    def to_bytes(self, **kwargs):\n",
    "        \"\"\"Serialize the sentencizer to a bytestring.\n",
    "\n",
    "        RETURNS (bytes): The serialized object.\n",
    "\n",
    "        DOCS: https://spacy.io/api/sentencizer#to_bytes\n",
    "        \"\"\"\n",
    "        return srsly.msgpack_dumps({\"punct_chars\": self.punct_chars})\n",
    "\n",
    "    def from_bytes(self, bytes_data, **kwargs):\n",
    "        \"\"\"Load the sentencizer from a bytestring.\n",
    "\n",
    "        bytes_data (bytes): The data to load.\n",
    "        returns (Sentencizer): The loaded object.\n",
    "\n",
    "        DOCS: https://spacy.io/api/sentencizer#from_bytes\n",
    "        \"\"\"\n",
    "        cfg = srsly.msgpack_loads(bytes_data)\n",
    "        self.punct_chars = cfg.get(\"punct_chars\", self.default_punct_chars)\n",
    "        return self\n",
    "\n",
    "    def to_disk(self, path, exclude=tuple(), **kwargs):\n",
    "        \"\"\"Serialize the sentencizer to disk.\n",
    "\n",
    "        DOCS: https://spacy.io/api/sentencizer#to_disk\n",
    "        \"\"\"\n",
    "        path = util.ensure_path(path)\n",
    "        path = path.with_suffix(\".json\")\n",
    "        srsly.write_json(path, {\"punct_chars\": self.punct_chars})\n",
    "\n",
    "\n",
    "    def from_disk(self, path, exclude=tuple(), **kwargs):\n",
    "        \"\"\"Load the sentencizer from disk.\n",
    "\n",
    "        DOCS: https://spacy.io/api/sentencizer#from_disk\n",
    "        \"\"\"\n",
    "        path = util.ensure_path(path)\n",
    "        path = path.with_suffix(\".json\")\n",
    "        cfg = srsly.read_json(path)\n",
    "        self.punct_chars = cfg.get(\"punct_chars\", self.default_punct_chars)\n",
    "        return self\n",
    "\n",
    "      \n",
    "__all__ = [\"Tagger\", \"DependencyParser\", \"EntityRecognizer\", \"Tensorizer\", \"TextCategorizer\", \"EntityLinker\", \"Sentencizer\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JegQEiSNdoH0"
   },
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JegQEiSNdoH0"
   },
   "source": [
    "## GENSIM loading Pretrained Word2Vec \n",
    "\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "# Load vectors directly from the file 1G\n",
    "model = KeyedVectors.load_word2vec_format('data/GoogleGoogleNews-vectors-negative300.bin', binary=True)\n",
    "# Access vectors for specific words with a keyed lookup:\n",
    "vector = model['easy']\n",
    "# see the shape of the vector (300,)\n",
    "vector.shape\n",
    "# Processing sentences is not as simple as with Spacy:\n",
    "vectors = [model[x] for x in \"This is some text I am processing with Spacy\".split(' ')]\n",
    "```\n",
    "\n",
    "## SpaCy Trick\n",
    "\n",
    "```python\n",
    "doc.to_json()\n",
    "```\n",
    "\n",
    "**CUSTOM EXTENSIONS**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "# Load the spacy model that you have installed\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# process a sentence using the model\n",
    "doc = nlp(\"This is some text that I am processing with Spacy\")\n",
    "# It's that simple - all of the vectors and words are assigned after this point\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SpaCy_Guide.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
